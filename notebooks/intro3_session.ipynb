{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LionAGI introduction 3 - LLM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a helpful assistant. You are asked to perform as a calculator. Return as an integer.\n",
    "\"\"\"\n",
    "\n",
    "calculator = li.Session(system=system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -3\n",
    "b = 4\n",
    "\n",
    "context = {\n",
    "    \"number1\": a,\n",
    "    \"number2\": b,\n",
    "}\n",
    "\n",
    "instruct1 = {\n",
    "    \"sum the absolute values\": \"provided with 2 numbers, return the sum of their absolute values\",}\n",
    "\n",
    "instruct2 = {\n",
    "    \"multiplication\": \"provided with 2 numbers, return their multiplication\",}\n",
    "\n",
    "instruct3 = {\n",
    "    \"case positive\": \"if the result from previous step is positive, times 2 to the previous step's result\",\n",
    "    \"case negative\": \"elif the result from previous step is negative, plus 2 to the previous step's result\",\n",
    "    \"case zero\": \"elif the result from previous step is zero, return the previous step's result\",}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:API call failed with error: {'message': 'Incorrect API key provided: None. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}\n",
      "WARNING:root:API call failed with error: {'message': 'Incorrect API key provided: None. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}\n",
      "WARNING:root:API call failed with error: {'message': 'Incorrect API key provided: None. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}\n",
      "ERROR:root:API call failed after all attempts.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lion/Documents/GitHub/lionagi/notebooks/intro3_session.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lion/Documents/GitHub/lionagi/notebooks/intro3_session.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m calculator\u001b[39m.\u001b[39;49minitiate(instruction\u001b[39m=\u001b[39;49minstruct1, context\u001b[39m=\u001b[39;49mcontext, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/lionagi/lionagi/session/session.py:80\u001b[0m, in \u001b[0;36mSession.initiate\u001b[0;34m(self, instruction, system, context, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m system \u001b[39m=\u001b[39m system \u001b[39mif\u001b[39;00m system \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39msystem\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39minitiate_conversation(\n\u001b[1;32m     77\u001b[0m     system\u001b[39m=\u001b[39msystem, instruction\u001b[39m=\u001b[39minstruction, context\u001b[39m=\u001b[39mcontext\n\u001b[1;32m     78\u001b[0m )\n\u001b[0;32m---> 80\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_OpenAI_ChatCompletion(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39mresponses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/lionagi/lionagi/session/session.py:157\u001b[0m, in \u001b[0;36mSession.call_OpenAI_ChatCompletion\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         payload\u001b[39m.\u001b[39mupdate({key: config[key]})\n\u001b[1;32m    156\u001b[0m completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_service\u001b[39m.\u001b[39mcall_api(\u001b[39mNone\u001b[39;00m, request_url, payload)  \u001b[39m# Assuming synchronous\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m completion \u001b[39m=\u001b[39m completion[\u001b[39m\"\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    158\u001b[0m llmlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39mmessages, completion)\n\u001b[1;32m    159\u001b[0m response \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: completion[\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "calculator.initiate(instruction=instruct1, context=context, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal1 = calculator.initiate(instruction=instruct1, context=context, model=\"gpt-3.5-turbo\")\n",
    "cal2 = calculator.followup(instruction=instruct3, model=\"gpt-4\", temperature=0.5)\n",
    "\n",
    "print(f\"Given {a} and {b}, the sum of absolute values is {cal1}\")\n",
    "print(f\"Since the step 1 result is {'positive' if int(cal1)>0 else 'negative'}, the second step result is {cal2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal1 = calculator.initiate(instruction=instruct2, context=context, model=\"gpt-3.5-turbo\")\n",
    "cal2 = calculator.followup(instruction=instruct3, model=\"gpt-4\", temperature=0.5, n=3)\n",
    "\n",
    "print(f\"Given {a} and {b}, the multiplication product is {cal1}\")\n",
    "print(f\"Since the step 1 result is {'positive' if int(cal1)>0 else 'negative'}, the second step result is {cal2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intruct4 = {\"string to number\": \"return as a number by itself\",}\n",
    "\n",
    "calculator.followup(instruction=intruct4, model=\"gpt-4\", temperature=0.5, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ok now let's see how we can make it more interesting\n",
    "import numpy as np\n",
    "num_iterations = 5\n",
    "\n",
    "ints1 = np.random.randint(-10, 10, size=num_iterations)\n",
    "ints2 = np.random.randint(0, 10, size=num_iterations)\n",
    "cases = np.random.randint(0,2, size=num_iterations)\n",
    "# let's define a simple parser function\n",
    "\n",
    "f = lambda i: {\"number1\": str(ints1[i]), \"number2\": str(ints2[i]), \"case_\": str(cases[i])}\n",
    "contexts = li.l_call(range(num_iterations), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a helpful assistant. You are asked to perform as a calculator. Return as an integer.\n",
    "\"\"\"\n",
    "\n",
    "context = {\n",
    "    \"number1\": a,\n",
    "    \"number2\": b,\n",
    "}\n",
    "\n",
    "instruct1 = {\n",
    "    \"sum the absolute values\": \"provided with 2 numbers, return the sum of their absolute values. i.e. |x|+|y|\",}\n",
    "\n",
    "instruct2 = {\n",
    "    \"diff the absolute values\": \"provided with 2 numbers, return the difference of absolute values. i.e. |x|-|y|\",}\n",
    "\n",
    "instruct3 = {\n",
    "    \"if previous response is positive\": \"times 2. i.e. *2\", # case 1\n",
    "    \"else\": \"plus 2. i.e. +2\",                              # case 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculator_workflow(context_):\n",
    "    calculator = li.Session(system=system, api_service=li.async_api_service)\n",
    "    context = context_.copy()\n",
    "    case = int(context.pop(\"case_\"))\n",
    "    \n",
    "    if case == 0:\n",
    "        await calculator.ainitiate(instruction=instruct1, context=context, model=\"gpt-4\", temperature=0.5)\n",
    "    elif case == 1:\n",
    "        await calculator.ainitiate(instruction=instruct2, context=context, model=\"gpt-4\", temperature=0.5)\n",
    "    \n",
    "    await calculator.afollowup(instruction=instruct3, model=\"gpt-4\", temperature=0.3)\n",
    "    return li.l_call(calculator.conversation.responses, lambda i: i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = timer()\n",
    "\n",
    "outs = await li.al_call(contexts, calculator_workflow)\n",
    "\n",
    "elapsed_time = timer() - start1\n",
    "print(f\"num_workload: {num_iterations}\")\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, out in enumerate(outs):\n",
    "    print(f\"Inputs: {ints1[idx]}, {ints2[idx]}, case: {cases[idx]}\\n\")\n",
    "    print(f\"Outputs: {out}\")\n",
    "    print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Notebook total runtime {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li.llmlog.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
