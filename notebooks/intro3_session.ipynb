{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LionAGI introduction 3 - LLM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a helpful assistant. You are asked to perform as a calculator. Return as an integer.\n",
    "\"\"\"\n",
    "\n",
    "calculator = li.Session(system=system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -3\n",
    "b = 4\n",
    "\n",
    "context = {\n",
    "    \"number1\": a,\n",
    "    \"number2\": b,\n",
    "}\n",
    "\n",
    "instruct1 = {\n",
    "    \"sum the absolute values\": \"provided with 2 numbers, return the sum of their absolute values\",}\n",
    "\n",
    "instruct2 = {\n",
    "    \"multiplication\": \"provided with 2 numbers, return their multiplication\",}\n",
    "\n",
    "instruct3 = {\n",
    "    \"case positive\": \"if the result from previous step is positive, times 2 to the previous step's result\",\n",
    "    \"case negative\": \"elif the result from previous step is negative, plus 2 to the previous step's result\",\n",
    "    \"case zero\": \"elif the result from previous step is zero, return the previous step's result\",}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/lion/Documents/GitHub/lionagi/notebooks/intro3_session.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lion/Documents/GitHub/lionagi/notebooks/intro3_session.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m calculator\u001b[39m.\u001b[39;49minitiate(instruction\u001b[39m=\u001b[39;49minstruct1, context\u001b[39m=\u001b[39;49mcontext, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/lionagi/lionagi/session/session.py:61\u001b[0m, in \u001b[0;36mSession.initiate\u001b[0;34m(self, instruction, system, context, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m system \u001b[39m=\u001b[39m system \u001b[39mif\u001b[39;00m system \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39msystem\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39minitiate_conversation(\n\u001b[1;32m     58\u001b[0m     system\u001b[39m=\u001b[39msystem, instruction\u001b[39m=\u001b[39minstruction, context\u001b[39m=\u001b[39mcontext\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_OpenAI_ChatCompletion(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39mresponses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/lionagi/lionagi/session/session.py:136\u001b[0m, in \u001b[0;36mSession.call_OpenAI_ChatCompletion\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m config[key] \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         payload\u001b[39m.\u001b[39mupdate({key: config[key]})\n\u001b[0;32m--> 136\u001b[0m completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_service\u001b[39m.\u001b[39;49mcall_api(payload)  \u001b[39m# Assuming synchronous\u001b[39;00m\n\u001b[1;32m    137\u001b[0m completion \u001b[39m=\u001b[39m completion[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    138\u001b[0m llmlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation\u001b[39m.\u001b[39mmessages, completion)\n",
      "File \u001b[0;32m~/Documents/GitHub/lionagi/lionagi/utils/oai_utils.py:638\u001b[0m, in \u001b[0;36mRateLimitedAPIService.call_api\u001b[0;34m(self, payload, input_kind, request_url)\u001b[0m\n\u001b[1;32m    636\u001b[0m request_url \u001b[39m=\u001b[39m request_url \u001b[39mif\u001b[39;00m request_url \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_url\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m input_kind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdict\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 638\u001b[0m     process_api_requests_from_dict(  \u001b[39m# Assuming this function is now synchronous\u001b[39;49;00m\n\u001b[1;32m    639\u001b[0m         d\u001b[39m=\u001b[39;49mpayload, \n\u001b[1;32m    640\u001b[0m         save_filepath\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_file_path, \n\u001b[1;32m    641\u001b[0m         request_url\u001b[39m=\u001b[39;49mrequest_url,\n\u001b[1;32m    642\u001b[0m         api_key\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_key,\n\u001b[1;32m    643\u001b[0m         max_requests_per_minute\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_requests_per_minute,\n\u001b[1;32m    644\u001b[0m         max_tokens_per_minute\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_tokens_per_minute,\n\u001b[1;32m    645\u001b[0m         token_encoding_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_encoding_name,\n\u001b[1;32m    646\u001b[0m         max_attempts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_attempts,\n\u001b[1;32m    647\u001b[0m         logging_level\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogging_level\n\u001b[1;32m    648\u001b[0m     )\n\u001b[1;32m    649\u001b[0m \u001b[39melif\u001b[39;00m input_kind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    650\u001b[0m     process_api_requests_from_file(  \u001b[39m# Assuming this function is now synchronous\u001b[39;00m\n\u001b[1;32m    651\u001b[0m         requests_filepath\u001b[39m=\u001b[39mpayload, \n\u001b[1;32m    652\u001b[0m         save_filepath\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_file_path, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    659\u001b[0m         logging_level\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogging_level\n\u001b[1;32m    660\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/lionagi/lionagi/utils/oai_utils.py:272\u001b[0m, in \u001b[0;36mprocess_api_requests_from_dict\u001b[0;34m(d, save_filepath, request_url, api_key, max_requests_per_minute, max_tokens_per_minute, token_encoding_name, max_attempts, logging_level)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_api_requests_from_dict\u001b[39m(    \n\u001b[1;32m    260\u001b[0m     d: \u001b[39mdict\u001b[39m,\n\u001b[1;32m    261\u001b[0m     save_filepath: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     max_attempts: \u001b[39mint\u001b[39m,\n\u001b[1;32m    268\u001b[0m     logging_level: \u001b[39mint\u001b[39m):\n\u001b[1;32m    270\u001b[0m     temp_file \u001b[39m=\u001b[39m dict_to_temp(d)  \u001b[39m# Assuming dict_to_tempfile is synchronous\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     process_api_requests_from_file(  \u001b[39m# Assuming this function is now synchronous\u001b[39;49;00m\n\u001b[1;32m    273\u001b[0m         requests_filepath\u001b[39m=\u001b[39;49mtemp_file\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    274\u001b[0m         save_filepath\u001b[39m=\u001b[39;49msave_filepath,\n\u001b[1;32m    275\u001b[0m         request_url\u001b[39m=\u001b[39;49mrequest_url,\n\u001b[1;32m    276\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m    277\u001b[0m         max_requests_per_minute\u001b[39m=\u001b[39;49mmax_requests_per_minute,\n\u001b[1;32m    278\u001b[0m         max_tokens_per_minute\u001b[39m=\u001b[39;49mmax_tokens_per_minute,\n\u001b[1;32m    279\u001b[0m         token_encoding_name\u001b[39m=\u001b[39;49mtoken_encoding_name,\n\u001b[1;32m    280\u001b[0m         max_attempts\u001b[39m=\u001b[39;49mmax_attempts,\n\u001b[1;32m    281\u001b[0m         logging_level\u001b[39m=\u001b[39;49mlogging_level,\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    284\u001b[0m     os\u001b[39m.\u001b[39mremove(temp_file\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m~/Documents/GitHub/lionagi/lionagi/utils/oai_utils.py:162\u001b[0m, in \u001b[0;36mprocess_api_requests_from_file\u001b[0;34m(requests_filepath, save_filepath, request_url, api_key, max_requests_per_minute, max_tokens_per_minute, token_encoding_name, max_attempts, logging_level)\u001b[0m\n\u001b[1;32m    159\u001b[0m logging\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile opened. Entering main loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[39m# Replace aiohttp.ClientSession with a synchronous HTTP client\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[39mwith\u001b[39;00m http\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mHTTPConnection(api_endpoint) \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m    163\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         \u001b[39mif\u001b[39;00m next_request \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "calculator.initiate(instruction=instruct1, context=context, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given -3 and 4, the sum of absolute values is 7\n",
      "Since the step 1 result is positive, the second step result is 14\n"
     ]
    }
   ],
   "source": [
    "cal1 = await calculator.initiate(instruction=instruct1, context=context, model=\"gpt-3.5-turbo\")\n",
    "cal2 = await calculator.followup(instruction=instruct3, model=\"gpt-4\", temperature=0.5)\n",
    "\n",
    "print(f\"Given {a} and {b}, the sum of absolute values is {cal1}\")\n",
    "print(f\"Since the step 1 result is {'positive' if int(cal1)>0 else 'negative'}, the second step result is {cal2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given -3 and 4, the multiplication product is 12\n",
      "Since the step 1 result is positive, the second step result is -12\n"
     ]
    }
   ],
   "source": [
    "cal1 = await calculator.initiate(instruction=instruct2, context=context, model=\"gpt-3.5-turbo\")\n",
    "cal2 = await calculator.followup(instruction=instruct3, model=\"gpt-4\", temperature=0.5, n=3)\n",
    "\n",
    "print(f\"Given {a} and {b}, the multiplication product is {cal1}\")\n",
    "print(f\"Since the step 1 result is {'positive' if int(cal1)>0 else 'negative'}, the second step result is {cal2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the result from a llm is not always reliable, \n",
    "# we can add another layer of llm trying to validate it to reduce eror rate\n",
    "\n",
    "intruct4 = {\"string to number\": \"return as a number by itself\",}\n",
    "\n",
    "cal4 = await calculator.followup(instruction=intruct4, model=\"gpt-4\", temperature=0.5, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-12'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3\n",
    "b = 4\n",
    "\n",
    "context = {\n",
    "    \"number1\": a,\n",
    "    \"number2\": b,\n",
    "}\n",
    "\n",
    "await calculator.initiate(instruction=instruct2, context=context, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await calculator.followup(instruction=instruct3, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ok now let's see how we can make it more interesting\n",
    "import numpy as np\n",
    "num_iterations = 5\n",
    "\n",
    "\n",
    "ints1 = np.random.randint(-10, 10, size=num_iterations)\n",
    "ints2 = np.random.randint(0, 10, size=num_iterations)\n",
    "cases = np.random.randint(0,2, size=num_iterations)\n",
    "# let's define a simple parser function\n",
    "\n",
    "f = lambda i: {\"number1\": str(ints1[i]), \"number2\": str(ints2[i]), \"case_\": str(cases[i])}\n",
    "contexts = li.l_return(range(num_iterations), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a helpful assistant. You are asked to perform as a calculator. Return as an integer.\n",
    "\"\"\"\n",
    "\n",
    "context = {\n",
    "    \"number1\": a,\n",
    "    \"number2\": b,\n",
    "}\n",
    "\n",
    "instruct1 = {\n",
    "    \"sum the absolute values\": \"provided with 2 numbers, return the sum of their absolute values. i.e. |x|+|y|\",}\n",
    "\n",
    "instruct2 = {\n",
    "    \"diff the absolute values\": \"provided with 2 numbers, return the difference of absolute values. i.e. |x|-|y|\",}\n",
    "\n",
    "instruct3 = {\n",
    "    \"if previous response is positive\": \"times 2. i.e. *2\", # case 1\n",
    "    \"else\": \"plus 2. i.e. +2\",                              # case 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculator_workflow(context_):\n",
    "    calculator = li.Session(system=system)\n",
    "    context = context_.copy()\n",
    "    case = int(context.pop(\"case_\"))\n",
    "    \n",
    "    if case == 0:\n",
    "        await calculator.initiate(instruction=instruct1, context=context, model=\"gpt-4\", temperature=0.5)\n",
    "    elif case == 1:\n",
    "        await calculator.initiate(instruction=instruct2, context=context, model=\"gpt-4\", temperature=0.5)\n",
    "    \n",
    "    await calculator.followup(instruction=instruct3, model=\"gpt-4\", temperature=0.3)\n",
    "    return li.l_return(calculator.conversation.responses, lambda i: i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workload: 5\n",
      "run clock time: 1.96 seconds\n"
     ]
    }
   ],
   "source": [
    "start1 = timer()\n",
    "\n",
    "outs = await li.async_l_return(contexts, calculator_workflow)\n",
    "\n",
    "elapsed_time = timer() - start1\n",
    "print(f\"num_workload: {num_iterations}\")\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 6, 7, case: 0\n",
      "\n",
      "Outputs: ['13', '26']\n",
      "------\n",
      "\n",
      "Inputs: 2, 7, case: 1\n",
      "\n",
      "Outputs: ['-5', '-3']\n",
      "------\n",
      "\n",
      "Inputs: 6, 0, case: 0\n",
      "\n",
      "Outputs: ['6', '12']\n",
      "------\n",
      "\n",
      "Inputs: 7, 7, case: 1\n",
      "\n",
      "Outputs: ['0', '2']\n",
      "------\n",
      "\n",
      "Inputs: -2, 2, case: 1\n",
      "\n",
      "Outputs: ['0', '2']\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, out in enumerate(outs):\n",
    "    print(f\"Inputs: {ints1[idx]}, {ints2[idx]}, case: {cases[idx]}\\n\")\n",
    "    print(f\"Outputs: {out}\")\n",
    "    print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook total runtime 7.32 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook total runtime {elapsed_time:0.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
