{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LionAGI Introduction 2: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LionAGI is very efficient and intuitive to handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext=[\".py\", \".ipynb\"]\n",
    "data_dir = Path.cwd().parent / 'data'\n",
    "project_name = \"lionagi_intro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1673 files with extension '.py' in source directory\n",
      "Found 375 files with extension '.ipynb' in source directory\n",
      "Found 2048 files in total in source directory\n"
     ]
    }
   ],
   "source": [
    "sources = li.dir_to_path(data_dir, ext, \n",
    "                         recursive=True, \n",
    "                         flat=False)\n",
    "\n",
    "print(f\"Found {len(sources[0])} files with extension '{ext[0]}' in source directory\")\n",
    "print(f\"Found {len(sources[1])} files with extension '{ext[1]}' in source directory\")\n",
    "print(f\"Found {len(li.to_list(sources, flat=True))} files in total in source directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998 logs saved to data/logs/sources/lionagi_intro_sources_2023-12-17T20_43_24_370487.csv\n",
      "There are in total 47,482,838 chracters in 1998 non-empty files\n"
     ]
    }
   ],
   "source": [
    "files = li.dir_to_files(data_dir, ext, \n",
    "                        recursive=True, \n",
    "                        project=project_name, to_csv=True) \n",
    "\n",
    "print(f\"There are in total {sum(li.l_call(files, lambda x: x['file_size'])):,} chracters in {len(files)} non-empty files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are read into <class 'dict'> type\n",
      "By default files include dict_keys(['project', 'folder', 'file', 'file_size', 'content'])\n",
      "\n",
      "-------------------------------------------------\n",
      "Sample from /Users/lion/Documents/GitHub/gitco/data/gitrepo/privateGPT-main/private_gpt/server/ingest/ingest_watcher.py\n",
      "-------------------------------------------------\n",
      "\n",
      "import argparse import sys from pathlib import Path from private_gpt.di import root_injector from private_gpt.server.ingest.ingest_service import IngestService from private_gpt.server.ingest.ingest_wa\n"
     ]
    }
   ],
   "source": [
    "test = files[50]\n",
    "\n",
    "print(f\"Files are read into {type(test)} type\")\n",
    "print(f\"By default files include {test.keys()}\\n\\n-------------------------------------------------\")\n",
    "print(f\"Sample from {li.to_list(sources, flat=True)[25]}\\n-------------------------------------------------\\n\")\n",
    "print(test['content'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length of files is 13 in characters\n",
      "Maximum length of files is 11,639,375 in characters\n",
      "Average length of files is 23,765 in characters\n",
      "\n",
      "the files seem to be fairly uneven in terms of length\n",
      "which could bring problems in our subsequent analysis, we can stardardize them into chunks \n",
      "one convinient way to do this is via file_to_chunks function, it breaks the files into organized chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens = li.l_call(files, lambda x: len(x['content']))\n",
    "min_, max_, avg_ = min(lens), max(lens), sum(lens)/len(lens)\n",
    "\n",
    "print(f\"Minimum length of files is {min_} in characters\")\n",
    "print(f\"Maximum length of files is {max_:,} in characters\")\n",
    "print(f\"Average length of files is {int(avg_):,} in characters\")\n",
    "\n",
    "print(\"\"\"\n",
    "the files seem to be fairly uneven in terms of length\n",
    "which could bring problems in our subsequent analysis, we can stardardize them into chunks \n",
    "one convinient way to do this is via file_to_chunks function, it breaks the files into organized chunks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48167 logs saved to data/logs/sources/lionagi_intro_chunks_2023-12-17T20_43_24_856171.csv\n"
     ]
    }
   ],
   "source": [
    "chunks = li.file_to_chunks(files, \n",
    "                            chunk_size=1000,  \n",
    "                            overlap=0.2, \n",
    "                            threshold=200, to_csv=True, project=project_name, filename=f\"{project_name}_chunks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 48,167 chunks\n",
      "Minimum length of content in chunk is 13 characters\n",
      "Maximum length of content in chunk is 1,300 characters\n",
      "Average length of content in chunk is 1,177 characters\n",
      "There are in total 56,716,638 chracters in total\n",
      "\n",
      "Though the chunk_size is set to be 1000 in this case, the actual chunk_size depends on a number of factors:\n",
      "- if the file is originally shorter than 1000, we will keep whole file as a chunk\n",
      "- we will chunk the files by 1000 characters, additionally\n",
      "    - we add overlap for each chunk with neighbor. For example, if\n",
      "        - first chunk would have one side of neighbor, it will be 1000 + 1000 * 0.2/2 = 1100\n",
      "        - second chunk would have two sides of neighbor, it will be 1000 + 1000 * 0.2 = 1200\n",
      "    - last chunk if longer than threshold, it will be 1000*0.2/2 + remaining length\n",
      "    - if the remaining length is shorter than threshold, we will merge it with the preceeding chunk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens = li.l_call(li.to_list(chunks, flat=True), lambda x: len(x[\"chunk_content\"]))\n",
    "min_, max_, avg_ = min(lens), max(lens), sum(lens)/len(lens)\n",
    "\n",
    "print(f\"There are in total {len(li.to_list(chunks,flat=True)):,} chunks\")\n",
    "print(f\"Minimum length of content in chunk is {min_} characters\")\n",
    "print(f\"Maximum length of content in chunk is {max_:,} characters\")\n",
    "print(f\"Average length of content in chunk is {int(avg_):,} characters\")\n",
    "print(f\"There are in total {sum(li.l_call(chunks, lambda x: x['chunk_size'])):,} chracters in total\")\n",
    "\n",
    "print(\"\"\"\n",
    "Though the chunk_size is set to be 1000, the actual chunk_size depends on a number of factors:\n",
    "- if the file is originally shorter than 1000, we will keep whole file as a chunk\n",
    "- for files longer than 1000, we will break them into chunks with 1000 characters\n",
    "    - we add overlap for each chunk with its neighbor. For example, if\n",
    "        - first chunk has one neighbor, it would be 1000 + 1000 * 0.2/2 = 1100\n",
    "        - second chunk has two neighbors, so it will be 1000 + 1000 * 0.2 = 1200\n",
    "    - last chunk if longer than threshold , it will be 1000*0.2/2 + remaining length\n",
    "    - if the remaining length is shorter than threshold, we will merge it with the preceeding chunk\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Aggregate into bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's say you conducted certain llm analysis or similar data transformation on the chunks\n",
      "and you now you want to put them in groups(bins) of certain range of length \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Let's say you conducted certain llm analysis or similar data transformation on the chunks\n",
    "and you now you want to put them in groups(bins) of certain range of length \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For bin size of 8000: There are in total 7839 bins\n",
      "For bin size of 4000: There are in total 15845 bins\n",
      "For bin size of 2000: There are in total 46908 bins\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x['chunk_content']\n",
    "inputs = li.to_list(li.l_call(chunks, f))\n",
    "\n",
    "bins = li.get_bins(inputs, upper=8000)\n",
    "print(f\"For bin size of 8000: There are in total {len(bins)} bins\")\n",
    "\n",
    "bins = li.get_bins(inputs, upper=4000)\n",
    "print(f\"For bin size of 4000: There are in total {len(bins)} bins\")\n",
    "\n",
    "bins = li.get_bins(inputs, upper=2000)\n",
    "print(f\"For bin size of 2000: There are in total {len(bins)} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapse = timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files processed 1,998, with 47,482,838 chracters of content in total\n",
      "Total chunks produced 48,167, with 56,716,638 chracters of content in total\n",
      "Total runtime: 1.467 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total files processed {len(files):,}, with {sum(li.l_call(files, lambda x: x['file_size'])):,} chracters of content in total\")\n",
    "print(f\"Total chunks produced {len(chunks):,}, with {sum(li.l_call(chunks, lambda x: x['chunk_size'])):,} chracters of content in total\")\n",
    "\n",
    "print(f\"Total runtime: {elapse:.03f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "chunks vs bins\n",
    "depends on use case and nature of data:\n",
    "- input type\n",
    "    - chunk_text works with a single input string\n",
    "    - get_bins works with a list of items, not just text\n",
    "- output type\n",
    "    - chunk_text returns a list of text chunks\n",
    "    - get_bins returns a list of list where each inner list contains indices of items forming a bin\n",
    "- use case considerations\n",
    "    - chunk_text for text segmentation and analysis, especially when dealing with a single large text\n",
    "    - get_bins when you have a list of items and want to group them into bins based on their cumulative length    \n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
