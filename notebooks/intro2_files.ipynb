{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LionAGI Introduction 2: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LionAGI is very efficient and intuitive to handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext=[\".py\", \".ipynb\"]\n",
    "data_dir = Path.cwd().parent / 'data'\n",
    "project_name = \"lionagi_intro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1673 files with extension '.py' in source directory\n",
      "Found 375 files with extension '.ipynb' in source directory\n",
      "Found 2048 files in total in source directory\n"
     ]
    }
   ],
   "source": [
    "sources = li.dir_to_path(data_dir, ext, \n",
    "                         recursive=True, \n",
    "                         flat=False)\n",
    "\n",
    "print(f\"Found {len(sources[0])} files with extension '{ext[0]}' in source directory\")\n",
    "print(f\"Found {len(sources[1])} files with extension '{ext[1]}' in source directory\")\n",
    "print(f\"Found {len(li.to_list(sources, flat=True))} files in total in source directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 47,597,917 chracters in 1998 non-empty files\n"
     ]
    }
   ],
   "source": [
    "files = li.dir_to_files(data_dir, ext, \n",
    "                        recursive=True, \n",
    "                        project=project_name)   #to_csv=True \n",
    "\n",
    "print(f\"There are in total {sum(li.l_call(files, lambda x: x['file_size'])):,} chracters in {len(files)} non-empty files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are read into <class 'dict'> type\n",
      "By default files include dict_keys(['project', 'folder', 'file', 'file_size', 'content'])\n",
      "\n",
      "-------------------------------------------------\n",
      "Sample from /Users/lion/Documents/GitHub/lionagi/data/gitrepo/privateGPT-main/private_gpt/server/ingest/ingest_watcher.py\n",
      "-------------------------------------------------\n",
      "\n",
      "import argparse\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "from private_gpt.di import root_injector\n",
      "from private_gpt.server.ingest.ingest_service import IngestService\n",
      "from private_gpt.server.ingest.ingest_w\n"
     ]
    }
   ],
   "source": [
    "test = files[50]\n",
    "\n",
    "print(f\"Files are read into {type(test)} type\")\n",
    "print(f\"By default files include {test.keys()}\\n\\n-------------------------------------------------\")\n",
    "print(f\"Sample from {li.to_list(sources, flat=True)[25]}\\n-------------------------------------------------\\n\")\n",
    "print(test['content'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length of files is 13 in characters\n",
      "Maximum length of files is 11,639,637 in characters\n",
      "Average length of files is 23,822 in characters\n",
      "\n",
      "the files seem to be fairly uneven in terms of length\n",
      "which could bring problems in our subsequent analysis, we can stardardize them into chunks \n",
      "one convinient way to do this is via file_to_chunks function, it breaks the files into organized chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens = li.l_call(files, lambda x: len(x['content']))\n",
    "min_, max_, avg_ = min(lens), max(lens), sum(lens)/len(lens)\n",
    "\n",
    "print(f\"Minimum length of files is {min_} in characters\")\n",
    "print(f\"Maximum length of files is {max_:,} in characters\")\n",
    "print(f\"Average length of files is {int(avg_):,} in characters\")\n",
    "\n",
    "print(\"\"\"\n",
    "the files seem to be fairly uneven in terms of length\n",
    "which could bring problems in our subsequent analysis, we can stardardize them into chunks \n",
    "one convinient way to do this is via file_to_chunks function, it breaks the files into organized chunks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = li.files_to_chunks(files, \n",
    "                            chunk_size=1000,  \n",
    "                            overlap=0.2, \n",
    "                            threshold=200)  #to_csv=True, project=project_name, filename=f\"{project_name}_chunks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 48,277 chunks\n",
      "Minimum length of content in chunk is 13 characters\n",
      "Maximum length of content in chunk is 1,400 characters\n",
      "Average length of content in chunk is 1,178 characters\n",
      "There are in total 56,874,923 chracters in total\n",
      "\n",
      "Though the chunk_size is set to be 1000 in this case, the actual chunk_size depends on a number of factors:\n",
      "- if the file is originally shorter than 1000, we will keep whole file as a chunk\n",
      "- we will chunk the files by 1000 characters, additionally\n",
      "    - we add overlap for each chunk with neighbor. For example, if\n",
      "        - first chunk would have one side of neighbor, it will be 1000 + 1000 * 0.2/2 = 1100\n",
      "        - second chunk would have two sides of neighbor, it will be 1000 + 1000 * 0.2 = 1200\n",
      "    - last chunk if longer than threshold, it will be 1000*0.2/2 + remaining length\n",
      "    - if the remaining length is shorter than threshold, we will merge it with the preceeding chunk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens = li.l_call(li.to_list(chunks, flat=True), lambda x: len(x[\"chunk_content\"]))\n",
    "min_, max_, avg_ = min(lens), max(lens), sum(lens)/len(lens)\n",
    "\n",
    "print(f\"There are in total {len(li.to_list(chunks,flat=True)):,} chunks\")\n",
    "print(f\"Minimum length of content in chunk is {min_} characters\")\n",
    "print(f\"Maximum length of content in chunk is {max_:,} characters\")\n",
    "print(f\"Average length of content in chunk is {int(avg_):,} characters\")\n",
    "print(f\"There are in total {sum(li.l_call(chunks, lambda x: x['chunk_size'])):,} chracters in total\")\n",
    "\n",
    "print(\"\"\"\n",
    "Though the chunk_size is set to be 1000 in this case, the actual chunk_size depends on a number of factors:\n",
    "- if the file is originally shorter than 1000, we will keep whole file as a chunk\n",
    "- we will chunk the files by 1000 characters, additionally\n",
    "    - we add overlap for each chunk with neighbor. For example, if\n",
    "        - first chunk would have one side of neighbor, it will be 1000 + 1000 * 0.2/2 = 1100\n",
    "        - second chunk would have two sides of neighbor, it will be 1000 + 1000 * 0.2 = 1200\n",
    "    - last chunk if longer than threshold, it will be 1000*0.2/2 + remaining length\n",
    "    - if the remaining length is shorter than threshold, we will merge it with the preceeding chunk\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Aggregate into bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's say you conducted certain llm analysis or similar data transformation on the chunks\n",
      "and you now you want to put them in groups(bins) of certain range of length \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Let's say you conducted certain llm analysis or similar data transformation on the chunks\n",
    "and you now you want to put them in groups(bins) of certain range of length \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For bin size of 8000: There are in total 7863 bins\n",
      "For bin size of 4000: There are in total 15882 bins\n",
      "For bin size of 2000: There are in total 47036 bins\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x['chunk_content']\n",
    "inputs = li.to_list(li.l_call(chunks, f))\n",
    "\n",
    "bins = li.get_bins(inputs, upper=8000)\n",
    "print(f\"For bin size of 8000: There are in total {len(bins)} bins\")\n",
    "\n",
    "bins = li.get_bins(inputs, upper=4000)\n",
    "print(f\"For bin size of 4000: There are in total {len(bins)} bins\")\n",
    "\n",
    "bins = li.get_bins(inputs, upper=2000)\n",
    "print(f\"For bin size of 2000: There are in total {len(bins)} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapse = timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files processed 1,998, with 47,597,917 chracters of content in total\n",
      "Total chunks produced 48,277, with 56,874,923 chracters of content in total\n",
      "Total runtime: 0.593 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total files processed {len(files):,}, with {sum(li.l_call(files, lambda x: x['file_size'])):,} chracters of content in total\")\n",
    "print(f\"Total chunks produced {len(chunks):,}, with {sum(li.l_call(chunks, lambda x: x['chunk_size'])):,} chracters of content in total\")\n",
    "\n",
    "print(f\"Total runtime: {elapse:.03f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
