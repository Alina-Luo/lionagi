{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior with LionAGI and LlamaIndex Vector Index\n",
    "\n",
    "-- how to do auto explorative research with LionAGI plus RAG using llamaindex Vector Index & embedding \n",
    "\n",
    "- [LionAGI](https://github.com/lion-agi/lionagi)\n",
    "- [LlamaIndex](https://www.llamaindex.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi pypdf llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Index volatility prediction with transformers'\n",
    "dir = \"data/log/researcher/\"\n",
    "num_papers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build a Vector Index with llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "loader = SimpleDirectoryReader(input_dir='papers/', required_exts='.pdf')\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "documents = loader.load_data(show_progress=False)\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents, show_progress=False)\n",
    "\n",
    "# set up index object\n",
    "llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "index1 = VectorStoreIndex(nodes, include_embeddings=True, \n",
    "                          service_context=service_context)\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index1.as_query_engine(\n",
    "    include_text=False, response_mode=\"tree_summarize\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tool description according to OpenAI schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"query_arxiv_papers\",\n",
    "            \"description\": \"\"\"\n",
    "                           Perform a query to a QA bot with access to an \n",
    "                           index built with papers from arxiv\n",
    "                          \"\"\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"str_or_query_bundle\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"a question to ask the QA bot\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"str_or_query_bundle\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# we will need to register both the function description \n",
    "# and actual implementation\n",
    "tool = li.Tool(func=query_engine.query, parser=lambda x: x.response, schema_=tools[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Research: PROMPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FORMATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rigidly set up prompt can help make outcome more deterministic\n",
    "# though any string will work as well. \n",
    "system = {\n",
    "    \"persona\": \"a helpful world-class researcher\",\n",
    "    \"requirements\": \"\"\"\n",
    "              think step by step before returning a clear, precise \n",
    "              worded answer with a humble yet confident tone\n",
    "          \"\"\",\n",
    "    \"responsibilities\": f\"\"\"\n",
    "              you are asked to help with researching on the topic \n",
    "              of {query}\n",
    "          \"\"\",\n",
    "    \"tools\": \"provided with a QA bot for grounding responses\"\n",
    "}\n",
    "\n",
    "# similarly, we can pass in any string or dictionary to instruction\n",
    "# here we are modifying model behavior by telling mdel how to output \n",
    "deliver_format1 = {\"return required\": \"yes\", \"return format\": \"paragraph\"}\n",
    "\n",
    "deliver_format2 = {\"return required\": \"yes\", \n",
    "    \"return format\": { \n",
    "        \"json_mode\": {\n",
    "            'paper': \"paper_name\",\n",
    "            \"summary\": \"...\", \n",
    "            \"research question\": \"...\", \n",
    "            \"talking points\": {\n",
    "                \"point 1\": \"...\",\n",
    "                \"point 2\": \"...\",\n",
    "                \"point 3\": \"...\"\n",
    "            }}}}\n",
    "            \n",
    "function_call = {\n",
    "    \"notice\":f\"\"\"\n",
    "        At each task step, identified by step number, you must use the tool \n",
    "        at least twice. Notice you are provided with a QA bot as your tool, \n",
    "        the bot has access to the {num_papers} papers via a queriable index \n",
    "        that takes natural language query and return a natural language \n",
    "        answer. You can decide whether to invoke the function call, you will \n",
    "        need to ask the bot when there are things need clarification or \n",
    "        further information. you provide the query by asking a question, \n",
    "        please use the tool as extensively as you can.\n",
    "       \"\"\"\n",
    "    }\n",
    "\n",
    "# here we create a two step process imitating the steps human would take to \n",
    "# perform the research task\n",
    "instruct1 = {\n",
    "    \"task step\": \"1\", \n",
    "    \"task name\": \"read paper abstracts\", \n",
    "    \"task objective\": \"get initial understanding of the papers of interest\", \n",
    "    \"task description\": \"\"\"\n",
    "            provided with abstracts of paper, provide a brief summary \n",
    "            highlighting the paper core points, the purpose is to extract \n",
    "            as much information as possible\n",
    "          \"\"\",\n",
    "    \"deliverable\": deliver_format1\n",
    "}\n",
    "\n",
    "\n",
    "instruct2 = {\n",
    "    \"task step\": \"2\",\n",
    "    \"task name\": \"propose research questions and talking points\", \n",
    "    \"task objective\": \"initial brainstorming\", \n",
    "    \"task description\": \"\"\"\n",
    "          from the improved understanding of the paper, please propose \n",
    "          an interesting, unique and practical research question, \n",
    "          support your reasoning. Kept on asking questions if things are \n",
    "          not clear. \n",
    "        \"\"\",\n",
    "    \"deliverable\": deliver_format2,\n",
    "    \"function calling\": function_call\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Research: Setup Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = \"\"\"\n",
    "Abstractâ€”Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def read_propose(context, num=5):\n",
    "    researcher = li.Session(system, dir=dir)\n",
    "    researcher.register_tools(tool)\n",
    "    \n",
    "    await researcher.initiate(instruct1, context=context, temperature=0.7)\n",
    "    await researcher.auto_followup(instruct2, tools=tools, num=num)\n",
    "    \n",
    "    # researcher.messages_to_csv()\n",
    "    # researcher.log_to_csv()\n",
    "    return researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Research: Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = await li.alcall(abstracts, read_propose)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract provided discusses the interplay between Large Language Models (LLMs), such as ChatGPT and GPT4, and Knowledge Graphs (KGs), highlighting their complementary nature. LLMs are adept at processing natural language but are considered \"black-box\" and may not effectively capture factual knowledge. KGs, like Wikipedia and Huapu, store structured factual knowledge, which can aid LLMs in inference and interpretability, although KGs are challenging to build and maintain due to their evolving nature.\n",
      "\n",
      "The paper proposes a roadmap for integrating LLMs and KGs to harness the strengths of both. This roadmap outlines three frameworks: KG-enhanced LLMs, where KGs are integrated during the pre-training and inference stages of LLMs or to improve understanding learned knowledge; LLM-augmented KGs, which use LLMs for KG-related tasks such as embedding, completion, and graph-to-text generation; and Synergized LLMs + KGs, where both models work together reciprocally for enhanced bidirectional reasoning. The abstract concludes by reviewing existing work within these frameworks and suggesting future research directions.\n",
      "\n",
      "{\"function_list\": [{\"function\": \"func_query_arxiv_papers\", \"arguments\": \"{\\\"str_or_query_bundle\\\":\\\"What are the key challenges in integrating Large Language Models with Knowledge Graphs?\\\"}\"}]}\n",
      "\n",
      "{\"function\": \"query_arxiv_papers\", \"arguments\": {\"str_or_query_bundle\": \"What are the key challenges in integrating Large Language Models with Knowledge Graphs?\"}, \"output\": \"Integrating Large Language Models (LLMs) with Knowledge Graphs presents several key challenges:\\n\\n1. Alignment of Representations: LLMs and Knowledge Graphs represent information differently. LLMs use distributed representations where knowledge is embedded in high-dimensional vector spaces, while Knowledge Graphs use structured representations with entities and relationships explicitly defined. Aligning these representations is non-trivial and requires sophisticated mapping techniques.\\n\\n2. Scalability: Knowledge Graphs can be vast, and ensuring that LLMs can efficiently access and utilize this structured knowledge without compromising performance is a significant challenge.\\n\\n3. Knowledge Graph Completeness: Knowledge Graphs may not be complete or up-to-date, which can lead to gaps in the knowledge that LLMs can leverage. Ensuring that the Knowledge Graph is comprehensive and current is an ongoing challenge.\\n\\n4. Reasoning Capabilities: While LLMs are adept at generating human-like text, reasoning over structured data from Knowledge Graphs requires additional capabilities. LLMs need to be able to perform logical reasoning and inference using the structured information from Knowledge Graphs.\\n\\n5. Contextual Understanding: LLMs must understand the context in which a query is made to effectively utilize Knowledge Graphs. This involves disambiguating entities and understanding the relationships between them within the context of a given task.\\n\\n6. Integration Complexity: Combining the unstructured data processing of LLMs with the structured nature of Knowledge Graphs involves complex integration challenges, including data format conversion, query processing, and maintaining the integrity of both data sources.\\n\\n7. Evaluation Metrics: Developing appropriate metrics to evaluate the performance of LLMs when integrated with Knowledge Graphs is challenging. These metrics must account for the quality of the generated text, the accuracy of the information retrieved from the Knowledge Graph, and the coherence of the combined output.\\n\\n8. Ethical and Privacy Concerns: Integrating Knowledge Graphs with LLMs raises concerns about data privacy, ethical use of information, and ensuring that the integrated system does not propagate biases present in the data sources.\\n\\nAddressing these challenges requires interdisciplinary efforts, combining insights from machine learning, natural language processing, knowledge representation, and other relevant fields.\"}\n",
      "\n",
      "Based on the improved understanding of the challenges in integrating Large Language Models (LLMs) with Knowledge Graphs (KGs), I propose the following research question and talking points:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"paper\": \"Unification of Large Language Models and Knowledge Graphs\",\n",
      "  \"summary\": \"The paper discusses the synergy between LLMs and KGs, detailing how LLMs can be enhanced by KGs for better factual knowledge understanding and how KGs can leverage LLMs for tasks such as embedding and graph-to-text generation. It proposes a roadmap with three frameworks for integration and highlights the potential for mutually beneficial bidirectional reasoning.\",\n",
      "  \"research question\": \"How can we develop a scalable framework for the dynamic integration of evolving Knowledge Graphs with Large Language Models to enhance bidirectional reasoning and maintain up-to-date knowledge?\",\n",
      "  \"talking points\": {\n",
      "    \"point 1\": \"Investigate methods for real-time updating of Knowledge Graphs to reflect the latest information and how LLMs can efficiently access these updates.\",\n",
      "    \"point 2\": \"Explore scalable architectures that can align the distributed representations of LLMs with the structured representations of KGs without compromising performance.\",\n",
      "    \"point 3\": \"Develop novel evaluation metrics that specifically measure the effectiveness of LLM-KG integration in terms of knowledge accuracy, contextual understanding, and reasoning capabilities.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This research question is interesting and unique because it focuses on the dynamic aspect of KGs and their integration with LLMs, which is crucial for applications that require up-to-date information. It is also practical, as the success of such a framework could significantly improve AI systems in terms of knowledge comprehension and reasoning. The talking points aim to address some of the key challenges identified, like real-time updates of KGs, scalable integration architectures, and appropriate evaluation metrics.\n",
      "\n",
      "To further refine this proposal, I'll next use the QA bot to ask additional questions to clarify any remaining ambiguities or to gather more information that could strengthen the research question and talking points.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for msg in researcher.conversation.messages:\n",
    "    if msg.role == \"assistant\":\n",
    "        print(f\"{msg.message_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the improved understanding of the challenges in integrating Large Language Models (LLMs) with Knowledge Graphs (KGs), I propose the following research question and talking points:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"paper\": \"Unification of Large Language Models and Knowledge Graphs\",\n",
       "  \"summary\": \"The paper discusses the synergy between LLMs and KGs, detailing how LLMs can be enhanced by KGs for better factual knowledge understanding and how KGs can leverage LLMs for tasks such as embedding and graph-to-text generation. It proposes a roadmap with three frameworks for integration and highlights the potential for mutually beneficial bidirectional reasoning.\",\n",
       "  \"research question\": \"How can we develop a scalable framework for the dynamic integration of evolving Knowledge Graphs with Large Language Models to enhance bidirectional reasoning and maintain up-to-date knowledge?\",\n",
       "  \"talking points\": {\n",
       "    \"point 1\": \"Investigate methods for real-time updating of Knowledge Graphs to reflect the latest information and how LLMs can efficiently access these updates.\",\n",
       "    \"point 2\": \"Explore scalable architectures that can align the distributed representations of LLMs with the structured representations of KGs without compromising performance.\",\n",
       "    \"point 3\": \"Develop novel evaluation metrics that specifically measure the effectiveness of LLM-KG integration in terms of knowledge accuracy, contextual understanding, and reasoning capabilities.\"\n",
       "  }\n",
       "}\n",
       "```\n",
       "\n",
       "This research question is interesting and unique because it focuses on the dynamic aspect of KGs and their integration with LLMs, which is crucial for applications that require up-to-date information. It is also practical, as the success of such a framework could significantly improve AI systems in terms of knowledge comprehension and reasoning. The talking points aim to address some of the key challenges identified, like real-time updates of KGs, scalable integration architectures, and appropriate evaluation metrics.\n",
       "\n",
       "To further refine this proposal, I'll next use the QA bot to ask additional questions to clarify any remaining ambiguities or to gather more information that could strengthen the research question and talking points."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(researcher.conversation.messages[-1].message_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
