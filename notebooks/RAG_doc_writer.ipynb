{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/lion/lionagi\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiohttp>=3.9.3 (from lionagi==0.2.0)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting python-dotenv==1.0.0 (from lionagi==0.2.0)\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken>=0.5.1 (from lionagi==0.2.0)\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting pydantic>=2.6.0 (from lionagi==0.2.0)\n",
      "  Using cached pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
      "Collecting aiocache>=0.12.2 (from lionagi==0.2.0)\n",
      "  Using cached aiocache-0.12.2-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandas>=2.1.0 (from lionagi==0.2.0)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp>=3.9.3->lionagi==0.2.0)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.9.3->lionagi==0.2.0)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.9.3->lionagi==0.2.0)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.9.3->lionagi==0.2.0)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.9.3->lionagi==0.2.0)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas>=2.1.0->lionagi==0.2.0)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from pandas>=2.1.0->lionagi==0.2.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.1.0->lionagi==0.2.0)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.1.0->lionagi==0.2.0)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.6.0->lionagi==0.2.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.3 (from pydantic>=2.6.0->lionagi==0.2.0)\n",
      "  Using cached pydantic_core-2.18.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic>=2.6.0->lionagi==0.2.0)\n",
      "  Using cached typing_extensions-4.12.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken>=0.5.1->lionagi==0.2.0)\n",
      "  Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken>=0.5.1->lionagi==0.2.0)\n",
      "  Using cached requests-2.32.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->lionagi==0.2.0) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken>=0.5.1->lionagi==0.2.0)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken>=0.5.1->lionagi==0.2.0)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken>=0.5.1->lionagi==0.2.0)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken>=0.5.1->lionagi==0.2.0)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Using cached aiocache-0.12.2-py2.py3-none-any.whl (28 kB)\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.3-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl (906 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached requests-2.32.2-py3-none-any.whl (63 kB)\n",
      "Using cached typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Building wheels for collected packages: lionagi\n",
      "  Building wheel for lionagi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lionagi: filename=lionagi-0.2.0-py3-none-any.whl size=336709 sha256=a1e91b4244a10272477b5bd999a552d58f525aa7fbef6291298f4a5d0f7d42c8\n",
      "  Stored in directory: /private/var/folders/5p/rcbw097d29j3s2qt861tsjfh0000gn/T/pip-ephem-wheel-cache-w3vqdac_/wheels/e3/93/5f/949a42835cddccba143a46a86ae67f0c9be8881a4f3dbf5403\n",
      "Successfully built lionagi\n",
      "Installing collected packages: pytz, aiocache, urllib3, tzdata, typing-extensions, regex, python-dotenv, numpy, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, annotated-types, yarl, requests, pydantic-core, pandas, aiosignal, tiktoken, pydantic, aiohttp, lionagi\n",
      "Successfully installed aiocache-0.12.2 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 frozenlist-1.4.1 idna-3.7 lionagi-0.2.0 multidict-6.0.5 numpy-1.26.4 pandas-2.2.2 pydantic-2.7.2 pydantic-core-2.18.3 python-dotenv-1.0.0 pytz-2024.1 regex-2024.5.15 requests-2.32.2 tiktoken-0.7.0 typing-extensions-4.12.0 tzdata-2024.1 urllib3-2.2.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /Users/lion/lionagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path.cwd() / \"lionagi_data\"  # Path to the data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files from directory\n",
    "docs = li.load(input_dir=data_path, recursive=True, required_exts=[\".py\"])\n",
    "docs = [doc for doc in docs if len(doc.content) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chunk\n",
    "# pile = li.chunk(docs=docs, chunk_size=2000, overlap=0.1)\n",
    "\n",
    "# embed\n",
    "# await pile.embed_pile()\n",
    "\n",
    "# # save\n",
    "# pile.to_csv(\"lionagi_embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload pile from saved csv\n",
    "pile = li.pile(csv_file=\"lionagi_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "write a good API documentation for this code, must use \n",
    "query engine to check meanings of related code concepts \n",
    "to accurately describe, for example if a name of a variable,\n",
    "function, class, or module is used but not present in context,\n",
    "you must check with the query engine. make sure to cross \n",
    "reference the code with the query engine to ensure the \n",
    "documentation is accurate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/test_v02/PROMPTS.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  sys_prompt = \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module llama_index or attribute None not found. Installing llama-index...\n",
      "Collecting llama-index\n",
      "  Using cached llama_index-0.10.39-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
      "  Using cached llama_index_agent_openai-0.2.5-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.39 (from llama-index)\n",
      "  Using cached llama_index_core-0.10.39.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Using cached llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
      "  Using cached llama_index_llms_openai-0.1.21-py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Using cached llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.1.23-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
      "  Using cached openai-1.30.4-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached SQLAlchemy-2.0.30-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index) (3.9.5)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index) (2.2.2)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index) (2.32.2)\n",
      "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index) (0.7.0)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index) (4.12.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
      "  Using cached llama_parse-0.4.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index) (1.9.4)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.39->llama-index) (2.7.2)\n",
      "Collecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index) (3.7)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.39->llama-index) (2024.5.15)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.39->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.39->llama-index) (2.2.1)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached greenlet-3.0.3-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.39->llama-index)\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.39->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.39->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.39->llama-index) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.39->llama-index) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.39->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.39->llama-index) (2.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lion/Library/Mobile Documents/com~apple~CloudDocs/GitHub/gitco/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.39->llama-index) (1.16.0)\n",
      "Using cached llama_index-0.10.39-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_agent_openai-0.2.5-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Using cached llama_index_core-0.10.39.post1-py3-none-any.whl (15.4 MB)\n",
      "Using cached llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
      "Using cached llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "Using cached llama_index_llms_openai-0.1.21-py3-none-any.whl (11 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
      "Using cached llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.1.23-py3-none-any.whl (36 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached llama_parse-0.4.3-py3-none-any.whl (7.7 kB)\n",
      "Using cached llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached openai-1.30.4-py3-none-any.whl (320 kB)\n",
      "Using cached pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached SQLAlchemy-2.0.30-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached greenlet-3.0.3-cp312-cp312-macosx_11_0_universal2.whl (273 kB)\n",
      "Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, wrapt, tqdm, tenacity, SQLAlchemy, soupsieve, sniffio, PyYAML, pypdf, pillow, networkx, mypy-extensions, marshmallow, joblib, h11, greenlet, fsspec, distro, click, typing-inspect, nltk, httpcore, deprecated, beautifulsoup4, anyio, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 anyio-4.4.0 beautifulsoup4-4.12.3 click-8.1.7 dataclasses-json-0.6.6 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 fsspec-2024.5.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 joblib-1.4.2 llama-index-0.10.39 llama-index-agent-openai-0.2.5 llama-index-cli-0.1.12 llama-index-core-0.10.39.post1 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.21 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.23 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.3 llamaindex-py-client-0.1.19 marshmallow-3.21.2 mypy-extensions-1.0.0 networkx-3.3 nltk-3.8.1 openai-1.30.4 pillow-10.3.0 pypdf-4.2.0 sniffio-1.3.1 soupsieve-2.5 striprtf-0.0.26 tenacity-8.3.0 tqdm-4.66.4 typing-inspect-0.9.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "from PROMPTS import sys_prompt  # put your system prompt here\n",
    "\n",
    "model = li.iModel(\n",
    "    model=\"gpt-4o\",\n",
    "    provider=\"openai\",\n",
    "    interval_tokens=1_000_000,\n",
    "    interval_requests=1_000,\n",
    "    interval=60,\n",
    ")\n",
    "\n",
    "tools = pile.as_query_tool(\n",
    "    name=\"qa_lionagi\",\n",
    "    guidance=\"Perform query to a QA bot\",\n",
    "    query_description=\"a term/phrase to lookup or a question to answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await pile.query_pile(\"what is a pile in lionagi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatting with model...\n",
      "Found action requests in model response. Processing actions...\n",
      "Actions processed!\n",
      "\n",
      "Found extension requests in model response.\n",
      "------------------- Processing extension No.1 -------------------\n",
      "Chatting with model...\n",
      "Analyzing action responses and generating answer...\n",
      "------------------- Extension completed -------------------\n",
      "\n",
      "Analyzing action responses and generating answer...\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Directive successfully completed!\n"
     ]
    }
   ],
   "source": [
    "branch = li.Branch(system=sys_prompt, tools=tools, imodel=model)\n",
    "\n",
    "form = await branch.direct(\n",
    "    instruction=instruction,\n",
    "    context=docs[83].content,\n",
    "    allow_action=True,\n",
    "    allow_extension=True,\n",
    "    verbose=True,\n",
    "    max_extensions=2,\n",
    "    retries=3,  # sometimes the model may fail to generate a valid response or refuse to take actions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ln_id</th>\n",
       "      <th>message_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>role</th>\n",
       "      <th>content</th>\n",
       "      <th>metadata</th>\n",
       "      <th>sender</th>\n",
       "      <th>recipient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>018e68d2a4d030515c50123fd7165a10</td>\n",
       "      <td>System</td>\n",
       "      <td>2024-05-28T21:46:35.441626</td>\n",
       "      <td>system</td>\n",
       "      <td>{'system_info': '\n",
       "you are a helpful assistant,...</td>\n",
       "      <td>{'last_updated': {'recipient': '2024-05-28T21:...</td>\n",
       "      <td>system</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a34732b8755be3e8abf1fdcf095288fd</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-05-28T21:46:35.442985</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': '\n",
       "        ## Task Instructions...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:46:...</td>\n",
       "      <td>user</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>534719e39ce75a0d7fdad2ae56b82189</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-05-28T21:46:42.030346</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': '```json\n",
       "{\n",
       "  \"answer\": ...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:46:...</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daf681b0a3a6e8ffba399eee7a9a864c</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-05-28T21:46:42.035028</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-05-28T21:4...</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "      <td>ac064e0c83163b39f989e5fd616710e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b3e04159d3481c1f7982e09e42bd73a8</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-05-28T21:46:42.035496</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-05-28T21:4...</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "      <td>ac064e0c83163b39f989e5fd616710e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6e42ed404ae2879b868be2938ad51789</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-05-28T21:46:42.035727</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-05-28T21:4...</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "      <td>ac064e0c83163b39f989e5fd616710e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01662558346028730684d1b85c2fa989</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-05-28T21:46:44.816031</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-05-28T21:4...</td>\n",
       "      <td>ac064e0c83163b39f989e5fd616710e1</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7c2a816d9bc496492a3ec1ffa620d62d</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-05-28T21:46:44.816316</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-05-28T21:4...</td>\n",
       "      <td>ac064e0c83163b39f989e5fd616710e1</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d7294f43e994f836a7ae8277753ff36e</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-05-28T21:46:44.816413</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-05-28T21:4...</td>\n",
       "      <td>ac064e0c83163b39f989e5fd616710e1</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1856db78ad1a954afe2ac9ae6cd46a37</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-05-28T21:46:44.816977</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': '\n",
       "        ## Task Instructions...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:46:...</td>\n",
       "      <td>user</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0d02f7d98cfee66c355a71d2f315df54</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-05-28T21:46:50.328896</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': '```json\n",
       "{\n",
       "  \"answer\": ...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:46:...</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dcbeb445eb77ca4bceb8c24a5c98b190</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-05-28T21:46:50.330235</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': 'please provide final answer b...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:46:...</td>\n",
       "      <td>user</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4c0b5f8e8f4ae20af94302dd9f74153e</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-05-28T21:47:22.302365</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': '```json\n",
       "{\n",
       "  \"answer\": ...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:47:...</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>c164226fc56fbe3a1db1e6492ff3f71c</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-05-28T21:47:22.305147</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': 'please provide final answer b...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:47:...</td>\n",
       "      <td>user</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>d284df487080959764da0892bd475711</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-05-28T21:47:54.916314</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': '## Class: `DirectiveMi...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:47:...</td>\n",
       "      <td>ae5ed231af63915188044f18c8194294</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ln_id       message_type  \\\n",
       "0   018e68d2a4d030515c50123fd7165a10             System   \n",
       "1   a34732b8755be3e8abf1fdcf095288fd        Instruction   \n",
       "2   534719e39ce75a0d7fdad2ae56b82189  AssistantResponse   \n",
       "3   daf681b0a3a6e8ffba399eee7a9a864c      ActionRequest   \n",
       "4   b3e04159d3481c1f7982e09e42bd73a8      ActionRequest   \n",
       "5   6e42ed404ae2879b868be2938ad51789      ActionRequest   \n",
       "6   01662558346028730684d1b85c2fa989     ActionResponse   \n",
       "7   7c2a816d9bc496492a3ec1ffa620d62d     ActionResponse   \n",
       "8   d7294f43e994f836a7ae8277753ff36e     ActionResponse   \n",
       "9   1856db78ad1a954afe2ac9ae6cd46a37        Instruction   \n",
       "10  0d02f7d98cfee66c355a71d2f315df54  AssistantResponse   \n",
       "11  dcbeb445eb77ca4bceb8c24a5c98b190        Instruction   \n",
       "12  4c0b5f8e8f4ae20af94302dd9f74153e  AssistantResponse   \n",
       "13  c164226fc56fbe3a1db1e6492ff3f71c        Instruction   \n",
       "14  d284df487080959764da0892bd475711  AssistantResponse   \n",
       "\n",
       "                     timestamp       role  \\\n",
       "0   2024-05-28T21:46:35.441626     system   \n",
       "1   2024-05-28T21:46:35.442985       user   \n",
       "2   2024-05-28T21:46:42.030346  assistant   \n",
       "3   2024-05-28T21:46:42.035028  assistant   \n",
       "4   2024-05-28T21:46:42.035496  assistant   \n",
       "5   2024-05-28T21:46:42.035727  assistant   \n",
       "6   2024-05-28T21:46:44.816031  assistant   \n",
       "7   2024-05-28T21:46:44.816316  assistant   \n",
       "8   2024-05-28T21:46:44.816413  assistant   \n",
       "9   2024-05-28T21:46:44.816977       user   \n",
       "10  2024-05-28T21:46:50.328896  assistant   \n",
       "11  2024-05-28T21:46:50.330235       user   \n",
       "12  2024-05-28T21:47:22.302365  assistant   \n",
       "13  2024-05-28T21:47:22.305147       user   \n",
       "14  2024-05-28T21:47:54.916314  assistant   \n",
       "\n",
       "                                              content  \\\n",
       "0   {'system_info': '\n",
       "you are a helpful assistant,...   \n",
       "1   {'instruction': '\n",
       "        ## Task Instructions...   \n",
       "2   {'assistant_response': '```json\n",
       "{\n",
       "  \"answer\": ...   \n",
       "3   {'action_request': {'function': 'qa_lionagi', ...   \n",
       "4   {'action_request': {'function': 'qa_lionagi', ...   \n",
       "5   {'action_request': {'function': 'qa_lionagi', ...   \n",
       "6   {'action_response': {'function': 'qa_lionagi',...   \n",
       "7   {'action_response': {'function': 'qa_lionagi',...   \n",
       "8   {'action_response': {'function': 'qa_lionagi',...   \n",
       "9   {'instruction': '\n",
       "        ## Task Instructions...   \n",
       "10  {'assistant_response': '```json\n",
       "{\n",
       "  \"answer\": ...   \n",
       "11  {'instruction': 'please provide final answer b...   \n",
       "12  {'assistant_response': '```json\n",
       "{\n",
       "  \"answer\": ...   \n",
       "13  {'instruction': 'please provide final answer b...   \n",
       "14  {'assistant_response': '## Class: `DirectiveMi...   \n",
       "\n",
       "                                             metadata  \\\n",
       "0   {'last_updated': {'recipient': '2024-05-28T21:...   \n",
       "1   {'last_updated': {'sender': '2024-05-28T21:46:...   \n",
       "2   {'last_updated': {'sender': '2024-05-28T21:46:...   \n",
       "3   {'last_updated': {'function': '2024-05-28T21:4...   \n",
       "4   {'last_updated': {'function': '2024-05-28T21:4...   \n",
       "5   {'last_updated': {'function': '2024-05-28T21:4...   \n",
       "6   {'last_updated': {'function': '2024-05-28T21:4...   \n",
       "7   {'last_updated': {'function': '2024-05-28T21:4...   \n",
       "8   {'last_updated': {'function': '2024-05-28T21:4...   \n",
       "9   {'last_updated': {'sender': '2024-05-28T21:46:...   \n",
       "10  {'last_updated': {'sender': '2024-05-28T21:46:...   \n",
       "11  {'last_updated': {'sender': '2024-05-28T21:46:...   \n",
       "12  {'last_updated': {'sender': '2024-05-28T21:47:...   \n",
       "13  {'last_updated': {'sender': '2024-05-28T21:47:...   \n",
       "14  {'last_updated': {'sender': '2024-05-28T21:47:...   \n",
       "\n",
       "                              sender                         recipient  \n",
       "0                             system  ae5ed231af63915188044f18c8194294  \n",
       "1                               user  ae5ed231af63915188044f18c8194294  \n",
       "2   ae5ed231af63915188044f18c8194294                              user  \n",
       "3   ae5ed231af63915188044f18c8194294  ac064e0c83163b39f989e5fd616710e1  \n",
       "4   ae5ed231af63915188044f18c8194294  ac064e0c83163b39f989e5fd616710e1  \n",
       "5   ae5ed231af63915188044f18c8194294  ac064e0c83163b39f989e5fd616710e1  \n",
       "6   ac064e0c83163b39f989e5fd616710e1  ae5ed231af63915188044f18c8194294  \n",
       "7   ac064e0c83163b39f989e5fd616710e1  ae5ed231af63915188044f18c8194294  \n",
       "8   ac064e0c83163b39f989e5fd616710e1  ae5ed231af63915188044f18c8194294  \n",
       "9                               user  ae5ed231af63915188044f18c8194294  \n",
       "10  ae5ed231af63915188044f18c8194294                              user  \n",
       "11                              user  ae5ed231af63915188044f18c8194294  \n",
       "12  ae5ed231af63915188044f18c8194294                              user  \n",
       "13                              user  ae5ed231af63915188044f18c8194294  \n",
       "14  ae5ed231af63915188044f18c8194294                              user  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**task**: \n",
       " Follow the prompt and provide the necessary output.\n",
       "- Additional instruction: \n",
       "write a good API documentation for this code, must use \n",
       "query engine to check meanings of related code concepts \n",
       "to accurately describe, for example if a name of a variable,\n",
       "function, class, or module is used but not present in context,\n",
       "you must check with the query engine. make sure to cross \n",
       "reference the code with the query engine to ensure the \n",
       "documentation is accurate.\n",
       "\n",
       "- Additional context: \"\"\"\n",
       "Copyright 2024 HaiyangLi\n",
       "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "    http://www.apache.org/licenses/LICENSE-2.0\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "\"\"\"\n",
       "# lionagi/core/session/directive_mixin.py\n",
       "from lionagi.libs.ln_convert import strip_lower\n",
       "from lionagi.core.unit import Unit\n",
       "\n",
       "class DirectiveMixin:\n",
       "    \"\"\"\n",
       "    Mixin class for handling chat interactions within the directive framework.\n",
       "    \"\"\"\n",
       "    async def chat(\n",
       "        self,\n",
       "        instruction,  # additional instruction\n",
       "        context=None,  # context to perform the instruction on\n",
       "        system=None,  # optionally swap system message\n",
       "        sender=None,  # sender of the instruction, default \"user\"\n",
       "        recipient=None,  # recipient of the instruction, default \"branch.ln_id\"\n",
       "        branch=None,\n",
       "        requested_fields=None,  # fields to request from the context, default None\n",
       "        form=None,  # form to create instruction from, default None,\n",
       "        tools=False,  # the tools to use, use True to consider all tools, no tools by default\n",
       "        invoke_tool=True,  # whether to invoke the tool when function calling, default True\n",
       "        return_for..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**reason**: \n",
       " Let's think step by step. To accurately document the provided code, we need to understand the specific meanings of 'directive framework', 'Unit' class, and 'strip_lower' function as they are not fully explained in the provided context. Querying these terms will ensure the documentation is precise and comprehensive."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**actions**: \n",
       " \n",
       " \n",
       "1. **qa_lionagi**(query: Meaning of 'directive framework' in the context of chat interactions), \n",
       " \n",
       "2. **qa_lionagi**(query: Meaning of 'Unit' class in the context of lionagi.core.unit), \n",
       " \n",
       "3. **qa_lionagi**(query: Meaning of 'strip_lower' function in the context of lionagi.libs.ln_convert)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**action_required**: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**extension_required**: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**action_response**: \n",
       " \n",
       " \n",
       "1. **qa_lionagi**(query: Meaning of 'Unit' class in the context of lionagi.core.unit) \n",
       " \n",
       " The 'Unit' class in the context of lionagi.core.unit is a specialized class that extends both the 'Directive' and 'DirectiveMixin' classes. It is designed to handle various operations related to directives within the system. The class initializes with parameters such as branch, imodel, template, and rulebook, and sets up a validator for rule enforcement. It also includes an asynchronous 'chat' method that facilitates communication and interaction, incorporating various parameters and options to customize its behavior., \n",
       " \n",
       "2. **qa_lionagi**(query: Meaning of 'strip_lower' function in the context of lionagi.libs.ln_convert) \n",
       " \n",
       " The 'strip_lower' function in the context of lionagi.libs.ln_convert is likely a utility function that processes a string by stripping any leading or trailing whitespace and converting the string to lowercase. This is inferred from its usage in the 'get_input_output_fields' function, where it is applied to each input and output field to ensure they are in a standardized format., \n",
       " \n",
       "3. **qa_lionagi**(query: Meaning of 'strip_lower' function in the context of lionagi.libs.ln_convert) \n",
       " \n",
       " The 'strip_lower' function in the context of lionagi.libs.ln_convert is likely a utility function that processes a string by stripping any leading or trailing whitespace and converting the string to lowercase. This is inferred from its usage in the 'get_input_output_fields' function, where it is applied to each input and output field to ensure they are in a standardized format., \n",
       " \n",
       "4. **qa_lionagi**(query: Meaning of 'directive framework' in the context of chat interactions) \n",
       " \n",
       " In the context of chat interactions, the 'directive framework' refers to a structured system for managing and executing instructions or commands. It involves handling various aspects such as the sender and recipient of the instruction, the context in which the instruction is performed, and the tools or rulebooks used for validation and execution. This framework ensures that chat interactions are processed in an organized manner, allowing for additional instructions, context-specific actions, and the use of tools or models to achieve the desired outcomes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**action_performed**: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**available_tools**: ['qa_lionagi']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**answer**: \n",
       " ## Class: `DirectiveMixin`\n",
       "\n",
       "**Description**:\n",
       "`DirectiveMixin` is a mixin class designed to handle chat interactions within the directive framework. It provides methods for processing instructions with various parameters and options, managing retries, timeouts, concurrency, and validation rules.\n",
       "\n",
       "### Method: `chat`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def chat(\n",
       "    self,\n",
       "    instruction: str = None,\n",
       "    context: Any = None,\n",
       "    system: str = None,\n",
       "    sender: str = 'user',\n",
       "    recipient: str = 'branch.ln_id',\n",
       "    branch: Branch = None,\n",
       "    requested_fields: dict[str, str] = None,\n",
       "    form: Any = None,\n",
       "    tools: bool = False,\n",
       "    invoke_tool: bool = True,\n",
       "    return_form: bool = True,\n",
       "    strict: bool = False,\n",
       "    rulebook: Any = None,\n",
       "    imodel: iModel = None,\n",
       "    clear_messages: bool = False,\n",
       "    use_annotation: bool = True,\n",
       "    retries: int = 3,\n",
       "    delay: float = 0,\n",
       "    backoff_factor: float = 1,\n",
       "    default: Any = None,\n",
       "    timeout: float = None,\n",
       "    timing: bool = False,\n",
       "    return_branch: bool = False,\n",
       "    **kwargs: Any\n",
       ") -> Any:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `instruction` (str, optional): Additional instruction to process.\n",
       "- `context` (Any, optional): Context to perform the instruction on.\n",
       "- `system` (str, optional): Optionally swap the system message.\n",
       "- `sender` (str, optional): Sender of the instruction, default is 'user'.\n",
       "- `recipient` (str, optional): Recipient of the instruction, default is 'branch.ln_id'.\n",
       "- `branch` (Branch, optional): The branch to use for the instruction.\n",
       "- `requested_fields` (dict[str, str], optional): Fields to request from the context.\n",
       "- `form` (Any, optional): Form to create instruction from, default is None.\n",
       "- `tools` (bool, optional): Tools to use, use True to consider all tools, no tools by default.\n",
       "- `invoke_tool` (bool, optional): Whether to invoke the tool when function calling, default is True.\n",
       "- `return_form` (bool, optional): Whether to return the form if a form is passed in, otherwise return a dict/str.\n",
       "- `strict` (bool, optional): Whether to strictly enforce rule validation, default is False.\n",
       "- `rulebook` (Any, optional): The rulebook to use for validation, default is None (uses default rulebook).\n",
       "- `imodel` (iModel, optional): Optionally swappable iModel for the commands, otherwise self.branch.imodel.\n",
       "- `clear_messages` (bool, optional): Whether to clear previous messages, default is False.\n",
       "- `use_annotation` (bool, optional): Whether to use annotation as rule qualifier, default is True (needs rulebook if False).\n",
       "- `retries` (int, optional): Number of retries if failed, default is 3.\n",
       "- `delay` (float, optional): Number of seconds to delay before retrying, default is 0.\n",
       "- `backoff_factor` (float, optional): Exponential backoff factor, default is 1 (no backoff).\n",
       "- `default` (Any, optional): Default value to return if all retries failed.\n",
       "- `timeout` (float, optional): Timeout for the call, default is None (no timeout).\n",
       "- `timing` (bool, optional): If True, will return a tuple (output, duration), default is False.\n",
       "- `return_branch` (bool, optional): Whether to return the branch after processing, default is False.\n",
       "- `**kwargs` (Any): Additional keyword arguments for further customization.\n",
       "\n",
       "**Return Values**:\n",
       "- `Any`: The result of the chat processing, format determined by the `return_form` parameter.\n",
       "\n",
       "**Exceptions Raised**:\n",
       "- `ValueError`: If an invalid combination of parameters is provided.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with instruction and context\n",
       "result = await self.chat(instruction='Hello', context={'data': 'example'})\n",
       "print(result)\n",
       "```\n",
       "\n",
       "### Method: `direct`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def direct(\n",
       "    self,\n",
       "    *,\n",
       "    instruction: str = None,\n",
       "    context: Any = None,\n",
       "    form: Any = None,\n",
       "    branch: Branch = None,\n",
       "    tools: bool = None,\n",
       "    return_branch: bool = False,\n",
       "    reason: bool = False,\n",
       "    predict: bool = False,\n",
       "    score: Any = None,\n",
       "    select: Any = None,\n",
       "    plan: Any = None,\n",
       "    allow_action: bool = False,\n",
       "    allow_extension: bool = False,\n",
       "    max_extension: int = None,\n",
       "    confidence: Any = None,\n",
       "    score_num_digits: int = None,\n",
       "    score_range: Any = None,\n",
       "    select_choices: Any = None,\n",
       "    plan_num_step: int = None,\n",
       "    predict_num_sentences: int = None,\n",
       "    imodel: iModel = None,\n",
       "    system: str = None,\n",
       "    rulebook: Any = None,\n",
       "    directive: Any = None,\n",
       "    **kwargs: Any\n",
       ") -> Any:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `instruction` (str, optional): Instruction to process.\n",
       "- `context` (Any, optional): Context to perform the instruction on.\n",
       "- `form` (Any, optional): Form to create instruction from, default is None.\n",
       "- `branch` (Branch, optional): The branch to use for the instruction.\n",
       "- `tools` (bool, optional): Tools to use, use True to consider all tools, no tools by default.\n",
       "- `return_branch` (bool, optional): Whether to return the branch after processing, default is False.\n",
       "- `reason` (bool, optional): Whether to provide reasoning, default is False.\n",
       "- `predict` (bool, optional): Whether to make predictions, default is False.\n",
       "- `score` (Any, optional): Score to use for evaluation.\n",
       "- `select` (Any, optional): Selection criteria.\n",
       "- `plan` (Any, optional): Plan to follow.\n",
       "- `allow_action` (bool, optional): Whether to allow actions, default is False.\n",
       "- `allow_extension` (bool, optional): Whether to allow extensions, default is False.\n",
       "- `max_extension` (int, optional): Maximum number of extensions, default is None.\n",
       "- `confidence` (Any, optional): Confidence level.\n",
       "- `score_num_digits` (int, optional): Number of digits for score.\n",
       "- `score_range` (Any, optional): Range for score.\n",
       "- `select_choices` (Any, optional): Choices for selection.\n",
       "- `plan_num_step` (int, optional): Number of steps in the plan.\n",
       "- `predict_num_sentences` (int, optional): Number of sentences for prediction.\n",
       "- `imodel` (iModel, optional): Optionally swappable iModel for the commands, otherwise self.branch.imodel.\n",
       "- `system` (str, optional): Optionally swap the system message.\n",
       "- `rulebook` (Any, optional): The rulebook to use for validation, default is None (uses default rulebook).\n",
       "- `directive` (Any, optional): Directive to follow.\n",
       "- `**kwargs` (Any): Additional keyword arguments for further customization.\n",
       "\n",
       "**Return Values**:\n",
       "- `Any`: The result of the direct processing.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with instruction and context\n",
       "result = await self.direct(instruction='Hello', context={'data': 'example'})\n",
       "print(result)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit = \"\"\"\n",
    "provide documentation only: final documentation in md \n",
    "format of the module of interest, do not include other \n",
    "fields do not present in JSON format, only markdown format \n",
    "you asked a lot of good questions and got plenty answers, \n",
    "please integrate your conversation, be a lot more technical, you will \n",
    "be rewarded with 500 dollars for great work, and \n",
    "punished for subpar work, take a deep breath, you can do it\n",
    "\"\"\"\n",
    "\n",
    "final_output = await branch.chat(instruction=edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# DirectiveMixin API Documentation\n",
       "\n",
       "## Class: `DirectiveMixin`\n",
       "\n",
       "**Description**:\n",
       "`DirectiveMixin` is a mixin class designed to handle chat interactions within the directive framework. It provides methods for processing instructions with various parameters and options, managing retries, timeouts, concurrency, and validation rules.\n",
       "\n",
       "### Method: `chat`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def chat(\n",
       "    self,\n",
       "    instruction: str = None,\n",
       "    context: Any = None,\n",
       "    system: str = None,\n",
       "    sender: str = 'user',\n",
       "    recipient: str = 'branch.ln_id',\n",
       "    branch: Branch = None,\n",
       "    requested_fields: dict[str, str] = None,\n",
       "    form: Any = None,\n",
       "    tools: bool = False,\n",
       "    invoke_tool: bool = True,\n",
       "    return_form: bool = True,\n",
       "    strict: bool = False,\n",
       "    rulebook: Any = None,\n",
       "    imodel: iModel = None,\n",
       "    clear_messages: bool = False,\n",
       "    use_annotation: bool = True,\n",
       "    retries: int = 3,\n",
       "    delay: float = 0,\n",
       "    backoff_factor: float = 1,\n",
       "    default: Any = None,\n",
       "    timeout: float = None,\n",
       "    timing: bool = False,\n",
       "    return_branch: bool = False,\n",
       "    **kwargs: Any\n",
       ") -> Any:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `instruction` (str, optional): Additional instruction to process.\n",
       "- `context` (Any, optional): Context to perform the instruction on.\n",
       "- `system` (str, optional): Optionally swap the system message.\n",
       "- `sender` (str, optional): Sender of the instruction, default is 'user'.\n",
       "- `recipient` (str, optional): Recipient of the instruction, default is 'branch.ln_id'.\n",
       "- `branch` (Branch, optional): The branch to use for the instruction.\n",
       "- `requested_fields` (dict[str, str], optional): Fields to request from the context.\n",
       "- `form` (Any, optional): Form to create instruction from, default is None.\n",
       "- `tools` (bool, optional): Tools to use, use True to consider all tools, no tools by default.\n",
       "- `invoke_tool` (bool, optional): Whether to invoke the tool when function calling, default is True.\n",
       "- `return_form` (bool, optional): Whether to return the form if a form is passed in, otherwise return a dict/str.\n",
       "- `strict` (bool, optional): Whether to strictly enforce rule validation, default is False.\n",
       "- `rulebook` (Any, optional): The rulebook to use for validation, default is None (uses default rulebook).\n",
       "- `imodel` (iModel, optional): Optionally swappable iModel for the commands, otherwise self.branch.imodel.\n",
       "- `clear_messages` (bool, optional): Whether to clear previous messages, default is False.\n",
       "- `use_annotation` (bool, optional): Whether to use annotation as rule qualifier, default is True (needs rulebook if False).\n",
       "- `retries` (int, optional): Number of retries if failed, default is 3.\n",
       "- `delay` (float, optional): Number of seconds to delay before retrying, default is 0.\n",
       "- `backoff_factor` (float, optional): Exponential backoff factor, default is 1 (no backoff).\n",
       "- `default` (Any, optional): Default value to return if all retries failed.\n",
       "- `timeout` (float, optional): Timeout for the call, default is None (no timeout).\n",
       "- `timing` (bool, optional): If True, will return a tuple (output, duration), default is False.\n",
       "- `return_branch` (bool, optional): Whether to return the branch after processing, default is False.\n",
       "- `**kwargs` (Any): Additional keyword arguments for further customization.\n",
       "\n",
       "**Return Values**:\n",
       "- `Any`: The result of the chat processing, format determined by the `return_form` parameter.\n",
       "\n",
       "**Exceptions Raised**:\n",
       "- `ValueError`: If an invalid combination of parameters is provided.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with instruction and context\n",
       "result = await self.chat(instruction='Hello', context={'data': 'example'})\n",
       "print(result)\n",
       "```\n",
       "\n",
       "### Method: `direct`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def direct(\n",
       "    self,\n",
       "    *,\n",
       "    instruction: str = None,\n",
       "    context: Any = None,\n",
       "    form: Any = None,\n",
       "    branch: Branch = None,\n",
       "    tools: bool = None,\n",
       "    return_branch: bool = False,\n",
       "    reason: bool = False,\n",
       "    predict: bool = False,\n",
       "    score: Any = None,\n",
       "    select: Any = None,\n",
       "    plan: Any = None,\n",
       "    allow_action: bool = False,\n",
       "    allow_extension: bool = False,\n",
       "    max_extension: int = None,\n",
       "    confidence: Any = None,\n",
       "    score_num_digits: int = None,\n",
       "    score_range: Any = None,\n",
       "    select_choices: Any = None,\n",
       "    plan_num_step: int = None,\n",
       "    predict_num_sentences: int = None,\n",
       "    imodel: iModel = None,\n",
       "    system: str = None,\n",
       "    rulebook: Any = None,\n",
       "    directive: Any = None,\n",
       "    **kwargs: Any\n",
       ") -> Any:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `instruction` (str, optional): Instruction to process.\n",
       "- `context` (Any, optional): Context to perform the instruction on.\n",
       "- `form` (Any, optional): Form to create instruction from, default is None.\n",
       "- `branch` (Branch, optional): The branch to use for the instruction.\n",
       "- `tools` (bool, optional): Tools to use, use True to consider all tools, no tools by default.\n",
       "- `return_branch` (bool, optional): Whether to return the branch after processing, default is False.\n",
       "- `reason` (bool, optional): Whether to provide reasoning, default is False.\n",
       "- `predict` (bool, optional): Whether to make predictions, default is False.\n",
       "- `score` (Any, optional): Score to use for evaluation.\n",
       "- `select` (Any, optional): Selection criteria.\n",
       "- `plan` (Any, optional): Plan to follow.\n",
       "- `allow_action` (bool, optional): Whether to allow actions, default is False.\n",
       "- `allow_extension` (bool, optional): Whether to allow extensions, default is False.\n",
       "- `max_extension` (int, optional): Maximum number of extensions, default is None.\n",
       "- `confidence` (Any, optional): Confidence level.\n",
       "- `score_num_digits` (int, optional): Number of digits for score.\n",
       "- `score_range` (Any, optional): Range for score.\n",
       "- `select_choices` (Any, optional): Choices for selection.\n",
       "- `plan_num_step` (int, optional): Number of steps in the plan.\n",
       "- `predict_num_sentences` (int, optional): Number of sentences for prediction.\n",
       "- `imodel` (iModel, optional): Optionally swappable iModel for the commands, otherwise self.branch.imodel.\n",
       "- `system` (str, optional): Optionally swap the system message.\n",
       "- `rulebook` (Any, optional): The rulebook to use for validation, default is None (uses default rulebook).\n",
       "- `directive` (Any, optional): Directive to follow.\n",
       "- `**kwargs` (Any): Additional keyword arguments for further customization.\n",
       "\n",
       "**Return Values**:\n",
       "- `Any`: The result of the direct processing.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with instruction and context\n",
       "result = await self.direct(instruction='Hello', context={'data': 'example'})\n",
       "print(result)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check extension forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**task**: \n",
       " Follow the prompt and provide the necessary output.\n",
       "- Additional instruction: N/A\n",
       "- Additional context: N/A\n",
       "- Reason and prepare actions with GIVEN TOOLS ONLY.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**reason**: \n",
       " Let's think step by step. The previous actions have provided the necessary information about the 'directive framework', 'Unit' class, and 'strip_lower' function. Now, I will proceed to write the API documentation for the provided code."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**actions**: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**action_required**: False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**available_tools**: ['qa_lionagi']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**answer**: \n",
       " ## Class: `DirectiveMixin`\n",
       "\n",
       "**Description**:\n",
       "`DirectiveMixin` is a mixin class designed to handle chat interactions within the directive framework. It provides methods for processing instructions with various parameters and options, managing retries, timeouts, concurrency, and validation rules.\n",
       "\n",
       "### Method: `chat`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def chat(\n",
       "    self,\n",
       "    instruction: str = None,\n",
       "    context: Any = None,\n",
       "    system: str = None,\n",
       "    sender: str = 'user',\n",
       "    recipient: str = 'branch.ln_id',\n",
       "    branch: Branch = None,\n",
       "    requested_fields: dict[str, str] = None,\n",
       "    form: Any = None,\n",
       "    tools: bool = False,\n",
       "    invoke_tool: bool = True,\n",
       "    return_form: bool = True,\n",
       "    strict: bool = False,\n",
       "    rulebook: Any = None,\n",
       "    imodel: iModel = None,\n",
       "    clear_messages: bool = False,\n",
       "    use_annotation: bool = True,\n",
       "    retries: int = 3,\n",
       "    delay: float = 0,\n",
       "    backoff_factor: float = 1,\n",
       "    default: Any = None,\n",
       "    timeout: float = None,\n",
       "    timing: bool = False,\n",
       "    return_branch: bool = False,\n",
       "    **kwargs: Any\n",
       ") -> Any:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `instruction` (str, optional): Additional instruction to process.\n",
       "- `context` (Any, optional): Context to perform the instruction on.\n",
       "- `system` (str, optional): Optionally swap the system message.\n",
       "- `sender` (str, optional): Sender of the instruction, default is 'user'.\n",
       "- `recipient` (str, optional): Recipient of the instruction, default is 'branch.ln_id'.\n",
       "- `branch` (Branch, optional): The branch to use for the instruction.\n",
       "- `requested_fields` (dict[str, str], optional): Fields to request from the context.\n",
       "- `form` (Any, optional): Form to create instruction from, default is None.\n",
       "- `tools` (bool, optional): Tools to use, use True to consider all tools, no tools by default.\n",
       "- `invoke_tool` (bool, optional): Whether to invoke the tool when function calling, default is True.\n",
       "- `return_form` (bool, optional): Whether to return the form if a form is passed in, otherwise return a dict/str.\n",
       "- `strict` (bool, optional): Whether to strictly enforce rule validation, default is False.\n",
       "- `rulebook` (Any, optional): The rulebook to use for validation, default is None (uses default rulebook).\n",
       "- `imodel` (iModel, optional): Optionally swappable iModel for the commands, otherwise self.branch.imodel.\n",
       "- `clear_messages` (bool, optional): Whether to clear previous messages, default is False.\n",
       "- `use_annotation` (bool, optional): Whether to use annotation as rule qualifier, default is True (needs rulebook if False).\n",
       "- `retries` (int, optional): Number of retries if failed, default is 3.\n",
       "- `delay` (float, optional): Number of seconds to delay before retrying, default is 0.\n",
       "- `backoff_factor` (float, optional): Exponential backoff factor, default is 1 (no backoff).\n",
       "- `default` (Any, optional): Default value to return if all retries failed.\n",
       "- `timeout` (float, optional): Timeout for the call, default is None (no timeout).\n",
       "- `timing` (bool, optional): If True, will return a tuple (output, duration), default is False.\n",
       "- `return_branch` (bool, optional): Whether to return the branch after processing, default is False.\n",
       "- `**kwargs` (Any): Additional keyword arguments for further customization.\n",
       "\n",
       "**Return Values**:\n",
       "- `Any`: The result of the chat processing, format determined by the `return_form` parameter.\n",
       "\n",
       "**Exceptions Raised**:\n",
       "- `ValueError`: If an invalid combination of parameters is provided.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with instruction and context\n",
       "result = await self.chat(instruction='Hello', context={'data': 'example'})\n",
       "print(result)\n",
       "```\n",
       "\n",
       "### Method: `direct`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def direct(\n",
       "    self,\n",
       "    *,\n",
       "    instruction: str = None,\n",
       "    context: Any = None,\n",
       "    form: Any = None,\n",
       "    branch: Branch = None,\n",
       "    tools: bool = None,\n",
       "    return_branch: bool = False,\n",
       "    reason: bool = False,\n",
       "    predict: bool = False,\n",
       "    score: Any = None,\n",
       "    select: Any = None,\n",
       "    plan: Any = None,\n",
       "    allow_action: bool = False,\n",
       "    allow_extension: bool = False,\n",
       "    max_extension: int = None,\n",
       "    confidence: Any = None,\n",
       "    score_num_digits: int = None,\n",
       "    score_range: Any = None,\n",
       "    select_choices: Any = None,\n",
       "    plan_num_step: int = None,\n",
       "    predict_num_sentences: int = None,\n",
       "    imodel: iModel = None,\n",
       "    system: str = None,\n",
       "    rulebook: Any = None,\n",
       "    directive: Any = None,\n",
       "    **kwargs: Any\n",
       ") -> Any:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `instruction` (str, optional): Instruction to process.\n",
       "- `context` (Any, optional): Context to perform the instruction on.\n",
       "- `form` (Any, optional): Form to create instruction from, default is None.\n",
       "- `branch` (Branch, optional): The branch to use for the instruction.\n",
       "- `tools` (bool, optional): Tools to use, use True to consider all tools, no tools by default.\n",
       "- `return_branch` (bool, optional): Whether to return the branch after processing, default is False.\n",
       "- `reason` (bool, optional): Whether to provide reasoning, default is False.\n",
       "- `predict` (bool, optional): Whether to make predictions, default is False.\n",
       "- `score` (Any, optional): Score to use for evaluation.\n",
       "- `select` (Any, optional): Selection criteria.\n",
       "- `plan` (Any, optional): Plan to follow.\n",
       "- `allow_action` (bool, optional): Whether to allow actions, default is False.\n",
       "- `allow_extension` (bool, optional): Whether to allow extensions, default is False.\n",
       "- `max_extension` (int, optional): Maximum number of extensions, default is None.\n",
       "- `confidence` (Any, optional): Confidence level.\n",
       "- `score_num_digits` (int, optional): Number of digits for score.\n",
       "- `score_range` (Any, optional): Range for score.\n",
       "- `select_choices` (Any, optional): Choices for selection.\n",
       "- `plan_num_step` (int, optional): Number of steps in the plan.\n",
       "- `predict_num_sentences` (int, optional): Number of sentences for prediction.\n",
       "- `imodel` (iModel, optional): Optionally swappable iModel for the commands, otherwise self.branch.imodel.\n",
       "- `system` (str, optional): Optionally swap the system message.\n",
       "- `rulebook` (Any, optional): The rulebook to use for validation, default is None (uses default rulebook).\n",
       "- `directive` (Any, optional): Directive to follow.\n",
       "- `**kwargs` (Any): Additional keyword arguments for further customization.\n",
       "\n",
       "**Return Values**:\n",
       "- `Any`: The result of the direct processing.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with instruction and context\n",
       "result = await self.direct(instruction='Hello', context={'data': 'example'})\n",
       "print(result)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if hasattr(form, \"extension_forms\"):\n",
    "    for i in form.extension_forms:\n",
    "        i.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task invoked query engine 3 times.\n",
      "The 'strip_lower' function in the context of lionagi.libs.ln_convert is likely a utility function that processes a string by stripping any leading or trailing whitespace and converting the string to lowercase. This is inferred from its usage in the 'get_input_output_fields' function, where it is applied to each input and output field to ensure they are in a standardized format.\n",
      "In the context of chat interactions, the 'directive framework' refers to a structured system for managing and executing instructions or commands. It involves handling various aspects such as the sender and recipient of the instruction, the context in which the instruction is performed, and the tools or rulebooks used for validation and execution. This framework ensures that chat interactions are processed in an organized manner, allowing for additional instructions, context-specific actions, and the use of tools or models to achieve the desired outcomes.\n",
      "The 'Unit' class in the context of lionagi.core.unit is a specialized class that extends both the 'Directive' and 'DirectiveMixin' classes. It is designed to handle various operations related to directives within the system. The class initializes with parameters such as branch, imodel, template, and rulebook, and sets up a validator for rule enforcement. It also includes an asynchronous 'chat' method that facilitates communication and interaction, incorporating various parameters and options to customize its behavior.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The task invoked query engine {len(pile.query_response)} times.\")\n",
    "\n",
    "for i in pile.query_response:\n",
    "    print(i.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
