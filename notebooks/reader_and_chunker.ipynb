{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader and Chunker\n",
    "\n",
    "LionAGI offers a unified interface for loading and chunking documents that supports:\n",
    "\n",
    "- plain text files reading and chunking\n",
    "- `langchain` document loaders and text splitters\n",
    "- `llamaindex` readers and node parsers\n",
    "- self-defined readers and chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reader Types: [<ReaderType.PLAIN: 'plain'>, <ReaderType.LANGCHAIN: 'langchain'>, <ReaderType.LLAMAINDEX: 'llama_index'>, <ReaderType.SELFDEFINED: 'self_defined'>]\n",
      "Chunker Types: [<ChunkerType.PLAIN: 'plain'>, <ChunkerType.LANGCHAIN: 'langchain'>, <ChunkerType.LLAMAINDEX: 'llama_index'>, <ChunkerType.SELFDEFINED: 'self_defined'>]\n"
     ]
    }
   ],
   "source": [
    "from lionagi import ReaderType, ChunkerType\n",
    "print('Reader Types:', list(ReaderType))\n",
    "print('Chunker Types:', list(ChunkerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"txt_examples/essay.txt\"\n",
    "file_path = \"txt_examples\"\n",
    "\n",
    "with open('txt_examples/essay.txt') as f:\n",
    "    essay = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "load(reader: Union[str, Callable],\n",
    "     reader_type=ReaderType.PLAIN,\n",
    "     reader_args=[],\n",
    "     reader_kwargs={},\n",
    "     load_args=[],\n",
    "     load_kwargs={},\n",
    "     to_datanode: Union[bool, Callable] = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReaderType.PLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we only supports text_reader under PLAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext_reader(reader_args, reader_kwargs)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "text_reader(reader_args, reader_kwargs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes1 = load('text_reader', \n",
    "                  reader_args=[file_path, '.txt'], \n",
    "                  reader_kwargs={'recursive': False, 'flatten': True, 'clean_text':True})\n",
    "\n",
    "# {'recursive': False, 'flatten': True, 'clean_text':True} are default settings\n",
    "# if you do not wish to change, you can just call:\n",
    "# datanodes1 = load('text_reader', reader_args=[file_path, '.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReaderType.LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquivalent to:\\n\\nlangchain_loader = reader(*reader_args, **reader_kwargs)\\ndocs = langchain_loader.load()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equivalent to:\n",
    "\n",
    "langchain_loader = reader(*reader_args, **reader_kwargs)\n",
    "docs = langchain_loader.load()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes2 = load('TextLoader', \n",
    "                  reader_type='langchain', \n",
    "                  reader_args=[file])\n",
    "\n",
    "# equivalent to\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "datanodes2 =load(TextLoader, \n",
    "                 reader_type='langchain', \n",
    "                 reader_args=[file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep Langchain document type, add to_datanode=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReaderType.LLAMAINDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquivalent to:\\n\\nloader = reader(*reader_args, **reader_kwargs)\\ndocs = loader.load_data(*load_args, **load_kwargs)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equivalent to:\n",
    "\n",
    "loader = reader(*reader_args, **reader_kwargs)\n",
    "docs = loader.load_data(*load_args, **load_kwargs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes3 = load('SimpleDirectoryReader', \n",
    "                  reader_type='llama_index', \n",
    "                  reader_args=[file_path])\n",
    "\n",
    "# equivalent to\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "datanodes3 = load(SimpleDirectoryReader, \n",
    "                  reader_type='llama_index', \n",
    "                  reader_args=[file_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep LlamaIndex node type, add to_datanode=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReaderType.SELFDEFINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self defined reader classes with `load()` function are also supported. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class demo_reader:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "    \n",
    "    def load(self):\n",
    "        with open(self.file) as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes4 = load(demo_reader, \n",
    "                  reader_type='self_defined', \n",
    "                  reader_args=[file], \n",
    "                  to_datanode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse outputs to DataNodes, an explicitly defined parsing function is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import DataNode\n",
    "def to_datanode(content):\n",
    "    return DataNode(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes5 = load(demo_reader, \n",
    "                  reader_type='self_defined', \n",
    "                  reader_args=[file], \n",
    "                  to_datanode=to_datanode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import chunk, datanodes_convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "chunk(documents,\n",
    "      chunker,\n",
    "      chunker_type=ChunkerType.PLAIN,\n",
    "      chunker_args=[],\n",
    "      chunker_kwargs={},\n",
    "      chunking_kwargs={},\n",
    "      documents_convert_func=None,\n",
    "      to_datanode: Union[bool, Callable] = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document type needs to be supported by the selected chunker. If not, a converting function is needed in documents_convert_func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nformat:\\n# for langchain\\ndocuments = documents_convert_func(documents, 'langchain')\\n\\n# for llama_index\\ndocuments = documents_convert_func(documents, 'llama_index')\\n\\ndatanodes_convert: avaliable to convert from List[DataNode] to List[Document] (langchain) or List[TextNode] (llama_index)\\nfrom lionagi.loaders import datanodes_convert\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "format:\n",
    "# for langchain\n",
    "documents = documents_convert_func(documents, 'langchain')\n",
    "\n",
    "# for llama_index\n",
    "documents = documents_convert_func(documents, 'llama_index')\n",
    "\n",
    "datanodes_convert: avaliable to convert from List[DataNode] to List[Document] (langchain) or List[TextNode] (llama_index)\n",
    "from lionagi.loaders import datanodes_convert\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChunkerType.PLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we only supports text_chunker under PLAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext_chunker(documents, chunker_args, chunker_kwargs)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "text_chunker(documents, chunker_args, chunker_kwargs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes6 = chunk(datanodes1, \n",
    "                   chunker='text_chunker', \n",
    "                   chunker_kwargs={'field': 'content', 'chunk_size': 1500, 'overlap': 0.1, 'threshold':200})\n",
    "\n",
    "# {'field': 'content', 'chunk_size': 1500, 'overlap': 0.1, 'threshold':200} are default settings\n",
    "# if you do not wish to change, you can just call:\n",
    "# datanodes6 = chunk(datanodes1, chunker='text_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChunkerType.LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquivalent to:\\n\\n## if documents are strings\\nsplitter = chunker(*chunker_args, **chunker_kwargs)\\nsplitter.split_text(documents) \\n\\n## if documents are list of langchain Documents\\nsplitter = chunker(*chunker_args, **chunker_kwargs)\\nsplitter.split_documents(documents)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equivalent to:\n",
    "\n",
    "## if documents are strings\n",
    "splitter = chunker(*chunker_args, **chunker_kwargs)\n",
    "splitter.split_text(documents) \n",
    "\n",
    "## if documents are list of langchain Documents\n",
    "splitter = chunker(*chunker_args, **chunker_kwargs)\n",
    "splitter.split_documents(documents)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes7 = chunk(essay, \n",
    "                   'CharacterTextSplitter', \n",
    "                   chunker_type='langchain', \n",
    "                   chunker_kwargs={'chunk_size':200, 'chunk_overlap': 20, 'length_function': len})\n",
    "\n",
    "# equivalent to\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "datanodes7 = chunk(essay, \n",
    "                   CharacterTextSplitter, \n",
    "                   chunker_type='langchain', \n",
    "                   chunker_kwargs={'chunk_size':200, 'chunk_overlap': 20, 'length_function': len})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep Langchain document type, add to_datanode=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChunkerType.LLAMAINDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquivalent to:\\n\\nnode_parser = chunker(*chunker_args, **chunker_kwargs)\\nnodes = node_parser.get_nodes_from_documents(documents, **chunking_kwargs)\\n\\nor\\n\\nnode_parser = chunker.from_defaults(*chunker_args, **chunker_kwargs)\\nnodes = node_parser.get_nodes_from_documents(documents, **chunking_kwargs)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equivalent to:\n",
    "\n",
    "node_parser = chunker(*chunker_args, **chunker_kwargs)\n",
    "nodes = node_parser.get_nodes_from_documents(documents, **chunking_kwargs)\n",
    "\n",
    "or\n",
    "\n",
    "node_parser = chunker.from_defaults(*chunker_args, **chunker_kwargs)\n",
    "nodes = node_parser.get_nodes_from_documents(documents, **chunking_kwargs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnodes = load('SimpleDirectoryReader', reader_type = 'llama_index', reader_args=[file_path], to_datanode=False)\n",
    "datanodes8 = chunk(tnodes, \n",
    "                   'SentenceSplitter', \n",
    "                   chunker_type='llama_index')\n",
    "\n",
    "# equivalent to\n",
    "\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "datanodes8 = chunk(tnodes, \n",
    "                   SentenceSplitter, \n",
    "                   chunker_type='llama_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep LlamaIndex node type, add to_datanode=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChunkerType.SELFDEFINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self defined reader classes with `split()` function are also supported. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class demo_chunker:\n",
    "    def __init__(self, seperator):\n",
    "        self.seperator = seperator\n",
    "    \n",
    "    def split(self, content):\n",
    "        return content.split(self.seperator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk(essay, \n",
    "               demo_chunker, \n",
    "               chunker_type = 'self_defined', \n",
    "               chunker_args=['.'], \n",
    "               to_datanode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse outputs to DataNodes, an explicitly defined parsing function is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import DataNode\n",
    "def to_datanode(content):\n",
    "    dlist = []\n",
    "    for c in content:\n",
    "        dlist.append(DataNode(content=c))\n",
    "    return dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanodes9 = chunk(essay,\n",
    "                   demo_chunker, \n",
    "                   chunker_type = 'self_defined', \n",
    "                   chunker_args=['.'], \n",
    "                   to_datanode=to_datanode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
