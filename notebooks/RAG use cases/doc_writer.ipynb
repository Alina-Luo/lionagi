{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path.cwd() / \"lionagi_data\"  # Path to the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare QA Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load/chunk data\n",
    "\n",
    "docs = li.load(\n",
    "    input_dir=data_path, recursive=True, required_exts=[\".py\"], to_lion=False\n",
    ")\n",
    "\n",
    "docs = [i for i in docs if len(i.text) > 100]\n",
    "\n",
    "# chunks = li.chunk(\n",
    "#     docs, chunker = \"CodeSplitter\", chunker_type = \"llama_index\",\n",
    "#     to_lion=False,\n",
    "#     chunker_kwargs = {\n",
    "#         \"language\": \"python\",\n",
    "#         \"chunk_lines\": 100,\n",
    "#         \"chunk_lines_overlap\": 10,\n",
    "#         \"max_chars\": 2000,},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=OpenAIEmbeddingModelType.TEXT_EMBED_3_LARGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# index = VectorStoreIndex(chunks)\n",
    "# index.storage_context.persist(persist_dir=\"./lionagi_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "\n",
    "index_id = \"91fe61e0-89b5-4202-acff-435707e60119\"\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./lionagi_index\")\n",
    "index = load_index_from_storage(storage_context, index_id=index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "reranker = LLMRerank(choice_batch_size=10, top_n=5)\n",
    "query_engine = index.as_query_engine(node_postprocessors=[reranker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_codes_responses = []\n",
    "\n",
    "\n",
    "async def query_codebase(query):\n",
    "    \"\"\"\n",
    "    Perform a query to a QA bot with access to a vector index built with package lionagi codebase\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for in the LionAGI codebase.\n",
    "\n",
    "    Returns:\n",
    "        str: The string representation of the response content from the codebase query.\n",
    "    \"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    source_codes_responses.append(response)\n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "write a good API documentation for this code, make sure you use query \n",
    "engine to check meanings of code concepts to accurately describe them, \n",
    "must integrate the information from query engine to verify the correctness \n",
    "of the documentation.\n",
    "\"\"\"\n",
    "\n",
    "edit = \"\"\"\n",
    "you asked a lot of good questions and got plenty answers, please integrate your \n",
    "conversation, be a lot more technical, you will be rewarded with 500 dollars for \n",
    "great work, and punished for subpar work, take a deep breath, you can do it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PROMPTS import sys_prompt  # put your system prompt here\n",
    "\n",
    "tools = li.func_to_tool(query_codebase)\n",
    "\n",
    "model = li.iModel(\n",
    "    model=\"gpt-4o\",\n",
    "    provider=\"openai\",\n",
    "    interval_tokens=5_000_000,\n",
    "    interval_requests=5_000,\n",
    "    interval=60,\n",
    ")\n",
    "\n",
    "\n",
    "@li.cd.max_concurrency(20)\n",
    "@li.cd.default((None, None))\n",
    "async def write_doc(context):\n",
    "    branch = li.Branch(system=sys_prompt, tools=[query_codebase], imodel=model)\n",
    "\n",
    "    form = await branch.direct(\n",
    "        instruction=instruction,\n",
    "        context=context,\n",
    "        reason=True,\n",
    "        score=True,\n",
    "        action_allowed=True,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    final_doc = await branch.chat(\n",
    "        instruction=edit,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    form.add_field(\"final_documentation\", final_doc)\n",
    "\n",
    "    # save all messages into a unique file\n",
    "    df = branch.to_df()\n",
    "    df.to_csv(f\"lion_doc_{branch.ln_id[:8]}.csv\", index=False)\n",
    "\n",
    "    return form, branch\n",
    "\n",
    "\n",
    "contexts = [i.text for i in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the workflow across all contexts with a maximum of 20 concurrent processes\n",
    "results = await li.alcall(contexts, write_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = [i[0] for i in results if i[0]]\n",
    "branches = [i[1] for i in results if i[1]]\n",
    "\n",
    "docs = [i.final_documentation for i in forms if i is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each document to a file\n",
    "for i, doc in enumerate(docs):\n",
    "    with open(f\"doc_{i}.txt\", \"w\") as f:\n",
    "        f.write(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
