{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG assisted Auto Developer \n",
    "-- with LionAGI, LlamaIndex, Autogen and OAI code interpreter\n",
    "\n",
    "\n",
    "Let us develop a dev bot that can \n",
    "- read and understand lionagi's existing codebase\n",
    "- QA with the codebase to clarify tasks\n",
    "- produce and tests pure python codes with code interpreter with automatic followup if quality is less than expected\n",
    "- output final runnable python codes \n",
    "\n",
    "This tutorial shows you how you can automatically produce high quality prototype and drafts codes customized for your own codebase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new folder, Download the notebook in it, and open in IDE.\n",
    "\n",
    "\n",
    "2. input OPENAI_API_KEY in a `.env` file under project directory\n",
    "\n",
    "\n",
    "3. download lionagi's source code, which is the whole folder `lionagi` under the root directory of lionagi Github Repo, (not the whole repo)\n",
    "\n",
    "\n",
    "4. rename the folder to `'lionagi_codes'` and put in the same directory as your notebook. \n",
    "\n",
    "5. we will use `lionagi`'s entire source codes, not documentations, as our data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext=[\".py\"]             # extension of files of interest\n",
    "data_dir = Path.cwd() / 'lionagi_codes'   # directory of source data - lionagi codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: only works with llama_index legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. define functions to interact with `llama_index` \n",
    "\n",
    "- define a index object creation function\n",
    "- define a `query_engine` object creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check import and will attempt to install for you\n",
    "from lionagi.util.import_util import ImportUtil\n",
    "\n",
    "ImportUtil.check_import('llama-index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to build index object\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "def get_index(chunks, persist_dir, persist=True):\n",
    "    llm = OpenAI(temperature=0.1, model=\"gpt-4-turbo-preview\")\n",
    "    embedding = OpenAIEmbedding(model='text-embedding-3-large')     # use the large embedding model for better performance\n",
    "    index_service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding)\n",
    "    \n",
    "    _index = VectorStoreIndex(\n",
    "        chunks, \n",
    "        include_embeddings=True, \n",
    "        service_context=index_service_context\n",
    "    )\n",
    "    if persist:\n",
    "        _index.storage_context.persist(persist_dir=persist_dir)\n",
    "    return _index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to build query engine with LLM rerank\n",
    "from llama_index.postprocessor import LLMRerank\n",
    "def get_query_engine(index_, temperature=0.1, model=\"gpt-4-turbo-preview\", rerank=True, **kwargs):\n",
    "    \n",
    "    llm = OpenAI(temperature=temperature, model=model)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    if rerank:\n",
    "        reranker = LLMRerank(\n",
    "            choice_batch_size=10,\n",
    "            top_n=5,\n",
    "            service_context=service_context,\n",
    "        )\n",
    "        kwargs.update({\"node_postprocessors\":[reranker]})\n",
    "        \n",
    "    query_engine = index_.as_query_engine(**kwargs)\n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Setup Autogen\n",
    "\n",
    "- define a function to return autogen `user_proxy` and `gpt_assistant`, \n",
    "- which will be used to create codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi.util.import_util import ImportUtil\n",
    "\n",
    "ImportUtil.check_import(\n",
    "    package_name = 'autogen', \n",
    "    module_name = 'agentchat', \n",
    "    pip_name = 'pyautogen'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder_instruction = \"\"\"\n",
    "    You are an expert at writing python codes. \n",
    "    1. Write pure python codes, and \n",
    "    2. run it to validate the codes\n",
    "    3. then return with the full implementation when the task is resolved and there is no problem. and add the word   TERMINATE\n",
    "    4. Reply FAILED if you cannot solve the problem.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n",
    "from autogen.agentchat import UserProxyAgent\n",
    "\n",
    "def get_autogen_coder():\n",
    "    config_list = autogen.config_list_from_json(\n",
    "        \"OAI_CONFIG_LIST\",\n",
    "        file_location=\".\",\n",
    "        filter_dict={\n",
    "            \"model\": [\"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-4\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-turbo-preview\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Initiate an agent equipped with code interpreter\n",
    "    gpt_assistant = GPTAssistantAgent(\n",
    "        name=\"Coder Assistant\",\n",
    "        llm_config={\n",
    "            \"assistant_id\": None,\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"type\": \"code_interpreter\"\n",
    "                }\n",
    "            ],\n",
    "            \"config_list\": config_list,\n",
    "        },\n",
    "        instructions=coder_instruction,\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    user_proxy = UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "        is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "        code_execution_config={\n",
    "            \"work_dir\": \"coding\",\n",
    "            \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
    "        },\n",
    "        human_input_mode=\"NEVER\"\n",
    "    )\n",
    "    \n",
    "    return gpt_assistant, user_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Create Tools\n",
    "\n",
    "- ingest data\n",
    "- create tools for query engine,\n",
    "- create tools for autgen coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c-1:  Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of v0.0.300, load and chunk functions are depreciated\n",
    "# please use llama_index or langchain directly \n",
    "# will be reintroduced in 0.0.301\n",
    "\n",
    "\n",
    "# # with lionagi interface using llama_index loaders to load and chunk codebase\n",
    "# load_config = {\n",
    "#     \"reader\": \"SimpleDirectoryReader\", \n",
    "#     \"reader_type\": \"llama_index\", \n",
    "#     \"reader_kwargs\": {\n",
    "#         \"required_exts\":ext,\n",
    "#         \"input_dir\": data_dir, \n",
    "#         \"recursive\": True\n",
    "#         }, \n",
    "#     \"to_datanode\": False\n",
    "# }\n",
    "\n",
    "# chunk_config = {\n",
    "#     \"chunker\": \"CodeSplitter\", \n",
    "#     \"chunker_type\": 'llama_index',\n",
    "#     \"chunker_kwargs\": {    \n",
    "#         \"language\":\"python\",\n",
    "#         \"chunk_lines\":200,  # lines per chunk\n",
    "#         \"chunk_lines_overlap\":10,  # lines overlap between chunks\n",
    "#         \"max_chars\":3000,  # max chars per chunk\n",
    "#     },\n",
    "#     \"to_datanode\": False\n",
    "# }\n",
    "\n",
    "# docs = li.load(**load_config)\n",
    "# chunks = li.chunk(docs, **chunk_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c-2: Create Query tool to QA codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = ''     # after you get the chunks using llama-index\n",
    "index = get_index(chunks, persist_dir='storage/lionagi_codes_index', persist=True)\n",
    "query_engine = get_query_engine(index, rerank=True)\n",
    "\n",
    "source_codes_responses = []\n",
    "\n",
    "# the queries will be answered in concurrently in parallel\n",
    "async def query_codebase(query):\n",
    "    \"\"\"\n",
    "    Perform a query to a QA bot with access to a vector index built with package lionagi codebase\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for in the LionAGI codebase.\n",
    "\n",
    "    Returns:\n",
    "        str: The string representation of the response content from the codebase query.\n",
    "    \"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    source_codes_responses.append(response)\n",
    "    return str(response.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 8c0432b4-4c3d-4ee9-9160-fd100f2fd89c\n",
      "Text: class Session:\n",
      "Score:  9.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'think step by step, explain what does session object do in details'\n",
    "a = await query_codebase(query)\n",
    "\n",
    "print(source_codes_responses[0].source_nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The `Session` object, as part of a larger application, appears to be designed to manage or facilitate a session within the context of the application's functionality. While the specific details of the `Session` class's methods and properties are not provided, we can infer its potential responsibilities and interactions within the application based on the imports and the few lines of code mentioned.\n",
       "\n",
       "1. **Environment Configuration**: The `load_dotenv()` function call at the beginning suggests that the `Session` class may rely on environment variables for configuration. This could include settings like API keys, database connections, or other external service configurations that are necessary for the session's operation.\n",
       "\n",
       "2. **OpenAI Service Integration**: The instantiation of `OpenAIService()` into the `OAIService` variable indicates that the `Session` object interacts with OpenAI services. This could be for a variety of purposes such as generating text, processing natural language, or any other AI-driven task that OpenAI's APIs offer.\n",
       "\n",
       "3. **Data Handling with Pandas**: The import of `pandas` suggests that the `Session` object may deal with data manipulation, analysis, or transformation tasks. Pandas is a powerful library for handling structured data, so the `Session` could be involved in preparing or processing data for the session's main functionality.\n",
       "\n",
       "4. **Type Annotations**: The import of various types from the `typing` module (e.g., `Any`, `List`, `Union`, `Dict`, `Optional`, `Callable`, `Tuple`) implies that the `Session` class is designed with type annotations in mind. This helps with code clarity and type checking, indicating a well-structured approach to defining the inputs and outputs of the class's methods.\n",
       "\n",
       "5. **Application Components Integration**: The imports from relative paths (e.g., `lionagi.schema`, `lionagi._services.oai`, `..messages.messages`, `..branch.branch`, `..branch.branch_manager`) suggest that the `Session` object interacts with various components of the application. This could include:\n",
       "   - Utilizing schemas defined in `lionagi.schema` for data validation or structure.\n",
       "   - Sending or receiving messages through components imported from `..messages.messages`.\n",
       "   - Managing or interacting with branches of a project or workflow, as suggested by the imports of `Branch` and `BranchManager`.\n",
       "\n",
       "Based on these observations, the `Session` object likely serves as a central part of the application, orchestrating interactions between different services (like OpenAI), managing data, and facilitating the flow of information between various components of the application. It could be responsible for initializing the environment, handling user sessions, processing data, and leveraging AI services to achieve its goals. However, without more detailed information about the methods and properties of the `Session` class, this analysis remains speculative."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c-3. Create Coder tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_assistant, user_proxy = '', ''\n",
    "coder_responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "surpress the messages from creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "gpt_assistant, user_proxy = get_autogen_coder();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_code_python(instruction):\n",
    "    \"\"\"\n",
    "    Continues an ongoing chat session. Give an instruction to a coding assistant to write pure python codes\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The follow-up instruction or query to send to the GPT assistant.\n",
    "\n",
    "    Returns:\n",
    "        str: The latest message received from the GPT assistant in response to the follow-up instruction.\n",
    "    \"\"\"\n",
    "    user_proxy.send(recipient=gpt_assistant, message=instruction)\n",
    "    coder_responses.append(gpt_assistant.chat_messages)\n",
    "    return str(gpt_assistant.last_message())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to Coder Assistant):\n",
      "\n",
      "here is a test, create a hello world in python\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder Assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The \"Hello, World!\" program in Python has been created and successfully run. Here is the full implementation:\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lion/anaconda3/envs/import_test1/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:838: UserWarning: No summary_method provided or summary_method is not supported: \n",
      "  warnings.warn(\"No summary_method provided or summary_method is not supported: \")\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    gpt_assistant, \n",
    "    message='here is a test, create a hello world in python'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Put into `lionagi.Tool` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li\n",
    "\n",
    "funcs = [query_codebase, continue_code_python]\n",
    "tools = li.lcall(funcs, li.func_to_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"query_codebase\",\n",
      "        \"description\": \"Perform a query to a QA bot with access to a vector index built with package lionagi codebase\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"query\": {\n",
      "                    \"type\": \"string\",\n",
      "                    \"description\": \"The query string to search for in the LionAGI codebase.\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\n",
      "                \"query\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(li.to_readable_dict(tools[0].schema_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Draft Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = {\n",
    "    \"persona\": \"A helpful software engineer\",\n",
    "    \"requirements\": \"\"\"\n",
    "        Think step-by-step and provide thoughtful, clear, precise answers. \n",
    "        Maintain a humble yet confident tone.\n",
    "    \"\"\",\n",
    "    \"responsibilities\": \"\"\"\n",
    "        Assist with coding in the lionagi Python package.\n",
    "    \"\"\",\n",
    "    \"tools\": \"\"\"\n",
    "        Use a QA bot for grounding responses and a coding assistant \n",
    "        for writing pure Python code.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "function_call1 = {\n",
    "    \"notice\": \"\"\"\n",
    "        Use the QA bot tool at least 2 times at each task step, \n",
    "        identified by the step number. You can ask up to 3 questions \n",
    "        each time. This bot can query source codes with natural language \n",
    "        questions and provides natural language answers. Decide when to \n",
    "        invoke function calls. You have to ask the bot for clarifications \n",
    "        or additional information as needed, up to ten times if necessary.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "function_call2 = {\n",
    "    \"notice\": \"\"\"\n",
    "        Use the coding assistant tool at least once at each task step, \n",
    "        and again if a previous run failed. This assistant can write \n",
    "        and run Python code in a sandbox environment, responding to \n",
    "        natural language instructions with 'success' or 'failed'. Provide \n",
    "        clear, detailed instructions for AI-based coding assistance. \n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Step 1: Understanding User Requirements\n",
    "instruct1 = {\n",
    "    \"task_step\": \"1\",\n",
    "    \"task_name\": \"Understand User Requirements\",\n",
    "    \"task_objective\": \"Comprehend user-provided task fully\",\n",
    "    \"task_description\": \"\"\"\n",
    "        Analyze and understand the user's task. Develop plans \n",
    "        for approach and delivery. \n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Step 2: Proposing a Pure Python Solution\n",
    "instruct2 = {\n",
    "    \"task_step\": \"2\",\n",
    "    \"task_name\": \"Propose a Pure Python Solution\",\n",
    "    \"task_objective\": \"Develop a detailed pure Python solution\",\n",
    "    \"task_description\": \"\"\"\n",
    "        Customize the coding task for lionagi package requirements. \n",
    "        Use a QA bot for clarifications. Focus on functionalities \n",
    "        and coding logic. Add lots more details here for \n",
    "        more finetuned specifications. If the assistant's work is not\n",
    "        of sufficient quality, rerun the assistant tool\n",
    "    \"\"\",\n",
    "    \"function_call\": function_call1\n",
    "}\n",
    "\n",
    "\n",
    "# Step 3: Writing Pure Python Code\n",
    "instruct3 = {\n",
    "    \"task_step\": \"3\",\n",
    "    \"task_name\": \"Write Pure Python Code\",\n",
    "    \"task_objective\": \"Give detailed instruction to a coding bot\",\n",
    "    \"task_description\": \"\"\"\n",
    "        Instruct the coding assistant to write executable Python code \n",
    "        based on improved task understanding. The bot doesn't know\n",
    "        the previous conversation, so you need to integrate the conversation\n",
    "        and give detailed instructions. You cannot just say things like, \n",
    "        as previsouly described. You must give detailedm instruction such\n",
    "        that a bot can write it. Provide a complete, well-structured full \n",
    "        implementation if successful. If failed, rerun, report 'Task failed' \n",
    "        and the most recent code attempt after a second failure. If the \n",
    "        assistant's work is not of sufficient quality, rerun the assistant tool\n",
    "    \"\"\",\n",
    "    \"function_call\": function_call2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = {\n",
    "    \"create template class hierarchy\":\"\"\"\n",
    "        currently lionagi intakes string or dictionary for instruction\n",
    "        and context inputs, but as workflow gets complex, it is not \n",
    "        convinient, basing on lionagi's codebase, propose and implement\n",
    "        a suitable template class hierarchy. \n",
    "            1. return the fully implemented class hierarchy\n",
    "            2. google style docstring \n",
    "            3. pep-8 linting\n",
    "            4. add type hints\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def solve_in_python(context):\n",
    "    \n",
    "    coder = li.Session(system, tools=tools)\n",
    "    \n",
    "    await coder.chat(instruct1, context=context, temperature=0.4)\n",
    "    \n",
    "    # instruction 2: clarify needs according to codebase\n",
    "    await coder.auto_followup(\n",
    "        instruct2, max_followup=5, temperature=0.4, tools=tools[0])\n",
    "    \n",
    "    # use OpenAI code Interpreter to write pure python solution\n",
    "    await coder.auto_followup(\n",
    "        instruct3, max_followup=3, temperature=0.4, tools=tools[1])\n",
    "    \n",
    "    # save to csv\n",
    "    coder.to_csv()\n",
    "    coder.log_to_csv()\n",
    "    \n",
    "    return coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to Coder Assistant):\n",
      "\n",
      "Could you please write a Python code implementation that defines a class hierarchy for handling instructions and contexts in a more structured way? The hierarchy should include:\n",
      "\n",
      "1. Abstract base classes named `InstructionBase` and `ContextBase`. Each of these classes should have at least one abstract method. For `InstructionBase`, define an abstract method `execute` that when implemented, should execute the instruction. For `ContextBase`, define an abstract method `get_context` that when implemented, should return the context data as a dictionary.\n",
      "\n",
      "2. Concrete classes named `StringInstruction` and `DictInstruction` that inherit from `InstructionBase`. `StringInstruction` should accept a string as an instruction and return it upon execution. `DictInstruction` should accept a dictionary as an instruction and return it upon execution.\n",
      "\n",
      "3. A concrete class named `ComplexWorkflowContext` that inherits from `ContextBase`. It should accept a dictionary representing context data and return it when `get_context` is called.\n",
      "\n",
      "4. Ensure that all classes include Google style docstrings explaining their purpose and usage. Also, add type hints to improve code readability and assist with type checking.\n",
      "\n",
      "5. The code should follow PEP-8 standards for Python code styling.\n",
      "\n",
      "This implementation will help in managing complex workflows more efficiently in the `lionagi` package by providing a structured approach to handling instructions and contexts.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder Assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The Python code implementation that defines a class hierarchy for handling instructions and contexts in a structured way has been created. The hierarchy includes the following:\n",
      "\n",
      "- Abstract base classes `InstructionBase` and `ContextBase` with abstract methods `execute` and `get_context` respectively.\n",
      "- Concrete classes `StringInstruction` and `DictInstruction` that inherit from `InstructionBase`. They return strings and dictionaries upon execution.\n",
      "- A concrete class `ComplexWorkflowContext` that inherits from `ContextBase`. It returns a dictionary representing context data.\n",
      "- Google style docstrings explaining the classes' purpose and usage have been included.\n",
      "- Type hints are added for better code readability and type checking.\n",
      "- The code conforms to PEP-8 standards for Python styling.\n",
      "\n",
      "Here is the full implementation with docstrings and type hints:\n",
      "\n",
      "```python\n",
      "from abc import ABC, abstractmethod\n",
      "from typing import Any, Dict\n",
      "\n",
      "# Abstract base classes\n",
      "class InstructionBase(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for all instructions.\n",
      "    \n",
      "    Subclasses must implement the `execute` method.\n",
      "    \"\"\"\n",
      "    \n",
      "    @abstractmethod\n",
      "    def execute(self) -> Any:\n",
      "        \"\"\"\n",
      "        Execute the instruction.\n",
      "        \n",
      "        Returns:\n",
      "            The result of executing the instruction.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "class ContextBase(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for all contexts.\n",
      "    \n",
      "    Subclasses must implement the `get_context` method.\n",
      "    \"\"\"\n",
      "    \n",
      "    @abstractmethod\n",
      "    def get_context(self) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Get the context data.\n",
      "        \n",
      "        Returns:\n",
      "            A dictionary representing the context data.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "# Concrete instruction classes\n",
      "class StringInstruction(InstructionBase):\n",
      "    \"\"\"\n",
      "    Represents an instruction in the form of a string.\n",
      "    \n",
      "    Args:\n",
      "        instruction (str): The instruction to be executed.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, instruction: str):\n",
      "        self.instruction = instruction\n",
      "    \n",
      "    def execute(self) -> str:\n",
      "        \"\"\"\n",
      "        Return the instruction string.\n",
      "        \n",
      "        Returns:\n",
      "            The string representing the instruction.\n",
      "        \"\"\"\n",
      "        return self.instruction\n",
      "\n",
      "class DictInstruction(InstructionBase):\n",
      "    \"\"\"\n",
      "    Represents an instruction in the form of a dictionary.\n",
      "    \n",
      "    Args:\n",
      "        instruction (Dict[str, Any]): The instruction to be executed.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, instruction: Dict[str, Any]):\n",
      "        self.instruction = instruction\n",
      "    \n",
      "    def execute(self) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Return the instruction dictionary.\n",
      "        \n",
      "        Returns:\n",
      "            The dictionary representing the instruction.\n",
      "        \"\"\"\n",
      "        return self.instruction\n",
      "\n",
      "# Concrete context class\n",
      "class ComplexWorkflowContext(ContextBase):\n",
      "    \"\"\"\n",
      "    Represents a context for complex workflows.\n",
      "    \n",
      "    Args:\n",
      "        context_data (Dict[str, Any]): A dictionary representing the context data.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, context_data: Dict[str, Any]):\n",
      "        self.context_data = context_data\n",
      "    \n",
      "    def get_context(self) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Return the context data.\n",
      "        \n",
      "        Returns:\n",
      "            The context data as a dictionary.\n",
      "        \"\"\"\n",
      "        return self.context_data\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "coder = await solve_in_python(issue, num=5, filename=\"engineer_msgs3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Abstract base classes\n",
    "class InstructionBase(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all instructions.\n",
    "    \n",
    "    Subclasses must implement the `execute` method.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self) -> Any:\n",
    "        \"\"\"\n",
    "        Execute the instruction.\n",
    "        \n",
    "        Returns:\n",
    "            The result of executing the instruction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class ContextBase(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all contexts.\n",
    "    \n",
    "    Subclasses must implement the `get_context` method.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_context(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the context data.\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary representing the context data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Concrete instruction classes\n",
    "class StringInstruction(InstructionBase):\n",
    "    \"\"\"\n",
    "    Represents an instruction in the form of a string.\n",
    "    \n",
    "    Args:\n",
    "        instruction (str): The instruction to be executed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, instruction: str):\n",
    "        self.instruction = instruction\n",
    "    \n",
    "    def execute(self) -> str:\n",
    "        \"\"\"\n",
    "        Return the instruction string.\n",
    "        \n",
    "        Returns:\n",
    "            The string representing the instruction.\n",
    "        \"\"\"\n",
    "        return self.instruction\n",
    "\n",
    "class DictInstruction(InstructionBase):\n",
    "    \"\"\"\n",
    "    Represents an instruction in the form of a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        instruction (Dict[str, Any]): The instruction to be executed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, instruction: Dict[str, Any]):\n",
    "        self.instruction = instruction\n",
    "    \n",
    "    def execute(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return the instruction dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            The dictionary representing the instruction.\n",
    "        \"\"\"\n",
    "        return self.instruction\n",
    "\n",
    "# Concrete context class\n",
    "class ComplexWorkflowContext(ContextBase):\n",
    "    \"\"\"\n",
    "    Represents a context for complex workflows.\n",
    "    \n",
    "    Args:\n",
    "        context_data (Dict[str, Any]): A dictionary representing the context data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context_data: Dict[str, Any]):\n",
    "        self.context_data = context_data\n",
    "    \n",
    "    def get_context(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return the context data.\n",
    "        \n",
    "        Returns:\n",
    "            The context data as a dictionary.\n",
    "        \"\"\"\n",
    "        return self.context_data\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
