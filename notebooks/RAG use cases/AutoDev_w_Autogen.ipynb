{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG assisted Auto Developer \n",
    "-- with LionAGI, LlamaIndex, Autogen and OAI code interpreter\n",
    "\n",
    "\n",
    "Let us develop a dev bot that can \n",
    "- read and understand lionagi's existing codebase\n",
    "- QA with the codebase to clarify tasks\n",
    "- produce and tests pure python codes with code interpreter with automatic followup if quality is less than expected\n",
    "- output final runnable python codes \n",
    "\n",
    "This tutorial shows you how you can automatically produce high quality prototype and drafts codes customized for your own codebase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --force-reinstall lionagi llama-index pyautogen tree_sitter tree_sitter_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new folder, Download the notebook in it, and open in IDE.\n",
    "\n",
    "\n",
    "2. input OPENAI_API_KEY in a `.env` file under project directory\n",
    "\n",
    "\n",
    "3. download lionagi's source code, which is the whole folder `lionagi` under the root directory of lionagi Github Repo, (not the whole repo)\n",
    "\n",
    "\n",
    "4. rename the folder to `'lionagi_codes'` and put in the same directory as your notebook. \n",
    "\n",
    "5. we will use `lionagi`'s entire source codes, not documentations, as our data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = [\".py\"]  # extension of files of interest\n",
    "data_dir = Path.cwd() / \"lionagi_codes\"  # directory of source data - lionagi codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.23.post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index.core\n",
    "\n",
    "llama_index.core.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "- llamaIndex query engine\n",
    "- autogen coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. LlamaIndex query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-turbo-preview\")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=OpenAIEmbeddingModelType.TEXT_EMBED_3_LARGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# from llama_index.core.text_splitter import CodeSplitter\n",
    "\n",
    "# code_splitter = CodeSplitter(\n",
    "#     language=\"python\",\n",
    "#     chunk_lines=100,\n",
    "#     chunk_lines_overlap=10,\n",
    "#     max_chars=1500\n",
    "# )\n",
    "\n",
    "# reader = SimpleDirectoryReader(data_dir, required_exts=['.py'], recursive=True)\n",
    "# documents = reader.load_data()\n",
    "# nodes = code_splitter.get_nodes_from_documents(documents);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# index = VectorStoreIndex(nodes)\n",
    "# index.storage_context.persist(persist_dir=\"./lionagi_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build from Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "\n",
    "index_id = \"759f925b-5c71-4f59-a11f-4c018feb9e0f\"\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./lionagi_index\")\n",
    "index = load_index_from_storage(storage_context, index_id=index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "reranker = LLMRerank(choice_batch_size=10, top_n=5)\n",
    "query_engine = index.as_query_engine(node_postprocessors=[reranker])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"think step by step, explain what does BaseAgent object do in details, also explain its relationship with executable branch\"\n",
    "response = await query_engine.aquery(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The `BaseAgent` object, as defined in the provided code, is a complex entity designed to manage and execute tasks within a given structure, alongside an executable object, and potentially parse the output of these tasks. Here's a detailed breakdown of its functionality and its relationship with the executable branch:\n",
       "\n",
       "1. **Initialization (`__init__` method)**: When a `BaseAgent` object is instantiated, it initializes itself with a given structure, an executable object, and an optional output parser. It also creates a `StartMail` object and a `MailManager` object. The `MailManager` is initialized with a list containing the structure, the executable object, and the `StartMail` object. This setup suggests that the agent is designed to manage communications or tasks involving these components.\n",
       "\n",
       "2. **Mail Manager Control (`mail_manager_control` method)**: This asynchronous method continuously checks if either the structure or the executable object has been instructed to stop executing. If either is set to stop, the method then instructs the `MailManager` to also stop. This loop runs at a specified refresh time interval, ensuring that the agent can respond promptly to stop commands.\n",
       "\n",
       "3. **Execution (`execute` method)**: The execution flow begins by setting the `start_context` with a provided context and triggering the `start` method of the `StartMail` object with the context and identifiers for the structure and executable. It then asynchronously calls multiple execute methods (for the structure, the executable, the mail manager, and the mail manager control) with a specified delay. After these calls, it resets the stop flags for the structure, executable, and mail manager to `False`, indicating they are ready for further operations. If an output parser is defined, it processes the agent itself as its input and returns the result.\n",
       "\n",
       "4. **Relationship with the Executable Branch**: The executable branch seems to interact with the `BaseAgent` in a couple of ways, primarily through the `_agent_process` method. This method takes an agent (presumably a `BaseAgent` instance) and a verbosity flag as inputs. It then executes the agent with a given context (responses), optionally prints verbose messages, and appends the result of the agent's execution to its responses. This suggests that the executable branch is responsible for managing or orchestrating the execution of agents, possibly handling their outputs and integrating them into a larger workflow or process. The `_process_start` and `_process_end` methods in the executable branch indicate it also manages the lifecycle of tasks or operations, initiating them with a start mail and concluding them upon receiving an end mail, further integrating with the agent's communication or task management mechanisms.\n",
       "\n",
       "In summary, the `BaseAgent` is designed as a central component in a system that manages and executes tasks, communicates through mails, and potentially processes the results of these tasks. Its relationship with the executable branch indicates a collaborative interaction where the branch orchestrates the execution of agents, manages their lifecycle, and integrates their outputs into broader processes or workflows."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 14a70fdf-9ab7-4171-a28e-19f3f0e18467\n",
      "Text: class BaseAgent(BaseRelatableNode):     def __init__(self,\n",
      "structure, executable_obj, output_parser=None) -> None:\n",
      "super().__init__()         self.structure = structure\n",
      "self.executable = executable_obj         self.start = StartMail()\n",
      "self.mailManager = MailManager([self.structure, self.executable,\n",
      "self.start])         s...\n",
      "Score:  10.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(response.response))\n",
    "print(response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Setup Autogen\n",
    "\n",
    "- define a function to return autogen `user_proxy` and `gpt_assistant`, \n",
    "- which will be used to create codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder_instruction = \"\"\"\n",
    "You are an expert at writing python codes. \n",
    "1. Write pure python codes, and \n",
    "2. run it to validate the codes\n",
    "3. then return with the full implementation when the task is resolved and there is no problem. and add the word   TERMINATE\n",
    "4. Reply FAILED if you cannot solve the problem.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n",
    "from autogen.agentchat import UserProxyAgent\n",
    "\n",
    "\n",
    "def get_autogen_coder():\n",
    "    config_list = autogen.config_list_from_json(\n",
    "        \"OAI_CONFIG_LIST\",\n",
    "        file_location=\".\",\n",
    "        filter_dict={\n",
    "            \"model\": [\n",
    "                \"gpt-4-turbo-preview\",\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Initiate an agent equipped with code interpreter\n",
    "    gpt_assistant = GPTAssistantAgent(\n",
    "        name=\"Coder Assistant\",\n",
    "        llm_config={\n",
    "            \"assistant_id\": None,\n",
    "            \"tools\": [{\"type\": \"code_interpreter\"}],\n",
    "            \"config_list\": config_list,\n",
    "        },\n",
    "        instructions=coder_instruction,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    user_proxy = UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "        is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "        code_execution_config={\n",
    "            \"work_dir\": \"coding\",\n",
    "            \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
    "        },\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "    return gpt_assistant, user_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Create Tools\n",
    "\n",
    "- ingest data\n",
    "- create tools for query engine,\n",
    "- create tools for autgen coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_codes_responses = []\n",
    "\n",
    "\n",
    "async def query_codebase(query):\n",
    "    \"\"\"\n",
    "    Perform a query to a QA bot with access to a vector index built with package lionagi codebase\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for in the LionAGI codebase.\n",
    "\n",
    "    Returns:\n",
    "        str: The string representation of the response content from the codebase query.\n",
    "    \"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    source_codes_responses.append(response)\n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c-3. Create Coder tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_assistant, user_proxy = \"\", \"\"\n",
    "coder_responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "surpress the messages from creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "gpt_assistant, user_proxy = get_autogen_coder();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_code_python(instruction):\n",
    "    \"\"\"\n",
    "    Continues an ongoing chat session. Give an instruction to a coding assistant to write pure python codes\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The follow-up instruction or query to send to the GPT assistant.\n",
    "\n",
    "    Returns:\n",
    "        str: The latest message received from the GPT assistant in response to the follow-up instruction.\n",
    "    \"\"\"\n",
    "    user_proxy.send(recipient=gpt_assistant, message=instruction)\n",
    "    coder_responses.append(gpt_assistant.chat_messages)\n",
    "    return str(gpt_assistant.last_message())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to Coder Assistant):\n",
      "\n",
      "here is a test, create a hello world in python\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder Assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Here is the Python code that prints \"Hello, World!\":\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    gpt_assistant, message=\"here is a test, create a hello world in python\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Put into `lionagi.Tool` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import func_to_tool\n",
    "\n",
    "tools = func_to_tool([query_codebase, continue_code_python])\n",
    "tools[0].content = source_codes_responses\n",
    "tools[1].content = coder_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Draft Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = {\n",
    "    \"persona\": \"A helpful software engineer\",\n",
    "    \"requirements\": \"\"\"\n",
    "        Think step-by-step and provide thoughtful, clear, precise answers. \n",
    "        Maintain a humble yet confident tone.\n",
    "    \"\"\",\n",
    "    \"responsibilities\": \"\"\"\n",
    "        Assist with coding in the lionagi Python package.\n",
    "    \"\"\",\n",
    "    \"tools\": \"\"\"\n",
    "        Use a QA bot for grounding responses and a coding assistant \n",
    "        for writing pure Python code.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "function_call1 = {\n",
    "    \"notice\": \"\"\"\n",
    "        Use the QA bot tool at least 2 times at each task step, \n",
    "        identified by the step number. You can ask up to 3 questions \n",
    "        each time. This bot can query source codes with natural language \n",
    "        questions and provides natural language answers. Decide when to \n",
    "        invoke function calls. You have to ask the bot for clarifications \n",
    "        or additional information as needed, up to ten times if necessary.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "function_call2 = {\n",
    "    \"notice\": \"\"\"\n",
    "        Use the coding assistant tool at least once at each task step, \n",
    "        and again if a previous run failed. This assistant can write \n",
    "        and run Python code in a sandbox environment, responding to \n",
    "        natural language instructions with 'success' or 'failed'. Provide \n",
    "        clear, detailed instructions for AI-based coding assistance. \n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Step 1: Understanding User Requirements\n",
    "instruct1 = {\n",
    "    \"task_step\": \"1\",\n",
    "    \"task_name\": \"Understand User Requirements\",\n",
    "    \"task_objective\": \"Comprehend user-provided task fully\",\n",
    "    \"task_description\": \"\"\"\n",
    "        Analyze and understand the user's task. Develop plans \n",
    "        for approach and delivery. \n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "# Step 2: Proposing a Pure Python Solution\n",
    "instruct2 = {\n",
    "    \"task_step\": \"2\",\n",
    "    \"task_name\": \"Propose a Pure Python Solution\",\n",
    "    \"task_objective\": \"Develop a detailed pure Python solution\",\n",
    "    \"task_description\": \"\"\"\n",
    "        Customize the coding task for lionagi package requirements. \n",
    "        Use a QA bot for clarifications. Focus on functionalities \n",
    "        and coding logic. Add lots more details here for \n",
    "        more finetuned specifications. If the assistant's work is not\n",
    "        of sufficient quality, rerun the assistant tool\n",
    "    \"\"\",\n",
    "    \"function_call\": function_call1,\n",
    "}\n",
    "\n",
    "\n",
    "# Step 3: Writing Pure Python Code\n",
    "instruct3 = {\n",
    "    \"task_step\": \"3\",\n",
    "    \"task_name\": \"Write Pure Python Code\",\n",
    "    \"task_objective\": \"Give detailed instruction to a coding bot\",\n",
    "    \"task_description\": \"\"\"\n",
    "        Instruct the coding assistant to write executable Python code \n",
    "        based on improved task understanding. The bot doesn't know\n",
    "        the previous conversation, so you need to integrate the conversation\n",
    "        and give detailed instructions. You cannot just say things like, \n",
    "        as previsouly described. You must give detailedm instruction such\n",
    "        that a bot can write it. Provide a complete, well-structured full \n",
    "        implementation if successful. If failed, rerun, report 'Task failed' \n",
    "        and the most recent code attempt after a second failure. If the \n",
    "        assistant's work is not of sufficient quality, rerun the assistant tool\n",
    "    \"\"\",\n",
    "    \"function_call\": function_call2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = {\n",
    "    \"create template class hierarchy\": \"\"\"\n",
    "        currently lionagi intakes string or dictionary for instruction\n",
    "        and context inputs, but as workflow gets complex, it is not \n",
    "        convinient, basing on lionagi's codebase, propose and implement\n",
    "        a suitable template class hierarchy. \n",
    "            1. return the fully implemented class hierarchy\n",
    "            2. google style docstring \n",
    "            3. pep-8 linting\n",
    "            4. add type hints\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import Session\n",
    "\n",
    "\n",
    "async def solve_in_python(context):\n",
    "\n",
    "    coder = Session(system, tools=tools)\n",
    "    coder.llmconfig.update({\"temperature\": 0.4})\n",
    "\n",
    "    await coder.chat(instruct1, context=context)\n",
    "\n",
    "    # instruction 2: clarify needs according to codebase\n",
    "    await coder.followup(instruct2, max_followup=5, tools=tools[0])\n",
    "\n",
    "    # use OpenAI code Interpreter to write pure python solution\n",
    "    await coder.followup(instruct3, max_followup=3, tools=tools[1])\n",
    "\n",
    "    # save to csv\n",
    "    coder.to_csv_file()\n",
    "    coder.log_to_csv()\n",
    "\n",
    "    return coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to Coder Assistant):\n",
      "\n",
      "Write a Python class hierarchy for managing instructions and contexts in a workflow system. The hierarchy should include:\n",
      "\n",
      "1. An abstract base class named `InstructionBase` with an abstract method `process` that does not take any arguments besides `self` and does not return anything.\n",
      "\n",
      "2. Two derived classes from `InstructionBase` named `StringInstruction` and `DictInstruction`. `StringInstruction` should have an `__init__` method that accepts a string `instruction` and stores it. `DictInstruction` should have an `__init__` method that accepts a dictionary `instruction` and stores it. Both classes should implement the `process` method, which can simply pass for now.\n",
      "\n",
      "3. An abstract base class named `ContextBase` with an abstract method `update_context` that accepts a new context as an argument and does not return anything.\n",
      "\n",
      "4. Two derived classes from `ContextBase` named `SimpleContext` and `ComplexContext`. `SimpleContext` should have an `__init__` method that accepts any type of context and stores it. `ComplexContext` should have an `__init__` method that accepts a dictionary as the context and stores it. Both classes should implement the `update_context` method, which can simply pass for now.\n",
      "\n",
      "5. A class named `WorkflowManager` with an `__init__` method that accepts a `ContextBase` instance. It should have a list to store `InstructionBase` instances, a method `add_instruction` to add an `InstructionBase` instance to the list, a method `set_context` to set the current context using a `ContextBase` instance, and a method `run` to iterate over the instructions list and call the `process` method on each.\n",
      "\n",
      "Ensure the code includes Google style docstrings for each class and method, adheres to PEP-8 standards, and includes type hints.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder Assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "All the requested classes have been defined successfully with the required methods and type hints, along with docstrings following the Google style. Each class in the hierarchy is meant to manage instructions and contexts in a workflow system as specified.\n",
      "\n",
      "Here's the full implementation:\n",
      "\n",
      "```python\n",
      "from abc import ABC, abstractmethod\n",
      "from typing import Any, Dict, List\n",
      "\n",
      "class InstructionBase(ABC):\n",
      "    \"\"\"Abstract base class for an instruction.\"\"\"\n",
      "    \n",
      "    @abstractmethod\n",
      "    def process(self) -> None:\n",
      "        \"\"\"Process the instruction.\"\"\"\n",
      "        pass\n",
      "        \n",
      "class StringInstruction(InstructionBase):\n",
      "    \"\"\"Represents an instruction based on a string.\"\"\"\n",
      "    \n",
      "    def __init__(self, instruction: str) -> None:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            instruction: The string based instruction.\n",
      "        \"\"\"\n",
      "        self.instruction = instruction\n",
      "        \n",
      "    def process(self) -> None:\n",
      "        \"\"\"Process the string instruction.\"\"\"\n",
      "        pass\n",
      "\n",
      "class DictInstruction(InstructionBase):\n",
      "    \"\"\"Represents an instruction based on a dictionary.\"\"\"\n",
      "    \n",
      "    def __init__(self, instruction: Dict[str, Any]) -> None:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            instruction: The dictionary based instruction.\n",
      "        \"\"\"\n",
      "        self.instruction = instruction\n",
      "        \n",
      "    def process(self) -> None:\n",
      "        \"\"\"Process the dictionary instruction.\"\"\"\n",
      "        pass\n",
      "\n",
      "class ContextBase(ABC):\n",
      "    \"\"\"Abstract base class for a context.\"\"\"\n",
      "    \n",
      "    @abstractmethod\n",
      "    def update_context(self, new_context: Any) -> None:\n",
      "        \"\"\"Update the current context.\n",
      "        \n",
      "        Args:\n",
      "            new_context: The new context to be updated.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "class SimpleContext(ContextBase):\n",
      "    \"\"\"Represents a simple context.\"\"\"\n",
      "    \n",
      "    def __init__(self, context: Any) -> None:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            context: The initially set context.\n",
      "        \"\"\"\n",
      "        self.context = context\n",
      "        \n",
      "    def update_context(self, new_context: Any) -> None:\n",
      "        \"\"\"Update the simple context.\"\"\"\n",
      "        pass\n",
      "\n",
      "class ComplexContext(ContextBase):\n",
      "    \"\"\"Represents a complex context based on a dictionary.\"\"\"\n",
      "    \n",
      "    def __init__(self, context: Dict[str, Any]) -> None:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            context: The initially set dictionary context.\n",
      "        \"\"\"\n",
      "        self.context = context\n",
      "        \n",
      "    def update_context(self, new_context: Dict[str, Any]) -> None:\n",
      "        \"\"\"Update the complex context.\"\"\"\n",
      "        pass\n",
      "        \n",
      "class WorkflowManager:\n",
      "    \"\"\"Manages workflows by handling instructions and contexts.\"\"\"\n",
      "    \n",
      "    def __init__(self, context: ContextBase) -> None:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            context: The initial context for the workflow.\n",
      "        \"\"\"\n",
      "        self.context = context\n",
      "        self.instructions: List[InstructionBase] = []\n",
      "\n",
      "    def add_instruction(self, instruction: InstructionBase) -> None:\n",
      "        \"\"\"Add an instruction to the workflow.\n",
      "        \n",
      "        Args:\n",
      "            instruction: The instruction to be added.\n",
      "        \"\"\"\n",
      "        self.instructions.append(instruction)\n",
      "        \n",
      "    def set_context(self, context: ContextBase) -> None:\n",
      "        \"\"\"Set the current context for the workflow.\n",
      "        \n",
      "        Args:\n",
      "            context: The new context to be set.\n",
      "        \"\"\"\n",
      "        self.context = context\n",
      "        \n",
      "    def run(self) -> None:\n",
      "        \"\"\"Run the workflow by processing each instruction.\"\"\"\n",
      "        for instruction in self.instructions:\n",
      "            instruction.process()\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "22 messages saved to data/logs/main_messages_20240325122433.csv\n",
      "9 logs saved to data/logs/main_log_20240325122433.csv\n"
     ]
    }
   ],
   "source": [
    "coder = await solve_in_python(issue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
