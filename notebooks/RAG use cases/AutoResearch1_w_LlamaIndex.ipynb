{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "    TODO: update this to new lionagi version\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior with LionAGI and LlamaIndex Vector Index\n",
    "\n",
    "-- how to do auto explorative research with LionAGI plus RAG using llamaindex Vector Index & embedding \n",
    "\n",
    "- [LionAGI](https://github.com/lion-agi/lionagi)\n",
    "- [LlamaIndex](https://www.llamaindex.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi pypdf llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: only works with llama_index legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build a Vector Index with llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get index\n",
    "\n",
    "def get_index(chunks):\n",
    "    from llama_index import ServiceContext, VectorStoreIndex\n",
    "    from llama_index.llms import OpenAI\n",
    "\n",
    "    llm = OpenAI(temperature=0.1, model=\"gpt-4-turbo-preview\")\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    return VectorStoreIndex(chunks, include_embeddings=True, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get llamaindex textnodes, if to_datanode is True, you will get Lion DataNode\n",
    "text_nodes = li.load(\n",
    "    'SimpleDirectoryReader', \n",
    "    reader_type='llama_index', \n",
    "    reader_args=['papers/'], \n",
    "    to_datanode=False,\n",
    ")\n",
    "\n",
    "chunks = li.chunk(\n",
    "    documents=text_nodes, \n",
    "    chunker_type = 'llama_index', \n",
    "    chunker='SentenceSplitter', \n",
    "    chunker_kwargs={'chunk_size': 512, 'chunk_overlap':20}, \n",
    "    to_datanode=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_index(chunks)\n",
    "query_engine = index.as_query_engine(include_text=False, response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tool description according to OpenAI schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "# you can create any tools suitable for openai by simply writing a \n",
    "# google style docstring\n",
    "\n",
    "def query_paper(query: str):\n",
    "    \"\"\"\n",
    "    Query a vector index built with papers. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = query_engine.query(query)\n",
    "    responses.append(response)\n",
    "    \n",
    "    return str(response.response)\n",
    "\n",
    "\n",
    "# create tool object\n",
    "tools = li.func_to_tool(query_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Research: PROMPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FORMATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = {\n",
    "    \"persona\": \"a helpful world-class researcher\",\n",
    "    \"requirements\": \"Provide clear, precise answers with a confident tone.\",\n",
    "    \"responsibilities\": \"Assist in researching the topic of {query}.\",\n",
    "    \"tools\": \"QA bot for grounding responses\"\n",
    "}\n",
    "\n",
    "deliver_format1 = {\"return required\": \"yes\", \"return format\": \"paragraph\"}\n",
    "\n",
    "deliver_format2 = {\n",
    "    \"return required\": \"yes\", \n",
    "    \"return format\": { \n",
    "        \"json_mode\": {\n",
    "            'paper': \"paper_name\",\n",
    "            \"summary\": \"...\", \n",
    "            \"research question\": \"...\", \n",
    "            \"talking points\": {\n",
    "                \"point 1\": \"...\", \n",
    "                \"point 2\": \"...\", \n",
    "                \"point 3\": \"...\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "function_call = {\n",
    "    \"notice\": \"\"\"\n",
    "        Use the QA bot tool at least twice at each task step. \n",
    "        The bot can query {num_papers} papers to provide natural \n",
    "        language answers.\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "instruct1 = {\n",
    "    \"task step\": \"1\", \n",
    "    \"task name\": \"read paper abstracts\", \n",
    "    \"task objective\": \"Understand the papers' core points \\\n",
    "                       from their abstracts.\", \n",
    "    \"deliverable\": deliver_format1\n",
    "}\n",
    "\n",
    "instruct2 = {\n",
    "    \"task step\": \"2\",\n",
    "    \"task name\": \"propose research questions and talking points\", \n",
    "    \"task objective\": \"Brainstorm research questions based on paper \\\n",
    "                       understanding.\", \n",
    "    \"deliverable\": deliver_format2,\n",
    "    \"function calling\": function_call\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Research: Setup Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the context of your choice\n",
    "context = \"\"\"\n",
    ".....\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def read_propose(context, num=5):\n",
    "    \n",
    "    researcher = li.Session(system)\n",
    "    researcher.register_tools(tools)\n",
    "    \n",
    "    await researcher.chat(instruct1, context=context, temperature=0.7)\n",
    "    await researcher.auto_followup(instruct2, tools=True, num=num)\n",
    "    \n",
    "    return researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Research: Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = li.to_list(\n",
    "    await li.alcall(context, read_propose), flatten=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>role</th>\n",
       "      <th>name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e3d8202ddcd80950b619664e6030566a</td>\n",
       "      <td>system</td>\n",
       "      <td>system</td>\n",
       "      <td>2024-01-18 13:31:27.516618</td>\n",
       "      <td>{\"system_info\": {\"persona\": \"a helpful world-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6f63a57c38f0382a40c21cb08d292b2</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-01-18 13:31:27.517302</td>\n",
       "      <td>{\"instruction\": {\"task step\": \"1\", \"task name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e1e291bd9ab5f3181a0b8d7519eb7ed</td>\n",
       "      <td>assistant</td>\n",
       "      <td>assistant</td>\n",
       "      <td>2024-01-18 13:31:44.000922</td>\n",
       "      <td>{\"response\": \"Certainly, the abstract provided...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93f6af19b2432c802d12a8f68285b903</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-01-18 13:31:44.002273</td>\n",
       "      <td>{\"instruction\": {\"task step\": \"2\", \"task name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0c395f94f6e27b2ae69d8814bcf16c30</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_request</td>\n",
       "      <td>2024-01-18 13:31:52.241974</td>\n",
       "      <td>{\"action_request\": [{\"action\": \"action_query_arxi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            node_id       role            name  \\\n",
       "0  e3d8202ddcd80950b619664e6030566a     system          system   \n",
       "1  d6f63a57c38f0382a40c21cb08d292b2       user            user   \n",
       "2  4e1e291bd9ab5f3181a0b8d7519eb7ed  assistant       assistant   \n",
       "3  93f6af19b2432c802d12a8f68285b903       user            user   \n",
       "4  0c395f94f6e27b2ae69d8814bcf16c30  assistant  action_request   \n",
       "\n",
       "                   timestamp  \\\n",
       "0 2024-01-18 13:31:27.516618   \n",
       "1 2024-01-18 13:31:27.517302   \n",
       "2 2024-01-18 13:31:44.000922   \n",
       "3 2024-01-18 13:31:44.002273   \n",
       "4 2024-01-18 13:31:52.241974   \n",
       "\n",
       "                                             content  \n",
       "0  {\"system_info\": {\"persona\": \"a helpful world-c...  \n",
       "1  {\"instruction\": {\"task step\": \"1\", \"task name\"...  \n",
       "2  {\"response\": \"Certainly, the abstract provided...  \n",
       "3  {\"instruction\": {\"task step\": \"2\", \"task name\"...  \n",
       "4  {\"action_request\": [{\"action\": \"action_query_arxi...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session.conversation is another name for session.current_branch\n",
    "df = researcher.default_branch.messages\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['system', 'user', 'assistant', 'action_request', 'action_response'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"action\": \"action_query_arxiv_papers\",\n",
      "    \"arguments\": \"{\\\"str_or_query_bundle\\\":\\\"What are the current challenges in integrating Knowledge Graphs with Large Language Models?\\\"}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# let us check the questions assistant asked\n",
    "df_requests = df[df.sender == \"action_request\"]\n",
    "\n",
    "for content in df_requests.content:\n",
    "    for i in li.as_dict(content)['action_request']:\n",
    "        print(li.to_readable_dict(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>role</th>\n",
       "      <th>name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c0699e2742950b636ad61a48650b8ea4</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-01-18 13:32:25.292728</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_arxiv_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            node_id       role             name  \\\n",
       "5  c0699e2742950b636ad61a48650b8ea4  assistant  action_response   \n",
       "\n",
       "                   timestamp  \\\n",
       "5 2024-01-18 13:32:25.292728   \n",
       "\n",
       "                                             content  \n",
       "5  {\"action_response\": {\"function\": \"query_arxiv_...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us check the answers from query engine\n",
    "df_response= df[df.sender == \"action_response\"]\n",
    "content = df_response.content.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Current challenges in integrating Knowledge Graphs with Large Language Models (LLMs) include:\n",
       "\n",
       "1. **Scalability**: As LLMs and knowledge graphs grow in size, it becomes increasingly difficult to efficiently integrate and update the vast amounts of information contained within them.\n",
       "\n",
       "2. **Alignment**: Ensuring that the knowledge graph's structured information aligns with the LLM's learned representations can be challenging, as LLMs may develop their own idiosyncratic understanding of concepts.\n",
       "\n",
       "3. **Dynamic Knowledge**: Knowledge graphs need to be constantly updated to reflect new information, but integrating these updates into an LLM that has been trained on a static snapshot of data can be problematic.\n",
       "\n",
       "4. **Reasoning and Inference**: While LLMs are adept at generating human-like text, they may struggle with logical reasoning or inference tasks that knowledge graphs can support. Bridging the gap between neural text generation and structured logical reasoning is a non-trivial challenge.\n",
       "\n",
       "5. **Contextual Understanding**: LLMs may not always effectively leverage the context provided by a knowledge graph, leading to responses that are factually incorrect or lack relevance.\n",
       "\n",
       "6. **Complex Queries**: Handling complex queries that require multi-hop reasoning over a knowledge graph is difficult, as it requires the LLM to maintain coherence over long text generations and to accurately access and apply relevant information from the graph.\n",
       "\n",
       "7. **Interpretability**: Ensuring that the integration of knowledge graphs into LLMs is interpretable and transparent is important for trust and reliability, but this remains a difficult task given the often opaque nature of neural network decision-making processes.\n",
       "\n",
       "8. **Data Quality and Bias**: The quality of the data in the knowledge graph can affect the performance of the LLM, and biases present in the data can propagate through the model, leading to biased outputs.\n",
       "\n",
       "Addressing these challenges requires ongoing research and development in the fields of machine learning, natural language processing, and knowledge representation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(li.as_dict(content)['action_response']['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us read the assistant's responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assistant = df[df.sender == \"assistant\"]\n",
    "len(df_assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly, the abstract provided outlines the interplay between Large Language Models (LLMs) like ChatGPT and GPT-4, and Knowledge Graphs (KGs) such as Wikipedia and Huapu. The core point of the paper is that while LLMs are powerful in processing natural language, they tend to lack in capturing and accessing concrete factual knowledge, which is where KGs excel. The paper's purpose is to explore ways to unify LLMs and KGs to harness their respective strengths. \n",
       "\n",
       "The authors propose a roadmap with three general frameworks for this unification: \n",
       "\n",
       "1. KG-enhanced LLMs, which integrate KGs into various stages of LLM development and usage, either to assist with pre-training and inference or to improve the LLMs' grasp of the knowledge they've learned.\n",
       "\n",
       "2. LLM-augmented KGs, in which LLMs are utilized to perform tasks related to KGs, including embedding, completion, construction, and more complex functions like graph-to-text generation and question answering.\n",
       "\n",
       "3. Synergized LLMs + KGs, a model where LLMs and KGs collaborate closely, providing mutual benefits and enabling bidirectional reasoning that incorporates both data and knowledge.\n",
       "\n",
       "The abstract concludes by reviewing existing efforts in these areas and suggesting future research directions, indicating that this is a forward-looking and potentially transformative approach to advancing the field of AI and natural language understanding."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first response corresponds to the first user instruction, which is to read through the abstract\n",
    "\n",
    "response1 = li.as_dict(df_assistant.content.iloc[0])['response']\n",
    "Markdown(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the improved understanding of the challenges in integrating Knowledge Graphs with Large Language Models, a research question that arises might be:\n",
       "\n",
       "**Research Question:** How can we develop an adaptive integration framework for LLMs and KGs that maintains the up-to-dateness of the knowledge graph while ensuring the scalability and alignment of the LLM?\n",
       "\n",
       "**Supporting Reasoning:** This question is practical as it addresses the dynamic nature of knowledge and the need for LLMs to continuously learn from updated information. It is unique in its focus on creating an adaptive framework that can handle the scalability issues that come with the ever-growing size of LLMs and KGs, as well as ensuring that the LLM's learned representations align with the structured information of the KG.\n",
       "\n",
       "**Talking Points:**\n",
       "- **Point 1:** Scalability and efficiency are major concerns as both LLMs and KGs grow; an adaptive framework could include mechanisms for incremental learning or modular updates that prevent the need for retraining from scratch.\n",
       "- **Point 2:** Alignment between the evolving representations of knowledge in LLMs and the structured format of KGs requires continuous synchronization methods, possibly utilizing advanced alignment algorithms or transfer learning techniques.\n",
       "- **Point 3:** Keeping the knowledge graph up-to-date in a way that the LLM can efficiently utilize is crucial; this might involve real-time updating mechanisms or periodic 'knowledge refreshes' that the LLM can integrate without compromising performance. \n",
       "\n",
       "To further clarify the potential of this research direction, I will invoke the function call once more to ask a follow-up question.\n",
       "\n",
       "{\"action_request\": [{\"action\": \"action_query_arxiv_papers\", \"arguments\": \"{\\\"str_or_query_bundle\\\":\\\"What are the latest approaches to ensuring the scalability and alignment of LLMs in the context of knowledge graph integration?\\\"}\"}]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second is the second instruciton, which is the final output in this case\n",
    "\n",
    "response2 = li.as_dict(df_assistant.content.iloc[1])['response']\n",
    "Markdown(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{dir}researcher1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
