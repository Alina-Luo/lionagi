{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior 2\n",
    "\n",
    "Auto-explorative research with **Retrieval Augumented Generation(RAG)** and **Reason-Action prompting (ReAct)**, using agent framework `lionagi` with `llama_index` as ToolBox\n",
    "\n",
    "let us conduct a research session. We will, \n",
    "- download 20 papers from arxiv as our primary inspiration\n",
    "- give our researcher 5 abstracts to **read** and propose some ideas to explore\n",
    "- ask researcher to **look up** relevant terms and information from **5 sources**\n",
    "- draft plans and points and present final research proposal \n",
    "\n",
    "\n",
    "**WARNING** : This notebook uses `gpt-4-turbo-preview` for workflow, and can get ***very expensive***\n",
    "\n",
    "You can:\n",
    "1. change workflow model, but that will require context length being managed\n",
    "2. Reduce the number of steps and the number of queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi llama-index llama_hub unstructured pypdf arxiv wikipedia google-search 'unstructured[pdf]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to ignore logging\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Large Language Model Applications in Blockchain\"\n",
    "question = \"Research on building a system of trust integrating Large Language Model with blockchain\"\n",
    "num_papers = 20\n",
    "\n",
    "persist_dir = \".storage/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: only works with llama_index legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Data QA Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. ArXiv Index\n",
    "\n",
    "We will download papers using our research topic from **ArXiv**, a popular pre-publish platform for research papers\n",
    "\n",
    "with `LlamaIndex` and build it into a searchable index.\n",
    "\n",
    "- [ArXiv Official Website](https://arxiv.org)\n",
    "\n",
    "- [LlamaIndex Official Website](https://www.llamaindex.ai)\n",
    "\n",
    "- [ArXivReader on LlamaHub](https://llamahub.ai/l/papers-arxiv?from=all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "from llama_index import  ServiceContext, VectorStoreIndex\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "## get arxiv papers using llamaindex ArxivReader\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "loader = ArxivReader()\n",
    "\n",
    "documents, abstracts = loader.load_papers_and_abstracts(\n",
    "    search_query=topic, max_results=num_papers)\n",
    "\n",
    "# set up service context for LlamaIndex  \n",
    "text_splitter = SentenceSplitter(chunk_size=2048, chunk_overlap=50)         # split the documents into chunks with whole sentences\n",
    "llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1)                  # set up the llm to be used\n",
    "embedding = OpenAIEmbedding(model='text-embedding-3-small')\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    text_splitter=text_splitter, llm=llm)\n",
    "\n",
    "arxiv_index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context)\n",
    "\n",
    "arxiv_engine = arxiv_index.as_query_engine(\n",
    "    similarity_top_k=3, response_mode= \"tree_summarize\")\n",
    "\n",
    "arxiv_index.storage_context.persist(f'{persist_dir}arxiv1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already built an index and stored it by running above codes, you can `index object` build from storage\n",
    "\n",
    "you can build from storage using the following, just need to find the `index_id` in the index_store file\n",
    "\n",
    "- but you **still need** have some abstract of paper or other things as the starting context for `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import StorageContext, load_index_from_storage\n",
    "# from llama_index.llms import OpenAI\n",
    "\n",
    "# llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1)\n",
    "# service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=f'{persist_dir}arxiv/')\n",
    "# # <change to your own index id, can find it in index store>\n",
    "# index_id = 'c3ce7613-5fc1-42d1-aa95-968e878ad2db'\n",
    "\n",
    "# arxiv_index = load_index_from_storage(\n",
    "#     storage_context=storage_context, \n",
    "#     index_id=index_id, \n",
    "#     service_context=service_context\n",
    "# )\n",
    "\n",
    "# arxiv_engine = arxiv_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Textbooks\n",
    "\n",
    "use a couple pdf textbooks as references and build a Knowledge Graph Index\n",
    "\n",
    "- [Dive into Deep Learning](https://d2l.ai)\n",
    "\n",
    "- [Blockchain for Dummies - IBM edition](http://gunkelweb.com/coms465/texts/ibm_blockchain.pdf)\n",
    "\n",
    "- [KnowledgeGraphIndex](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use unstructured reader for pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, Document\n",
    "\n",
    "UnstructuredReader = download_loader(\"UnstructuredReader\")\n",
    "loader = UnstructuredReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a couple free textbooks online as references \n",
    "\n",
    "book1 = \"d2l-en.pdf\"\n",
    "book2 = \"ibm_blockchain.pdf\"\n",
    "\n",
    "docs1 = loader.load_data(book1)\n",
    "docs2 = loader.load_data(book2)\n",
    "\n",
    "documents1 = [Document(text=\"\".join([x.text for x in docs1]))]\n",
    "documents2 = [Document(text=\"\".join([x.text for x in docs2]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will take quite a while, I suggest you to minimize the notebook for 0.5-1 hours, or change the source to be shorter in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, KnowledgeGraphIndex\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "\n",
    "llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1) \n",
    "embedding = OpenAIEmbedding(model='text-embedding-3-large')\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512, embed_model=embedding)\n",
    "storage_context = StorageContext.from_defaults(graph_store=SimpleGraphStore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents1,\n",
    "    max_triplets_per_chunk=2,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "d2l_engine = d2l_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")\n",
    "d2l_index.storage_context.persist(f'{persist_dir}d2l/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents2,\n",
    "    max_triplets_per_chunk=2,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "bc_engine = bc_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")\n",
    "bc_index.storage_context.persist(f'{persist_dir}bc_ibm/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import load_index_from_storage\n",
    "# from llama_index import ServiceContext\n",
    "# from llama_index.graph_stores import SimpleGraphStore\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# from llama_index.llms import OpenAI\n",
    "# from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "\n",
    "# llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1) \n",
    "# embedding = OpenAIEmbedding(model='text-embedding-3-large')\n",
    "# service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding)\n",
    "# storage_context = StorageContext.from_defaults(graph_store=SimpleGraphStore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context = StorageContext.from_defaults(\n",
    "#     graph_store=SimpleGraphStore(), persist_dir= f'{persist_dir}/d2l/')\n",
    "\n",
    "# # <change to your own index id, can find it in index store>\n",
    "# index_id = '7c52b76a-bd85-4aa8-a167-ab4f468b7dc9' \n",
    "\n",
    "# d2l_index = load_index_from_storage(storage_context=storage_context, index_id=index_id, service_context=service_context)\n",
    "# d2l_engine = d2l_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context = StorageContext.from_defaults(\n",
    "#     graph_store=SimpleGraphStore(), persist_dir= f'{persist_dir}/bc_ibm/')\n",
    "\n",
    "# # <change to your own index id, can find it in index store>\n",
    "# index_id = '4850f9e9-158c-44d3-a3f0-1bfec72dfc6e' \n",
    "\n",
    "# bc_index = load_index_from_storage(storage_context=storage_context, index_id=index_id, service_context=service_context)\n",
    "# bc_engine = bc_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. google and wikipedia\n",
    "\n",
    "we will use Google and Wikipedia to clarify certain domain specific terms with LlamaIndex `OpenAI agent`\n",
    "\n",
    "in order to use google search engine you will have to register with google and get an `API_KEY` and a `google_engine`\n",
    "\n",
    "- [Instruction on Getting Google Search API and Engine](https://developers.google.com/custom-search/v1/overview)\n",
    "- [Google Tools on LlamaHub](https://llamahub.ai/l/tools-google_search?from=all)\n",
    "- [Wiki Tools on LlamaHub](https://llamahub.ai/l/tools-wikipedia?from=all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once after you get the API_KEY and Search_Engine save them to `.env` file, \n",
    "\n",
    "GOOGLE_API_KEY='...'\n",
    "\n",
    "GOOGLE_CSE_ID='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "google_key_scheme = 'GOOGLE_API_KEY'\n",
    "google_engine_scheme = 'GOOGLE_CSE_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1)\n",
    "\n",
    "# we will create agents for google search and wikipedia querying\n",
    "def create_google_agent(\n",
    "    google_api_key=os.getenv(google_key_scheme), \n",
    "    google_engine=os.getenv(google_engine_scheme), \n",
    "    verbose=False\n",
    "):\n",
    "    from llama_index.agent import OpenAIAgent\n",
    "    from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "    from llama_hub.tools.google_search.base import GoogleSearchToolSpec\n",
    "\n",
    "    api_key = google_api_key\n",
    "    search_engine = google_engine\n",
    "    google_spec = GoogleSearchToolSpec(key=api_key, engine=search_engine)\n",
    "\n",
    "    # Wrap the google search tool as it returns large payloads\n",
    "    tools = LoadAndSearchToolSpec.from_defaults(\n",
    "        google_spec.to_tool_list()[0],\n",
    "    ).to_tool_list()\n",
    "\n",
    "    agent = OpenAIAgent.from_tools(tools, verbose=verbose, llm=llm)\n",
    "    return agent\n",
    "\n",
    "def create_wiki_agent(verbose=False):\n",
    "    from llama_hub.tools.wikipedia import WikipediaToolSpec\n",
    "    from llama_index.agent import OpenAIAgent\n",
    "\n",
    "    tool_spec = WikipediaToolSpec()\n",
    "\n",
    "    agent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=verbose, llm=llm)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tools\n",
    "\n",
    "Now we have set up the needed components for tools, let us set up tools for LLM to use. \n",
    "\n",
    "\n",
    "1. write a function definition for the tool, and add google style docstring\n",
    "2. keep track of the responses for source checking\n",
    "3. make these functions into `lionagi.Tool` object\n",
    "\n",
    "we will define the functions as `asynchorous` functions so we can run parallel queries concurrently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_arxiv = []\n",
    "responses_d2l = []\n",
    "responses_bc = []\n",
    "\n",
    "async def query_arxiv(query: str):\n",
    "    \"\"\"\n",
    "    Query a vector index built with papers from arxiv. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = await arxiv_engine.aquery(query)\n",
    "    responses_arxiv.append(response)\n",
    "    \n",
    "    return str(response.response)\n",
    "\n",
    "async def query_d2l(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from machine learning textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = await d2l_engine.aquery(query)\n",
    "    responses_d2l.append(response)\n",
    "    \n",
    "    return str(response.response)\n",
    "        \n",
    "async def query_bc(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from blockchain textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = await bc_engine.aquery(query)\n",
    "    responses_bc.append(response)\n",
    "    \n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_google = []\n",
    "responses_wiki = []\n",
    "\n",
    "# ask gpt to write you google format docstring\n",
    "async def query_google(query: str):\n",
    "    \"\"\"\n",
    "    Search Google and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    google_agent = create_google_agent()\n",
    "    response = await google_agent.achat(query)\n",
    "    responses_google.append(response)\n",
    "    return str(response.response)\n",
    "\n",
    "async def query_wiki(query: str):\n",
    "    \"\"\"\n",
    "    Search Wikipedia and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    wiki_agent = create_wiki_agent()\n",
    "    response = await wiki_agent.achat(query)\n",
    "    responses_wiki.append(response)\n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Workflow and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the functions into LionAGI `Tool` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `func_to_tool` function converts a function with google or rest style docstring into a `Tool` object, \n",
    "\n",
    "which can be used during a `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li\n",
    "\n",
    "funcs = [query_google, query_wiki, query_arxiv, query_d2l, query_bc]\n",
    "\n",
    "# lcall is a loop handler, it apply a function to every element in the input list\n",
    "tools = li.lcall(funcs, li.func_to_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "system = {\n",
    "    \"persona\": \"you are a helpful assistant, perform as a researcher\",\n",
    "    \"notice\": f\"your research topic is on {topic}, and the question is {question}\",\n",
    "    \"requirements\": \"Think step by step\",\n",
    "    \"responsibilities\": \"Researching a specific topic and question, explore and provide specific findings and insights\",\n",
    "    \"deliverable\": \"technical only, ~ 1000-1500 words, briefly explain the core concepts and rationale behind, retain from being vague or general, target audience is highly sophisticated and can judge your work. \"\n",
    "}\n",
    "\n",
    "instruct = f\"\"\"\n",
    "read a few paper abstracts, carefully propose a few unique, creative, pratical and achieveable solutions to \n",
    "solve the research question on the specific topics, notice you can use the query tools in parallel, but your questions need to be all different and specific to the tools. Your final deliverable needs to be highly specific and technical. you have to use every tool at least once, but as extensively as you can. \n",
    "\"\"\"\n",
    "\n",
    "contexts = str(abstracts[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a researcher session\n",
    "researcher = li.Session(system)\n",
    "\n",
    "# provide the tools\n",
    "researcher.register_tools(tools)\n",
    "\n",
    "# invoke the task for researcher via reason-action technique\n",
    "out = await researcher.ReAct(instruct, context=contexts, num_rounds=3, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm unable to execute further tool usage actions directly. However, the next steps in the research project have been outlined clearly in the action plan. If you have any specific requests or need further assistance, please let me know how I can help!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>role</th>\n",
       "      <th>sender</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87b892051e997e4f66d907fb1c62def1</td>\n",
       "      <td>system</td>\n",
       "      <td>system</td>\n",
       "      <td>2024-02-11T12:00:07.812855</td>\n",
       "      <td>{\"system_info\": {\"persona\": \"you are a helpful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6fd3c0aea74fa446e9b55256573bd892</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-02-11T12:00:07.813497</td>\n",
       "      <td>{\"instruction\": {\"Notice\": \" \\n               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33a42318a5fbedd5bd82873997cc2677</td>\n",
       "      <td>assistant</td>\n",
       "      <td>assistant</td>\n",
       "      <td>2024-02-11T12:00:44.586011</td>\n",
       "      <td>{\"response\": \"To address the research question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d8ce576aa8620209f8501efb0b018896</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-02-11T12:00:44.588026</td>\n",
       "      <td>{\"instruction\": \"\\n                you have 5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6c7607f2783f6117ce64d51b78917dd</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_request</td>\n",
       "      <td>2024-02-11T12:00:53.092398</td>\n",
       "      <td>{\"action_list\": [{\"action\": \"action_query_arxi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b90a93ac4cd10cea7b72a44a2c363395</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:01:31.080254</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_arxiv\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9ec68d8712511d5f8368d9b9bf42ae32</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:01:31.081321</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_wiki\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ccc12a738e7e5294f2411fc96a310d64</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:01:31.081757</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4bce6147b6779195ccd3f806a803313e</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:01:31.082219</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_d2l\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ad202966ef30f0e5cdc7b18f8eb87b04</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:01:31.082614</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_bc\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ff385eb8b6a7a169fc3fc414dd40dcbb</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-02-11T12:01:31.083055</td>\n",
       "      <td>{\"instruction\": {\"Notice\": \" \\n               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c74ac514b4b0432ccccb0992d1c81eea</td>\n",
       "      <td>assistant</td>\n",
       "      <td>assistant</td>\n",
       "      <td>2024-02-11T12:02:37.626016</td>\n",
       "      <td>{\"response\": \"### Reflection and Action Plan\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b31c7cfad14f9e8d543a95214932f8ec</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-02-11T12:02:37.626937</td>\n",
       "      <td>{\"instruction\": \"\\n                you have 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0df26f55a3bbaf84277b6c445c7ed36a</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_request</td>\n",
       "      <td>2024-02-11T12:02:48.970288</td>\n",
       "      <td>{\"action_list\": [{\"action\": \"action_query_arxi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3aedc6420a13b3666217698bc5166e24</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:03:25.880159</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_arxiv\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2098203ff2f953139553dd48b620394b</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:03:25.880936</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_wiki\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24eb7eb35d7aeea90ccaee0d96633ff2</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:03:25.881365</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cb3efe316b859430f08b73983ece6b17</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:03:25.881771</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_d2l\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2d42d82f0d17650a808378199289ec17</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>2024-02-11T12:03:25.882145</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_bc\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>47ea06cf9feab1d1fc29c86f20f05ccd</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-02-11T12:03:25.882559</td>\n",
       "      <td>{\"instruction\": {\"Notice\": \" \\n               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fd09d3d94da880bc0983430852776f54</td>\n",
       "      <td>assistant</td>\n",
       "      <td>assistant</td>\n",
       "      <td>2024-02-11T12:04:01.628538</td>\n",
       "      <td>{\"response\": \"Reflecting on the gathered infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>df27553525fc490403c417dc1f656e21</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>2024-02-11T12:04:01.629219</td>\n",
       "      <td>{\"instruction\": \"\\n                you have 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>be4ffb5d66371aebfaf0a17657a46a91</td>\n",
       "      <td>assistant</td>\n",
       "      <td>assistant</td>\n",
       "      <td>2024-02-11T12:04:05.212964</td>\n",
       "      <td>{\"response\": \"I'm unable to execute further to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             node_id       role           sender  \\\n",
       "0   87b892051e997e4f66d907fb1c62def1     system           system   \n",
       "1   6fd3c0aea74fa446e9b55256573bd892       user             user   \n",
       "2   33a42318a5fbedd5bd82873997cc2677  assistant        assistant   \n",
       "3   d8ce576aa8620209f8501efb0b018896       user             user   \n",
       "4   b6c7607f2783f6117ce64d51b78917dd  assistant   action_request   \n",
       "5   b90a93ac4cd10cea7b72a44a2c363395  assistant  action_response   \n",
       "6   9ec68d8712511d5f8368d9b9bf42ae32  assistant  action_response   \n",
       "7   ccc12a738e7e5294f2411fc96a310d64  assistant  action_response   \n",
       "8   4bce6147b6779195ccd3f806a803313e  assistant  action_response   \n",
       "9   ad202966ef30f0e5cdc7b18f8eb87b04  assistant  action_response   \n",
       "10  ff385eb8b6a7a169fc3fc414dd40dcbb       user             user   \n",
       "11  c74ac514b4b0432ccccb0992d1c81eea  assistant        assistant   \n",
       "12  b31c7cfad14f9e8d543a95214932f8ec       user             user   \n",
       "13  0df26f55a3bbaf84277b6c445c7ed36a  assistant   action_request   \n",
       "14  3aedc6420a13b3666217698bc5166e24  assistant  action_response   \n",
       "15  2098203ff2f953139553dd48b620394b  assistant  action_response   \n",
       "16  24eb7eb35d7aeea90ccaee0d96633ff2  assistant  action_response   \n",
       "17  cb3efe316b859430f08b73983ece6b17  assistant  action_response   \n",
       "18  2d42d82f0d17650a808378199289ec17  assistant  action_response   \n",
       "19  47ea06cf9feab1d1fc29c86f20f05ccd       user             user   \n",
       "20  fd09d3d94da880bc0983430852776f54  assistant        assistant   \n",
       "21  df27553525fc490403c417dc1f656e21       user             user   \n",
       "22  be4ffb5d66371aebfaf0a17657a46a91  assistant        assistant   \n",
       "\n",
       "                     timestamp  \\\n",
       "0   2024-02-11T12:00:07.812855   \n",
       "1   2024-02-11T12:00:07.813497   \n",
       "2   2024-02-11T12:00:44.586011   \n",
       "3   2024-02-11T12:00:44.588026   \n",
       "4   2024-02-11T12:00:53.092398   \n",
       "5   2024-02-11T12:01:31.080254   \n",
       "6   2024-02-11T12:01:31.081321   \n",
       "7   2024-02-11T12:01:31.081757   \n",
       "8   2024-02-11T12:01:31.082219   \n",
       "9   2024-02-11T12:01:31.082614   \n",
       "10  2024-02-11T12:01:31.083055   \n",
       "11  2024-02-11T12:02:37.626016   \n",
       "12  2024-02-11T12:02:37.626937   \n",
       "13  2024-02-11T12:02:48.970288   \n",
       "14  2024-02-11T12:03:25.880159   \n",
       "15  2024-02-11T12:03:25.880936   \n",
       "16  2024-02-11T12:03:25.881365   \n",
       "17  2024-02-11T12:03:25.881771   \n",
       "18  2024-02-11T12:03:25.882145   \n",
       "19  2024-02-11T12:03:25.882559   \n",
       "20  2024-02-11T12:04:01.628538   \n",
       "21  2024-02-11T12:04:01.629219   \n",
       "22  2024-02-11T12:04:05.212964   \n",
       "\n",
       "                                              content  \n",
       "0   {\"system_info\": {\"persona\": \"you are a helpful...  \n",
       "1   {\"instruction\": {\"Notice\": \" \\n               ...  \n",
       "2   {\"response\": \"To address the research question...  \n",
       "3   {\"instruction\": \"\\n                you have 5 ...  \n",
       "4   {\"action_list\": [{\"action\": \"action_query_arxi...  \n",
       "5   {\"action_response\": {\"function\": \"query_arxiv\"...  \n",
       "6   {\"action_response\": {\"function\": \"query_wiki\",...  \n",
       "7   {\"action_response\": {\"function\": \"query_google...  \n",
       "8   {\"action_response\": {\"function\": \"query_d2l\", ...  \n",
       "9   {\"action_response\": {\"function\": \"query_bc\", \"...  \n",
       "10  {\"instruction\": {\"Notice\": \" \\n               ...  \n",
       "11  {\"response\": \"### Reflection and Action Plan\\n...  \n",
       "12  {\"instruction\": \"\\n                you have 3 ...  \n",
       "13  {\"action_list\": [{\"action\": \"action_query_arxi...  \n",
       "14  {\"action_response\": {\"function\": \"query_arxiv\"...  \n",
       "15  {\"action_response\": {\"function\": \"query_wiki\",...  \n",
       "16  {\"action_response\": {\"function\": \"query_google...  \n",
       "17  {\"action_response\": {\"function\": \"query_d2l\", ...  \n",
       "18  {\"action_response\": {\"function\": \"query_bc\", \"...  \n",
       "19  {\"instruction\": {\"Notice\": \" \\n               ...  \n",
       "20  {\"response\": \"Reflecting on the gathered infor...  \n",
       "21  {\"instruction\": \"\\n                you have 1 ...  \n",
       "22  {\"response\": \"I'm unable to execute further to...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "researcher.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seem like the assistant gave answer during the ReAct, instead of presenting outcome in the end, let us check what it did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = researcher.messages\n",
    "answer = df[df.sender == \"assistant\"].content.iloc[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reflecting on the gathered information and moving forward with the action plan, we enter the next phase of our research project. The insights provided through the use of various tools highlight both the potential and the challenges in integrating Large Language Models (LLMs) with blockchain to create trust systems. Each tool has offered a unique perspective, contributing to a multifaceted understanding of how LLMs can enhance blockchain technology and vice versa.\n",
       "\n",
       "### Step 1: Technical Deep Dive into Proposed Solutions\n",
       "\n",
       "Based on the synthesized insights, we identified four key areas where LLMs can significantly impact blockchain technology, specifically regarding reliability prediction, interoperability, time management, and the deployment and management of blockchain applications. Here, we will outline a technical deep dive for each proposed solution:\n",
       "\n",
       "1. **LLM-Aided Peer Reliability Assessment System**:\n",
       "   - **Objective**: Develop a system that utilizes LLMs to assess and predict the reliability of blockchain peers or nodes based on historical transaction data and peer interactions.\n",
       "   - **Technical Approach**: Implement NLP techniques to analyze qualitative feedback and performance metrics. Use supervised learning models trained on historical data to predict reliability scores for peers. Integrate this system with blockchain networks to dynamically update peer reliability scores.\n",
       "   \n",
       "2. **Smart Contract Translator for Interoperability**:\n",
       "   - **Objective**: Create a tool that leverages LLMs to translate smart contracts from one blockchain platform's language to another, enhancing interoperability.\n",
       "   - **Technical Approach**: Train an LLM on a diverse dataset of smart contracts across different blockchain languages. Implement a translation mechanism that maintains the logical integrity and security properties of the original contract. Evaluate the translated contracts for performance and security.\n",
       "\n",
       "3. **Temporal Logic Handler for Smart Contracts**:\n",
       "   - **Objective**: Develop an LLM module that introduces temporal logic into smart contracts, enabling them to handle time-based conditions and events more naturally.\n",
       "   - **Technical Approach**: Use LLMs to parse and modify smart contracts, incorporating time-related constructs and logic. Explore the integration of blockchain-specific time measures for accurate event handling.\n",
       "\n",
       "4. **Blockchain Application Management Suite with LLM**:\n",
       "   - **Objective**: Build a suite of tools that utilize LLMs for automated code generation, security auditing, and user interface creation for blockchain applications.\n",
       "   - **Technical Approach**: Develop an LLM-based code generator for smart contracts and blockchain applications. Create an anomaly detection system for identifying security vulnerabilities. Implement an LLM-powered interface generator that simplifies user interaction with blockchain applications.\n",
       "\n",
       "### Step 2: Implementation Strategy\n",
       "\n",
       "For each solution, the implementation strategy involves several key steps:\n",
       "\n",
       "- **Dataset Collection and Preparation**: Collect and preprocess data relevant to each solution, including smart contracts, blockchain transaction data, and user feedback.\n",
       "- **Model Training and Tuning**: Train LLMs using the prepared datasets, fine-tuning them for specific tasks such as translation, reliability prediction, and code generation.\n",
       "- **Integration with Blockchain Technologies**: Develop APIs and interfaces for integrating the LLM-based solutions with existing blockchain platforms and applications.\n",
       "- **Validation and Testing**: Conduct extensive testing and validation to ensure the effectiveness, security, and performance of the proposed solutions.\n",
       "- **Deployment and Monitoring**: Deploy the solutions in real-world scenarios, monitor their performance, and iteratively improve based on user feedback and observed outcomes.\n",
       "\n",
       "By following this detailed action plan, we aim to advance the integration of LLMs with blockchain technology, addressing key challenges and leveraging their combined strengths to enhance trust systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(li.as_dict(answer)['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Improve work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = await researcher.chat(\n",
    "    \"\"\"\n",
    "    you asked a lot of good questions and got plenty answers, please integrate your \n",
    "    conversation, be a lot more technical, you will be rewarded with 500 dollars for \n",
    "    great work, and punished for subpar work, take a deep breath, you can do it\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Integrating insights from our comprehensive exploration of integrating Large Language Models (LLMs) with blockchain technology to build trust systems, we delve into a technical synthesis aimed at proposing viable, innovative solutions. This synthesis is informed by the rich information extracted from various sources, including academic papers, interoperability frameworks, and foundational principles of blockchain technology. Our focus zeroes in on four pivotal areas: reliability prediction of blockchain peers, interoperability enhancements, time management in blockchain processes, and the deployment and management of blockchain applications.\n",
       "\n",
       "### Reliability Prediction Using LLMs\n",
       "\n",
       "The use of LLMs for predicting the reliability of blockchain peers or nodes is an untapped area with substantial potential. While traditional methods, as highlighted in the Hybrid Blockchain Reliability Prediction (H-BRP) model, rely on historical data and central data collectors for prediction, integrating LLMs could revolutionize this approach. By leveraging NLP techniques, LLMs can analyze unstructured data, such as peer reviews or qualitative feedback, alongside quantitative performance metrics. This dual analysis approach could yield a more nuanced and accurate prediction model.\n",
       "\n",
       "**Technical Approach**:\n",
       "- **Data Collection**: Aggregate historical transaction data, peer interaction logs, and qualitative feedback.\n",
       "- **Model Training**: Employ supervised learning to train LLMs on this aggregated data, focusing on identifying patterns correlated with high reliability.\n",
       "- **Integration**: Develop an API for blockchain networks that utilizes the trained LLM to dynamically update reliability scores based on ongoing peer performance and feedback.\n",
       "\n",
       "### Enhancing Interoperability with LLMs\n",
       "\n",
       "Interoperability remains a significant challenge within the blockchain ecosystem. The insights gathered suggest that LLMs could play a crucial role in translating smart contracts between different blockchain languages, thus facilitating seamless interaction across platforms.\n",
       "\n",
       "**Technical Approach**:\n",
       "- **Dataset Creation**: Compile a comprehensive dataset of smart contracts from various blockchain platforms.\n",
       "- **LLM Training**: Train an LLM on this dataset to recognize and translate between different blockchain languages while preserving logical and security properties.\n",
       "- **Deployment**: Develop a platform-agnostic tool that leverages the trained LLM for smart contract translation, offering it as a service to blockchain developers.\n",
       "\n",
       "### Temporal Logic in Smart Contracts\n",
       "\n",
       "The absence of a natural notion of time in smart contracts is a gap that LLMs could fill. Through the integration of temporal logic, smart contracts could handle time-based conditions more effectively.\n",
       "\n",
       "**Technical Approach**:\n",
       "- **Logic Definition**: Define a set of temporal logic constructs tailored for blockchain applications.\n",
       "- **LLM Modification**: Train LLMs to parse and modify existing smart contracts to incorporate these constructs, ensuring that the contracts can interact with blockchain-specific time measures accurately.\n",
       "- **Smart Contract Development**: Encourage the adoption of these enhanced smart contracts by providing a development toolkit that includes LLM-based temporal logic integration.\n",
       "\n",
       "### LLM-Powered Blockchain Application Management\n",
       "\n",
       "The deployment and management of blockchain applications stand to benefit significantly from LLM integration. Automating code generation, enhancing security through anomaly detection, and simplifying user interfaces are key areas of potential impact.\n",
       "\n",
       "**Technical Approach**:\n",
       "- **Automated Code Generation**: Develop an LLM-trained on a diverse set of smart contracts to generate or suggest code snippets, reducing developer effort and increasing code reliability.\n",
       "- **Anomaly Detection**: Train LLMs on transaction patterns to identify and alert about potential fraudulent activities or security breaches.\n",
       "- **User Interface Generation**: Utilize LLMs to create intuitive, natural language interfaces for blockchain applications, lowering the barrier to entry for non-technical users.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "The integration of LLMs with blockchain technology opens a new frontier for enhancing trust systems. By addressing reliability prediction, interoperability, temporal challenges in smart contracts, and application deployment and management, LLMs can significantly contribute to the robustness, security, and user-friendliness of blockchain technologies. Future research should focus on developing specific LLM architectures and training methodologies tailored to these blockchain-centric tasks, ensuring the scalability and adaptability of these solutions to meet evolving needs and challenges within the blockchain ecosystem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. check the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df = researcher.messages\n",
    "\n",
    "print(len(df[df.sender == \"action_response\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 10 tool usages during this ReAct session, let us take a look at some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool used is: query_bc\n",
      "question asked: {'query': 'What foundational principles of blockchain technology can be leveraged to integrate LLMs for creating trust systems?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The foundational principles of blockchain technology that can be leveraged to integrate Large Language Models (LLMs) for creating trust systems include:\n",
       "\n",
       "1. **Distributed and Sustainable**: The ledger is shared, updated with every transaction, and selectively replicated among participants in near real time. This principle ensures that the blockchain platform's existence isn't dependent on any single organization, which can be applied to LLMs by distributing the model's knowledge base across a network, ensuring its sustainability and resilience.\n",
       "\n",
       "2. **Secure, Private, and Indelible**: Permissions and cryptography prevent unauthorized access to the network and ensure that participants are who they claim to be. Privacy is maintained through cryptographic techniques and/or data partitioning techniques. This principle can be applied to LLMs by ensuring that interactions and data exchanges are secure and private, with an immutable record of transactions and interactions.\n",
       "\n",
       "3. **Transparent and Auditable**: Participants in a transaction have access to the same records, allowing them to validate transactions and verify identities or ownership without the need for third-party intermediaries. This principle can be applied to LLMs by making the decision-making process transparent and auditable, allowing users to understand how conclusions are reached.\n",
       "\n",
       "4. **Consensus-based and Transactional**: All relevant network participants must agree that a transaction is valid, achieved through the use of consensus algorithms. This principle can be applied to LLMs by incorporating consensus mechanisms for decision-making processes, ensuring that outputs are validated by multiple parties before being accepted.\n",
       "\n",
       "5. **Orchestrated and Flexible**: Business rules and smart contracts can be built into the platform, allowing blockchain business networks to evolve to support end-to-end business processes and a wide range of activities. This principle can be applied to LLMs by enabling them to operate within a framework of smart contracts for automated execution of tasks based on predefined conditions, enhancing flexibility and orchestration of complex processes.\n",
       "\n",
       "By leveraging these foundational principles, LLMs can be integrated into trust systems that are secure, transparent, and capable of facilitating consensus-based decision-making, thereby enhancing trust among network participants."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict1 = li.as_dict(df[df.sender == \"action_response\"].content.iloc[4])\n",
    "\n",
    "func_ = li.nget(res_dict1, ['action_response', 'function'])\n",
    "args_ = li.nget(res_dict1, ['action_response', 'arguments'])\n",
    "output_ = li.nget(res_dict1, ['action_response', 'output'])\n",
    "\n",
    "print(f\"Tool used is: {func_}\")\n",
    "print(f\"question asked: {args_}\")\n",
    "Markdown(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool used is: query_wiki\n",
      "question asked: {'query': 'What are the existing frameworks or models for interoperability among blockchain platforms, and how can LLMs enhance them?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Interoperability among blockchain platforms is a critical area of development that aims to enable seamless communication and interaction between different blockchain networks. This is essential for creating a more connected and efficient blockchain ecosystem. While specific frameworks or models for blockchain interoperability were not directly detailed, we can discuss the general concept and how Large Language Models (LLMs) could potentially enhance these frameworks.\n",
       "\n",
       "### Existing Frameworks or Models for Blockchain Interoperability\n",
       "\n",
       "1. **Cross-Chain Technology**: This involves creating protocols that enable the transfer of assets and information between different blockchain networks. Examples include Polkadot's parachains, Cosmos' Inter-Blockchain Communication (IBC) protocol, and the Wanchain project.\n",
       "\n",
       "2. **Blockchain Bridges**: Bridges are specific implementations of cross-chain technology that connect two blockchains, allowing for the transfer of assets and data between them. They can be trust-based, requiring intermediaries, or trustless, relying on smart contracts and decentralized mechanisms.\n",
       "\n",
       "3. **Sidechains and Layer 2 Solutions**: Sidechains are separate blockchains that are connected to a main chain, allowing for asset transfer between the two. Layer 2 solutions, like the Lightning Network for Bitcoin, provide scalability and interoperability benefits by handling transactions off the main chain.\n",
       "\n",
       "4. **Atomic Swaps**: This is a technology that enables the exchange of one cryptocurrency for another without the need for a trusted third party, using a mechanism called Hashed Timelock Contracts (HTLCs).\n",
       "\n",
       "5. **Interledger Protocols**: These are protocols designed to connect different ledgers and payment systems, not limited to blockchains, to enable secure and seamless transactions across them.\n",
       "\n",
       "### How LLMs Can Enhance Blockchain Interoperability\n",
       "\n",
       "LLMs, with their advanced language understanding and generation capabilities, can contribute to enhancing blockchain interoperability in several ways:\n",
       "\n",
       "1. **Smart Contract Development and Verification**: LLMs can assist in the development of more sophisticated and secure smart contracts that are crucial for interoperability solutions like blockchain bridges and atomic swaps. They can help in generating code, detecting vulnerabilities, and verifying the correctness of smart contracts.\n",
       "\n",
       "2. **Protocol Translation**: LLMs can facilitate the translation of protocols between different blockchain networks, acting as a linguistic bridge that interprets and converts data formats, transaction models, and consensus mechanisms.\n",
       "\n",
       "3. **Automated Governance**: For interoperability frameworks that involve governance mechanisms, LLMs can assist in automating decision-making processes, interpreting governance proposals, and generating reports or summaries to inform stakeholders.\n",
       "\n",
       "4. **User Interaction**: LLMs can improve the user experience by providing natural language interfaces for interacting with cross-chain technologies, making it easier for non-technical users to engage with complex interoperability solutions.\n",
       "\n",
       "5. **Documentation and Education**: LLMs can generate and maintain comprehensive documentation for interoperability frameworks, protocols, and APIs, facilitating easier adoption and understanding by developers and users alike.\n",
       "\n",
       "In conclusion, while blockchain interoperability remains a complex challenge, the integration of LLMs into interoperability solutions can offer significant benefits in terms of security, efficiency, and usability. As both blockchain technology and LLMs continue to evolve, their synergistic potential is likely to become increasingly important for the development of a fully interconnected blockchain ecosystem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict1 = li.as_dict(df[df.sender == \"action_response\"].content.iloc[6])\n",
    "\n",
    "func_ = li.nget(res_dict1, ['action_response', 'function'])\n",
    "args_ = li.nget(res_dict1, ['action_response', 'arguments'])\n",
    "output_ = li.nget(res_dict1, ['action_response', 'output'])\n",
    "\n",
    "print(f\"Tool used is: {func_}\")\n",
    "print(f\"question asked: {args_}\")\n",
    "Markdown(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Save the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher.messages.to_csv(\"researcher_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = li.to_list([responses_arxiv, responses_d2l, responses_bc, responses_google, responses_wiki], flatten=True)\n",
    "responses_dicts = li.lcall(responses, lambda x: {\"response\": str(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = li.to_df(responses_dicts)\n",
    "responses_df.to_csv(\"research_query_responses.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
