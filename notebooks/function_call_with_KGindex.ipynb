{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior with LionAGI and LlamaIndex Knowledge Graph\n",
    "\n",
    "-- how to do auto explorative research with LionAGI plus RAG using llamaindex Knowledge Graph Index \n",
    "\n",
    "- [LionAGI](https://github.com/lion-agi/lionagi)\n",
    "- [LlamaIndex](https://www.llamaindex.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install the various dependencies, the first part of this tutorial is based on [Knowledge Graph w/ WikiData Filtering](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/knowledge_graph2.html)\n",
    "\n",
    "- if want to read further please check: [Make Meaningful Knowledge Graph from OpenSource REBEL Model](https://medium.com/@haiyangli_38602/make-meaningful-knowledge-graph-from-opensource-rebel-model-6f9729a55527)\n",
    "\n",
    "The following pip install is for MacOS, if you use cuda GPU, please just `pip install pytorch`, and change the device to `cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "# %pip install lionagi llama_index llama_hub transformers wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Large Language Model Time Series Analysis'\n",
    "dir = \"data/log/researcher/\"\n",
    "num_papers = 1\n",
    "num_pages = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Let us write a extriplets extraction function using REBEL model\n",
    "\n",
    "- [REBEL model](https://huggingface.co/Babelscape/rebel-large) is one of the best models out there for entity extraction. And it's free to use\n",
    "\n",
    "- we will use wikipedia to filter the entities extracted in order to validate the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lion/anaconda3/envs/lion_dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/lion/anaconda3/envs/lion_dev/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/lion/anaconda3/envs/lion_dev/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"Babelscape/rebel-large\",\n",
    "    tokenizer=\"Babelscape/rebel-large\",\n",
    "    # comment this line to run on CPU, use \"cuda:0\" to run on GPU, stay with \"mps:0\" to run on apple silicon\n",
    "    device=\"mps:0\",\n",
    ")\n",
    "\n",
    "def extract_triplets(input_text):\n",
    "    text = triplet_extractor.tokenizer.batch_decode(\n",
    "        [\n",
    "            triplet_extractor(input_text, return_tensors=True, \n",
    "                              return_text=False)[0][\"generated_token_ids\"]\n",
    "        ]\n",
    "    )[0]\n",
    "\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append(\n",
    "                    (\n",
    "                        subject.strip(),\n",
    "                        relation.strip(),\n",
    "                        object_.strip()\n",
    "                    )\n",
    "                )\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append(\n",
    "                    (\n",
    "                        subject.strip(),\n",
    "                        relation.strip(),\n",
    "                        object_.strip()\n",
    "                    )\n",
    "                )\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append(\n",
    "            (\n",
    "                subject.strip(),\n",
    "                relation.strip(),\n",
    "                object_.strip()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "class WikiFilter:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "\n",
    "    def filter(self, candidate_entity):\n",
    "        # check the cache to avoid network calls\n",
    "        if candidate_entity in self.cache:\n",
    "            return self.cache[candidate_entity]['title']\n",
    "\n",
    "        # pull the page from wikipedia -- if it exists\n",
    "        try:\n",
    "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
    "            entity_data = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"summary\": page.summary,\n",
    "            }\n",
    "\n",
    "            # cache the page title and original entity\n",
    "            self.cache[candidate_entity] = entity_data\n",
    "            self.cache[page.title] = entity_data\n",
    "\n",
    "            return entity_data['title']\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "wiki_filter = WikiFilter()\n",
    "\n",
    "def extract_triplets_wiki(text):\n",
    "    relations = extract_triplets(text)\n",
    "\n",
    "    filtered_relations = []\n",
    "    for relation in relations:\n",
    "        (subj, rel, obj) = relation\n",
    "        filtered_subj = wiki_filter.filter(subj)\n",
    "        filtered_obj = wiki_filter.filter(obj)\n",
    "\n",
    "        # skip if at least one entity not linked to wiki\n",
    "        if filtered_subj is None and filtered_obj is None:\n",
    "            continue\n",
    "\n",
    "        filtered_relations.append(\n",
    "            (\n",
    "                filtered_subj or subj,\n",
    "                rel,\n",
    "                filtered_obj or obj,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return filtered_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a Knowledge Graph Index with llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/lion/anaconda3/envs/lion_dev/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/lion/anaconda3/envs/lion_dev/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex, download_loader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "loader = ArxivReader()\n",
    "node_parser = SentenceSplitter(chunk_size=800, chunk_overlap=50)\n",
    "\n",
    "# let us download some papers from arvix\n",
    "documents, abstracts = loader.load_papers_and_abstracts(search_query=query, max_results=num_papers)\n",
    "nodes = node_parser.get_nodes_from_documents(documents, show_progress=False)\n",
    "\n",
    "# set up the knowledge index\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "index1 = KnowledgeGraphIndex(\n",
    "    nodes,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=3,\n",
    "    kg_triplet_extract_fn=extract_triplets_wiki,\n",
    "    storage_context=storage_context,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "query_engine = index1.as_query_engine(include_text=False, response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a tool description according to OpenAI schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"query_arvix_papers\",\n",
    "            \"description\": \"Perform a query to a QA bot with access to a knowledge graph index built with papers from arvix\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"a question to ask the QA bot\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "func = query_engine.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = {\n",
    "    \"persona\": \"a helpful world-class researcher\",\n",
    "    \"requirements\": \"think step by step before returning a clear, concise, precise worded answer with a humble yet confident tone\",\n",
    "    \"responsibilities\": f\"you are asked to help with researching on the topic of {query}\",\n",
    "    \"tools\": \"provided with a QA bot for grounding responses\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Research: PROMPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FORMATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliver_format1 = {\"return required\": \"yes\", \"return format\": \"paragraph\"}\n",
    "\n",
    "deliver_format2 = {\"return required\": \"yes\", \n",
    "    \"return format\": {\n",
    "        \"type\": \"json_mode\", \n",
    "        \"format\": {\n",
    "            'paper': \"paper_name\",\n",
    "            \"summary\": \"...\", \n",
    "            \"research question\": \"...\", \n",
    "            \"talking points\": {\n",
    "                \"point 1\": \"...\",\n",
    "                \"point 2\": \"...\",\n",
    "                \"point 3\": \"...\"\n",
    "            }}}},\n",
    "\n",
    "deliver_format3 = {\n",
    "    \"notice\":\"Notice you are provided with a QA bot as your tool, the bot has access to the papers via a queriable knowledge graph index that takes natural language query and return a natural language answer. You can decide whether to invoke the function call, you will need to ask the bot when there are things need clarification or further information. you provide the query by asking a question, please use the tool when appropriately need to\", \n",
    "    \"tool choice\": \"auto\", \n",
    "    \"format\":\"function calling\"}\n",
    "\n",
    "deliver_format4 = {\n",
    "    \"return required\": \"yes\", \n",
    "    \"return format\": {\n",
    "        \"type\": \"json_mode\",\n",
    "        \"format\": {\n",
    "            \"title\": \"our new research paper title\",\n",
    "            \"summary\": \"...\",\n",
    "            \"key words\": [\"xxx\", \"xxx\", \"xxx\"],\n",
    "            \"outline\": {\n",
    "                \"part zero: abstract\": \"plans for abstract\",\n",
    "                \"part one: introduction\": \"...\", \n",
    "                \"part two:...\":\"...\",\n",
    "                \"part three: ...\": \"...\",\n",
    "                \"...\": \"...\"\n",
    "            }}}}\n",
    "\n",
    "deliver_format5 = {\n",
    "    \"return required\": \"yes\", \n",
    "    \"return format\": {\n",
    "        \"type\": \"json_mode\",\n",
    "        \"format\": {\n",
    "            \"question 1\": \"...\", \n",
    "            \"question 2\": \"...\",\n",
    "            \"...\": \"...\"\n",
    "        }}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct1 = {\n",
    "    \"task step\": \"1\", \n",
    "    \"task name\": \"read paper abstracts\", \n",
    "    \"task objective\": \"get initial understanding of the papers of interest\", \n",
    "    \"task description\": \"provided with abstracts of paper, provide a brief summary highlighting the paper core points\",\n",
    "    \"deliverable\": deliver_format1\n",
    "}\n",
    "\n",
    "instruct2 = {\n",
    "    \"task step\": \"2\",\n",
    "    \"task name\": \"propose research questions and talking points\", \n",
    "    \"task objective\": \"initial brainstorming\", \n",
    "    \"task description\": \"from the improved understanding of the paper, please propose some research questions and talking points.\",\n",
    "    \"deliverable\": deliver_format2,\n",
    "    \"function calling\": deliver_format3\n",
    "}\n",
    "\n",
    "instruct3 = {\n",
    "    \"task step\": \"3\",\n",
    "    \"task name\": \"evaluate and brainstorm new research question\",\n",
    "    \"task objective\": \"identify a most interesting research question and provide talking points\",\n",
    "    \"task description\": f\"provided with a list of research questions and talking points from {num_papers} papers, please thoroughly read through the list and identify the most interesting research question and provide talking points. You need to provide reasoning to support your choice\",\n",
    "    \"deliverable\": deliver_format1\n",
    "}\n",
    "\n",
    "instruct4 = {\n",
    "    \"task step\": \"4\",\n",
    "    \"task name\": \"solidify the research question\",\n",
    "    \"task objective\": \"create an outline for on research topic\",\n",
    "    \"task description\": \"from our current understanding of task on hand, please propose a final research question as our project\",\n",
    "    \"deliverable\": deliver_format1,\n",
    "    \"function calling\": deliver_format3\n",
    "}\n",
    "\n",
    "instruct5 = {\n",
    "    \"task step\": \"5\",\n",
    "    \"task name\": \"validate outline\",\n",
    "    \"task objective\": \"validate the outline is indeed accurately described to the provided papers\",\n",
    "    \"deliverable\": deliver_format4, \n",
    "    \"function calling\": deliver_format3\n",
    "}\n",
    "\n",
    "instruct6 = {\n",
    "    \"task step\": \"6\",\n",
    "    \"task name\": f\"write the whole paper draft of around {num_pages} pages, you can do it\",\n",
    "    \"deliverable\": deliver_format1, \n",
    "    \"function calling\": deliver_format3\n",
    "}\n",
    "\n",
    "instruct7 = {\n",
    "    \"task step\": \"7\", \n",
    "    \"task name\": \"edit the paper as a whole\", \n",
    "    \"task description\": \"provided with everything we have worked on, please carefully read through everything and list a series of questions as critique\",\n",
    "    \"deliverable\": deliver_format1, \n",
    "    \"function calling\": deliver_format3\n",
    "}\n",
    "\n",
    "instruct8 = {\n",
    "    \"task step\": \"8\", \n",
    "    \"task name\": \"respond to the critism\", \n",
    "    \"task description\": \"given everything we have talked about, please address the critism and provide a revised version of the paper\",\n",
    "    \"deliverable\": deliver_format1, \n",
    "}\n",
    "\n",
    "instruct9 = {\n",
    "    \"task step\": \"9\", \n",
    "    \"task name\": \"proof reading\", \n",
    "    \"task description\": \"provided with everything we have talked about, please work on grammar and writing style to sound less robotic, return the whole edited paper, your response limit is 4000 tokens, you can totally deliver the whole thing, take a deep breath, I believe in you\",\n",
    "    \"deliverable\": deliver_format1, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Research setup workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _auto_followup(session, instruct, **kwargs):\n",
    "    await session.followup(instruct, **kwargs)\n",
    "    if session.conversation.messages and session.conversation.messages[-1]['role'] == 'tool':\n",
    "        return await session.followup(instruct, **kwargs)\n",
    "    return False\n",
    "\n",
    "async def auto_followup(session, instruct, num_func_call=2, **kwargs):\n",
    "    for _ in range(num_func_call):\n",
    "        if not await _auto_followup(session, instruct, **kwargs):\n",
    "            break\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_query = {\"type\":\"function\", \"function\":{\"name\": \"query_arvix_papers\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 1-2\n",
    "async def read_abstract(context):\n",
    "    researcher = li.Session(system, dir=dir)\n",
    "    researcher.register_tools(tools, func)\n",
    "    researcher.llmconfig.update({\"tools\": tools})\n",
    "    \n",
    "    await researcher.initiate(instruct1, context=context, temperature=0.7, tool_choice=\"none\")\n",
    "    researcher = await auto_followup(researcher, instruct2, temperature=0.4, \n",
    "                                     response_format= {'type':'json_object'}, tool_choice=use_query)\n",
    "    \n",
    "    researcher.messages_to_csv()\n",
    "    researcher.log_to_csv()\n",
    "    return researcher\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest\n",
    "async def write_paper(context):\n",
    "    researcher = li.Session(system, dir=dir)\n",
    "    researcher.register_tools(tools, func)\n",
    "    researcher.llmconfig.update({\"tools\": tools})\n",
    "    \n",
    "    await researcher.initiate(instruct3, context=context, temperature=0.5, tool_choice=\"none\")\n",
    "    researcher = await auto_followup(researcher, instruct4, temperature=0.5, tool_choice=use_query)\n",
    "    researcher = await auto_followup(researcher, instruct5, tempature=0.35, tool_choice=use_query)\n",
    "    researcher = await auto_followup(researcher, instruct6, temperature=0.75, tool_choice=use_query)\n",
    "    researcher = await auto_followup(researcher, instruct7, temperature=0.75, tool_choice=use_query)\n",
    "    await researcher.followup(instruct8, temperature=0.65, tool_choice=\"none\")\n",
    "    await researcher.followup(instruct9, temperature=0.45, tool_choice=\"none\")\n",
    "\n",
    "    researcher.messages_to_csv()\n",
    "    researcher.log_to_csv()\n",
    "    return researcher.conversation.messages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstracts = li.l_call(range(len(abstracts)), lambda i: abstracts[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 logs saved to data/log/researcher/_messages_2023-12-14T11_41_06_767212.csv\n",
      "2 logs saved to data/log/researcher/_llmlog_2023-12-14T11_41_06_769474.csv\n"
     ]
    }
   ],
   "source": [
    "# run first workflow over all paper abstracts\n",
    "researchers = await li.al_call(abstracts, read_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': 'The paper discusses the challenge of time-series analysis across various domains, where typically a unique model is developed for each specific task, requiring substantial data and domain expertise. Addressing this issue, the paper introduces a novel method called Large Pre-trained Time Series Model (LPTM). The key innovation of LPTM is its ability to pre-train on heterogeneous time-series datasets by identifying optimal, task-specific segmentation strategies for the input data, which is accomplished through a self-supervised learning loss. This approach enables the model to handle various time-series dynamics from different domains effectively. The paper claims that LPTM can match or exceed the performance of domain-specific models while being more efficient in terms of data and computational resources. It reportedly requires up to 40% less data and 50% less training time to achieve state-of-the-art results across a diverse range of time-series analysis tasks.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '{\"function\": \"func_query_arvix_papers\", \"arguments\": \"{\\\\\"query\\\\\":\\\\\"What are the current limitations of Large Pre-trained Time Series Models (LPTM)?\\\\\"}\"}'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "researchers[0].conversation.responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 logs saved to data/log/researcher/_messages_2023-12-14T11_24_57_780808.csv\n",
      "7 logs saved to data/log/researcher/_llmlog_2023-12-14T11_24_57_782343.csv\n"
     ]
    }
   ],
   "source": [
    "# extracts the final outputs from stage 1\n",
    "out1 = li.l_call(researchers, lambda x: x.conversation.messages[-1])\n",
    "\n",
    "# run second workflow\n",
    "paper = await write_paper(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = func(\"What are the current limitations of Large Pre-trained Time Series Models (LPTM)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The limitations of Large Pre-trained Time Series Models (LPTMs) are not specified in the provided context. To understand the current limitations of LPTMs, one would typically need to consult recent research papers, technical reports, or expert analyses that specifically address the performance, challenges, and drawbacks associated with these models.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the API runs\n",
    "session = li.Session(system)\n",
    "session.api_service.status_tracker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
