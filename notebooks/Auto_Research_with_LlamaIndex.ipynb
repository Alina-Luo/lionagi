{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior with LionAGI and LlamaIndex Vector Index\n",
    "\n",
    "-- how to do auto explorative research with LionAGI plus RAG using llamaindex Vector Index & embedding \n",
    "\n",
    "- [LionAGI](https://github.com/lion-agi/lionagi)\n",
    "- [LlamaIndex](https://www.llamaindex.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Large Language Model Time Series Analysis'\n",
    "dir = \"data/log/researcher/\"\n",
    "num_papers = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build a Vector Index with llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "loader = ArxivReader()\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "# let us download some papers from arvix\n",
    "documents, abstracts = loader.load_papers_and_abstracts(search_query=query, \n",
    "                                                        max_results=num_papers)\n",
    "nodes = node_parser.get_nodes_from_documents(documents, show_progress=False)\n",
    "\n",
    "# set up index object\n",
    "llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "index1 = VectorStoreIndex(nodes, include_embeddings=True, \n",
    "                          service_context=service_context)\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index1.as_query_engine(include_text=False, \n",
    "                                      response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tool description according to OpenAI schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"query_arxiv_papers\",\n",
    "            \"description\": \"\"\"\n",
    "                           Perform a query to a QA bot with access to an \n",
    "                           index built with papers from arxiv\n",
    "                          \"\"\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"str_or_query_bundle\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"a question to ask the QA bot\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"str_or_query_bundle\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# we will need to register both the function description \n",
    "# and actual implementation\n",
    "func = query_engine.query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Research: PROMPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FORMATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rigidly set up prompt can help make outcome more deterministic\n",
    "# though any string will work as well. \n",
    "system = {\n",
    "    \"persona\": \"a helpful world-class researcher\",\n",
    "    \"requirements\": \"\"\"\n",
    "              think step by step before returning a clear, precise \n",
    "              worded answer with a humble yet confident tone\n",
    "          \"\"\",\n",
    "    \"responsibilities\": f\"\"\"\n",
    "              you are asked to help with researching on the topic \n",
    "              of {query}\n",
    "          \"\"\",\n",
    "    \"tools\": \"provided with a QA bot for grounding responses\"\n",
    "}\n",
    "\n",
    "# similarly, we can pass in any string or dictionary to instruction\n",
    "# here we are modifying model behavior by telling mdel how to output \n",
    "deliver_format1 = {\"return required\": \"yes\", \"return format\": \"paragraph\"}\n",
    "\n",
    "deliver_format2 = {\"return required\": \"yes\", \n",
    "    \"return format\": { \n",
    "        \"json_mode\": {\n",
    "            'paper': \"paper_name\",\n",
    "            \"summary\": \"...\", \n",
    "            \"research question\": \"...\", \n",
    "            \"talking points\": {\n",
    "                \"point 1\": \"...\",\n",
    "                \"point 2\": \"...\",\n",
    "                \"point 3\": \"...\"\n",
    "            }}}}\n",
    "            \n",
    "function_call = {\n",
    "    \"notice\":f\"\"\"\n",
    "        At each task step, identified by step number, you must use the tool \n",
    "        at least twice. Notice you are provided with a QA bot as your tool, \n",
    "        the bot has access to the {num_papers} papers via a queriable index \n",
    "        that takes natural language query and return a natural language \n",
    "        answer. You can decide whether to invoke the function call, you will \n",
    "        need to ask the bot when there are things need clarification or \n",
    "        further information. you provide the query by asking a question, \n",
    "        please use the tool as extensively as you can.\n",
    "       \"\"\"\n",
    "    }\n",
    "\n",
    "# here we create a two step process imitating the steps human would take to \n",
    "# perform the research task\n",
    "instruct1 = {\n",
    "    \"task step\": \"1\", \n",
    "    \"task name\": \"read paper abstracts\", \n",
    "    \"task objective\": \"get initial understanding of the papers of interest\", \n",
    "    \"task description\": \"\"\"\n",
    "            provided with abstracts of paper, provide a brief summary \n",
    "            highlighting the paper core points, the purpose is to extract \n",
    "            as much information as possible\n",
    "          \"\"\",\n",
    "    \"deliverable\": deliver_format1\n",
    "}\n",
    "\n",
    "\n",
    "instruct2 = {\n",
    "    \"task step\": \"2\",\n",
    "    \"task name\": \"propose research questions and talking points\", \n",
    "    \"task objective\": \"initial brainstorming\", \n",
    "    \"task description\": \"\"\"\n",
    "          from the improved understanding of the paper, please propose \n",
    "          an interesting, unique and practical research question, \n",
    "          support your reasoning. Kept on asking questions if things are \n",
    "          not clear. \n",
    "        \"\"\",\n",
    "    \"deliverable\": deliver_format2,\n",
    "    \"function calling\": function_call\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Research: Setup Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [x.text for x in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def read_propose(context, num=5):\n",
    "    researcher = li.Session(system, dir=dir)\n",
    "    researcher.register_tools(tools, func)\n",
    "    \n",
    "    await researcher.initiate(instruct1, context=context, temperature=0.7)\n",
    "    await researcher.auto_followup(instruct2, tools=tools, \n",
    "                                   num=num, tool_parser=lambda x: x.response)\n",
    "    \n",
    "    researcher.messages_to_csv()\n",
    "    researcher.log_to_csv()\n",
    "    return researcher.conversation.messages[-1]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Research: Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 logs saved to data/log/researcher/_messages_2023-12-16T11_24_55_027803.csv\n",
      "3 logs saved to data/log/researcher/_llmlog_2023-12-16T11_24_55_029132.csv\n",
      "11 logs saved to data/log/researcher/_messages_2023-12-16T11_24_55_030996.csv\n",
      "4 logs saved to data/log/researcher/_llmlog_2023-12-16T11_24_55_031735.csv\n",
      "11 logs saved to data/log/researcher/_messages_2023-12-16T11_24_55_033999.csv\n",
      "4 logs saved to data/log/researcher/_llmlog_2023-12-16T11_24_55_034734.csv\n",
      "11 logs saved to data/log/researcher/_messages_2023-12-16T11_25_10_222276.csv\n",
      "4 logs saved to data/log/researcher/_llmlog_2023-12-16T11_25_10_222887.csv\n",
      "11 logs saved to data/log/researcher/_messages_2023-12-16T11_25_12_319998.csv\n",
      "4 logs saved to data/log/researcher/_llmlog_2023-12-16T11_25_12_320602.csv\n"
     ]
    }
   ],
   "source": [
    "out1 = await li.al_call(abstracts[8:13], read_propose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the understanding of the paper and the additional information gathered, we can formulate the following research question and talking points:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"paper\": \"SentimentArcs: A Novel Method for Self-Supervised Sentiment Analysis of Time Series Shows SOTA Transformers Can Struggle Finding Narrative Arcs\",\n",
       "  \"summary\": \"SentimentArcs is a self-supervised sentiment analysis methodology for time series data, addressing limitations of traditional sentiment analysis models such as overfitting and poor generalization. It uses an ensemble of models for synthetic ground truth generation and novel metrics for joint optimization, coupled with visualizations for domain experts to analyze narrative arcs.\",\n",
       "  \"research question\": \"How can the integration of domain expert human-in-the-loop in SentimentArcs methodology be enhanced to improve the accuracy and efficiency of sentiment analysis in complex narrative texts?\",\n",
       "  \"talking points\": {\n",
       "    \"point 1\": \"Exploring the potential for automated suggestions or guidelines to assist human experts in interpreting narrative arcs and reducing subjective bias.\",\n",
       "    \"point 2\": \"Investigating the scalability of SentimentArcs in processing and analyzing massive datasets while maintaining high levels of accuracy and involving human experts.\",\n",
       "    \"point 3\": \"Assessing the adaptability of SentimentArcs to different genres or styles of narratives, such as non-fiction or technical documents, where sentiment may be expressed differently.\"\n",
       "  }\n",
       "}\n",
       "```\n",
       "\n",
       "These research questions and talking points are designed to delve deeper into the capabilities and potential enhancements of the SentimentArcs methodology. They consider the practicality of incorporating human expertise more efficiently and the possible expansion of the methodology to diverse narrative forms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(out1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the understanding that the pyunicorn package can be applied in various fields for analyzing time series data and its specific mention of neuroscience as an application area, here's a research question and talking points:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"paper\": \"Unified functional network and nonlinear time series analysis for complex systems science: The pyunicorn package\",\n",
       "  \"summary\": \"The paper discusses the pyunicorn package, which is designed for analyzing complex systems through the construction and analysis of functional networks from time series data. The package combines complex network theory with nonlinear time series analysis and is applicable in various fields, including climatology and neuroscience.\",\n",
       "  \"research question\": \"How can the pyunicorn package enhance the understanding of neuroplasticity and the brain's adaptation mechanisms through the analysis of functional brain networks over time?\",\n",
       "  \"talking points\": {\n",
       "    \"point 1\": \"Investigating the adaptability of functional brain networks in different cognitive states or in response to various stimuli using the pyunicorn package.\",\n",
       "    \"point 2\": \"Exploring the potential of pyunicorn to identify early biomarkers of neurological disorders through the nuanced analysis of brain network dynamics.\",\n",
       "    \"point 3\": \"Assessing the efficacy of rehabilitation methods on brain network reorganization in patients with brain injuries or neurodegenerative diseases by longitudinal studies using pyunicorn.\"\n",
       "  }\n",
       "}\n",
       "```\n",
       "\n",
       "The proposed research question is interesting because it attempts to leverage the analytical strength of pyunicorn in understanding dynamic processes within the brain, such as neuroplasticity. The talking points support this by considering practical applications in cognitive science, clinical diagnosis, and treatment efficacy assessment, which are areas of significant interest in neuroscience research. If further clarification or expansion on these points is needed, we could consult the QA bot for more detailed applications or methods within pyunicorn that might be particularly useful for these types of studies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(out1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the limitations highlighted by the TimelyGPT study, an intriguing research question might be:\n",
       "\n",
       "**Research Question:** How can time-series transformers be further optimized to handle the multi-scale features of complex time-series data, particularly in scenarios involving irregular sampling and non-stationary signals?\n",
       "\n",
       "Supporting this question, the reasoning would be that addressing this gap could lead to significant improvements in the utility of transformers in real-world applications such as financial markets, weather forecasting, and patient health monitoring, where time-series data is often non-uniform and exhibits multi-scale characteristics.\n",
       "\n",
       "**Talking Points:**\n",
       "- **Point 1:** Investigate the potential of hybrid architectures that combine the strengths of transformers with other neural network paradigms, such as convolutional or recurrent layers, to enhance the model's ability to handle multi-scale features and irregular sampling.\n",
       "- **Point 2:** Explore advanced techniques for positional encoding or embedding to preserve temporal information more effectively, especially in long sequences where traditional positional encodings may fail.\n",
       "- **Point 3:** Conduct rigorous testing on diverse and large datasets, including those with non-stationary and irregularly sampled data, to assess the robustness and scalability of the proposed models.\n",
       "\n",
       "Before finalizing these points, let's use the QA bot tool to clarify two aspects:\n",
       "1. The effectiveness of hybrid architectures in current time-series transformer models.\n",
       "2. The current state of positional encoding techniques in time-series analysis and their limitations.\n",
       "\n",
       "Shall we proceed with querying the QA bot for this information?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(out1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the insights gained from both the summary of the paper and the responses from the function calls, here is a proposed research question with supporting talking points:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"paper\": \"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook\",\n",
       "  \"summary\": \"This paper conducts a survey on the use of large models for time series and spatio-temporal data analysis, examining the prevalent data types, model categories, scopes, and application areas. It identifies the need for enhanced pattern recognition, reasoning, and the potential for artificial general intelligence in this domain. The paper categorizes research into two main clusters (LM4TS and LM4STD) and provides an array of resources for practitioners.\",\n",
       "  \"research question\": \"How can theoretical analysis of large models specifically be improved to better understand and capture long-term dependencies in time series and spatio-temporal data sequences?\",\n",
       "  \"talking points\": {\n",
       "    \"point 1\": \"Investigate new architectural innovations or modifications to existing large models that could enhance their ability to model long-term dependencies more effectively.\",\n",
       "    \"point 2\": \"Explore the integration of self-supervised learning methods that could lead to better pre-training objectives, such as time-frequency consistency or mask-then-reconstruct strategies, to improve model transparency and reliability.\",\n",
       "    \"point 3\": \"Develop theoretical frameworks to improve interpretability and explainability, which would allow practitioners to better understand model predictions and facilitate more informed decision-making.\"\n",
       "  }\n",
       "}\n",
       "```\n",
       "\n",
       "This research question focuses on the theoretical underpinnings of large models, which is a current gap in the field. The talking points suggest practical steps towards addressing this gap, such as pursuing architectural changes, self-supervised learning, and theoretical frameworks for interpretability. This direction could enhance the performance of large models on time series and spatio-temporal data and make them more useful and trustworthy for practitioners."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(out1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the information we've gathered, let's formulate the following research question and talking points:\n",
       "\n",
       "Research Question:\n",
       "- How can the ESAN model be adapted to maintain predictive accuracy for stock prices well beyond the 19-month limitation identified in the paper, and what methodologies might be employed to adjust for the \"hot words\" phenomenon to ensure long-term relevance?\n",
       "\n",
       "Talking Points:\n",
       "1. **Model Adaptability**: Investigate the potential of incorporating dynamic word embedding updates or contextual relevance adjustments to handle the \"hot words\" phenomenon in financial texts. This involves exploring how the model could self-adjust and learn which terms are gaining or losing predictive power over time.\n",
       "   \n",
       "2. **Extended Predictive Horizon**: Examine the feasibility of using ensemble methods or additional temporal features to extend the forecasting capabilities of the ESAN model beyond the current 19-month timeframe. This would involve research into how different data sources and features could be combined to provide a more robust prediction for the long term.\n",
       "\n",
       "3. **Combining Strategies**: Discuss the application and potential improvements of the four strategies (single model, mean combinatory, concatenating combinatory, and exponential combinatory) proposed by the authors for timeliness issues. This point includes an analysis of how these strategies can be optimized or new strategies developed to enhance the model's performance for far-future predictions.\n",
       "\n",
       "JSON Format Deliverable:\n",
       "```json\n",
       "{\n",
       "  \"paper\": \"ESAN: Efficient Sentiment Analysis Network of A-Shares Research Reports for Stock Price Prediction\",\n",
       "  \"summary\": \"The ESAN model integrates an NLP module with a time-series forecasting model to predict stock prices. It utilizes RoBERTa for sentiment analysis and combines industry and transaction information with sentiment analysis outputs. The model shows a significant correlation between predictions and actual return rates but has limitations in predicting far-future stock prices beyond 19 months.\",\n",
       "  \"research question\": \"How can the ESAN model be adapted to maintain predictive accuracy for stock prices well beyond the 19-month limitation identified in the paper, and what methodologies might be employed to adjust for the 'hot words' phenomenon to ensure long-term relevance?\",\n",
       "  \"talking points\": {\n",
       "    \"point 1\": \"Investigate model adaptability through dynamic word embedding updates or contextual relevance adjustments to handle the 'hot words' phenomenon in financial texts.\",\n",
       "    \"point 2\": \"Examine the feasibility of using ensemble methods or additional temporal features to extend the forecasting capabilities of the ESAN model beyond the 19-month timeframe.\",\n",
       "    \"point 3\": \"Discuss the application and potential improvements of the four strategies proposed by the authors for addressing timeliness issues in predictions.\"\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(out1[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
