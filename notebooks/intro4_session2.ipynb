{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Session 2: Customization and Rate-Limited Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Session` object can be fully customized, including models, model parameters and rate limits, to accustom various usecases. \n",
    "\n",
    "Most usefully you can customize: \n",
    "- `llmconfig`: the default model parameters for every API call in the session\n",
    "- `api_service`: rate limit api_service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Default llmconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4-1106-preview',\n",
       " 'frequency_penalty': 0,\n",
       " 'max_tokens': None,\n",
       " 'n': 1,\n",
       " 'presence_penalty': 0,\n",
       " 'response_format': {'type': 'text'},\n",
       " 'seed': None,\n",
       " 'stop': None,\n",
       " 'stream': False,\n",
       " 'temperature': 0.7,\n",
       " 'top_p': 1,\n",
       " 'tools': None,\n",
       " 'tool_choice': 'none',\n",
       " 'user': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lionagi.llm_configs import oai_llmconfig\n",
    "oai_llmconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you wish to change the default behavior\n",
    "- you can either pass in a new llmconfig into the Session\n",
    "- or update the config in the session directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"you are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing in the llmconfig as a dict - recommended if want a completely different config or preset configuration\n",
    "# llmconfig = {...}\n",
    "# session1 = li.Session(system, \n",
    "#                       llmconfig=llmconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo',\n",
       " 'frequency_penalty': 0,\n",
       " 'max_tokens': None,\n",
       " 'n': 1,\n",
       " 'presence_penalty': 0,\n",
       " 'response_format': {'type': 'text'},\n",
       " 'seed': None,\n",
       " 'stop': None,\n",
       " 'stream': False,\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 1,\n",
       " 'tools': None,\n",
       " 'tool_choice': 'none',\n",
       " 'user': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lionagi as li\n",
    "# update the llmconfig directly in Session object - recommended if only a few changes\n",
    "session2 = li.Session(system)\n",
    "session2.llmconfig.update({\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.5})\n",
    "\n",
    "session2.llmconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Default api_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# let's say you use more than one API key\n",
    "# api_key2 = os.getenv(\"OPENAI_API_KEY2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi.services import OpenAIService\n",
    "\n",
    "# let us check the OpenAI api service\n",
    "service = OpenAIService(\n",
    "    # api_key = api_key2,           # you can change the api key here - default to OPENAI_API_KEY\n",
    "    # token_encoding_name,          # or token encoding name  - default to OpenAI ChatCompletion\n",
    "    max_requests_per_minute=10,     # or rate limits  - default to OpenAI tier-1 `gpt-4`\n",
    "    max_tokens_per_minute=10_000\n",
    "    )\n",
    "\n",
    "# and then you can pass in the api_service to the session\n",
    "session3 = li.Session(system, service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you wish the rate limit to be applied across sessions, you need to pass in the same api_service when creating the sessions\n",
    "\n",
    "session4 = li.Session(system, service=service)\n",
    "session5 = li.Session(system, service=service)\n",
    "session6 = li.Session(system, service=service)\n",
    "\n",
    "# now the rate limit is applied across session3-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use numpy to generate random numbers for this part\n",
    "# %pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us use a simple conditional calculator session as an example\n",
    "# in this example, we will have two steps in the instruction, first step would be choosing between sum or diff based on a case number\n",
    "# and second step would be choosing between times or plus based on the sign of the first step\n",
    "\n",
    "system = \"You are asked to perform as a calculator. Return only a numeric value, i.e. int or float, no text.\"\n",
    "\n",
    "instruct1 = {\n",
    "    \"sum the absolute values\": \"provided with 2 numbers, return the sum of their absolute values. i.e. |x|+|y|\",}\n",
    "\n",
    "instruct2 = {\n",
    "    \"diff the absolute values\": \"provided with 2 numbers, return the difference of absolute values. i.e. |x|-|y|\",}\n",
    "\n",
    "instruct3 = {\n",
    "    \"if previous response is positive\": \"times 2. i.e. *2\", # case 0\n",
    "    \"else\": \"plus 2. i.e. +2\",                              # case 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a case and context\n",
    "case = 0\n",
    "context = {\"x\": 7, \"y\": 3}\n",
    "instruct = instruct1 if case == 0 else instruct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1 result: 10\n",
      "step2 result: 12\n",
      "run clock time: 1.91 seconds\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "calculator = li.Session(system, dir='data/logs/calculator/')\n",
    "\n",
    "step1 = await calculator.initiate(instruct, context=context)\n",
    "step2 = await calculator.followup(instruct3, temperature=0.5)     # you can also modify parameters for each API call\n",
    "\n",
    "print(f\"step1 result: {step1}\")\n",
    "print(f\"step2 result: {step2}\")\n",
    "\n",
    "elapsed_time = timer() - start\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': '5', 'y': '5', 'case': '0'}\n",
      "{'x': '-6', 'y': '8', 'case': '0'}\n",
      "{'x': '-1', 'y': '7', 'case': '0'}\n",
      "{'x': '-4', 'y': '5', 'case': '1'}\n",
      "{'x': '8', 'y': '3', 'case': '1'}\n",
      "{'x': '8', 'y': '6', 'case': '1'}\n",
      "{'x': '-8', 'y': '0', 'case': '1'}\n",
      "{'x': '6', 'y': '9', 'case': '1'}\n",
      "{'x': '5', 'y': '1', 'case': '1'}\n",
      "{'x': '-6', 'y': '0', 'case': '0'}\n"
     ]
    }
   ],
   "source": [
    "# now let us run 10 senerios in parallel\n",
    "import numpy as np\n",
    "num_iterations = 10\n",
    "\n",
    "# generate random numbers\n",
    "ints1 = np.random.randint(-10, 10, size=num_iterations)\n",
    "ints2 = np.random.randint(0, 10, size=num_iterations)\n",
    "cases = np.random.randint(0,2, size=num_iterations)\n",
    "\n",
    "# let's define a simple parser function\n",
    "f = lambda i: {\"x\": str(ints1[i]), \"y\": str(ints2[i]), \"case\": str(cases[i])}\n",
    "\n",
    "# and create the various contexts, l_call (list call) is a helper function to simplify loop\n",
    "contexts = li.l_call(range(num_iterations), f)\n",
    "\n",
    "li.l_call(range(num_iterations), lambda i: print(contexts[i]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'data/logs/calculator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a workflow for concurrent execution\n",
    "\n",
    "async def calculator_workflow(context):\n",
    "    \n",
    "    calculator = li.Session(system, dir=dir)       # construct a session instance\n",
    "    context = context.copy()\n",
    "    case = int(context.pop(\"case\"))\n",
    "    \n",
    "    instruct = instruct1 if case == 0 else instruct2\n",
    "    await calculator.initiate(instruct, context=context)    # run the steps\n",
    "    await calculator.followup(instruct3, temperature=0.5)\n",
    "    \n",
    "    calculator.messages_to_csv()        # log all messages to csv\n",
    "    calculator.log_to_csv()             # log all api calls to csv\n",
    "    return li.l_call(calculator.conversation.responses, lambda i: i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_36_496524.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_36_497109.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_36_816258.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_36_816770.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_36_898741.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_36_899142.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_36_922867.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_36_923163.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_37_102491.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_37_103048.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_37_104106.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_37_104271.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_37_104808.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_37_104926.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_37_169313.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_37_169570.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_37_312646.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_37_313188.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_46_37_737793.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_46_37_738290.csv\n"
     ]
    }
   ],
   "source": [
    "# use al_call (async list call) to run the workflow concurrently over all senerios\n",
    "outs = await li.al_call(contexts, calculator_workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workload: 10\n",
      "run clock time: 2.07 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed_time = timer() - start\n",
    "print(f\"num_workload: {num_iterations}\")\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 5, 5, case: 0\n",
      "\n",
      "Outputs: ['10.0', '10.0']\n",
      "------\n",
      "\n",
      "Inputs: -6, 8, case: 0\n",
      "\n",
      "Outputs: ['14', '28']\n",
      "------\n",
      "\n",
      "Inputs: -1, 7, case: 0\n",
      "\n",
      "Outputs: ['8', '16']\n",
      "------\n",
      "\n",
      "Inputs: -4, 5, case: 1\n",
      "\n",
      "Outputs: ['1', '-3']\n",
      "------\n",
      "\n",
      "Inputs: 8, 3, case: 1\n",
      "\n",
      "Outputs: ['5', '10']\n",
      "------\n",
      "\n",
      "Inputs: 8, 6, case: 1\n",
      "\n",
      "Outputs: ['2', '4']\n",
      "------\n",
      "\n",
      "Inputs: -8, 0, case: 1\n",
      "\n",
      "Outputs: ['8', '16']\n",
      "------\n",
      "\n",
      "Inputs: 6, 9, case: 1\n",
      "\n",
      "Outputs: ['3', '5']\n",
      "------\n",
      "\n",
      "Inputs: 5, 1, case: 1\n",
      "\n",
      "Outputs: ['4', '8']\n",
      "------\n",
      "\n",
      "Inputs: -6, 0, case: 0\n",
      "\n",
      "Outputs: ['6.0', '2.0']\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, out in enumerate(outs):\n",
    "    print(f\"Inputs: {ints1[idx]}, {ints2[idx]}, case: {cases[idx]}\\n\")\n",
    "    print(f\"Outputs: {out}\")\n",
    "    print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Customized api_service concurrent calls\n",
    "\n",
    "by default, all the session will be created using the same default api_service to ensure rate limit is applied **globally**\n",
    "\n",
    "But if you would like to have a different api_service and use across sessions, you need to pass in the **same** api_service object during construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us change the rate limit to check whether it is working\n",
    "service = OpenAIService(max_requests_per_minute=10, max_tokens_per_minute=10_000)\n",
    "\n",
    "async def calculator_workflow(context):\n",
    "    \n",
    "    calculator = li.Session(system, dir=dir, service=service)       # construct a session instance\n",
    "    context = context.copy()\n",
    "    case = int(context.pop(\"case\"))\n",
    "    instruct = instruct1 if case == 0 else instruct2\n",
    "\n",
    "    await calculator.initiate(instruct, context=context)    # run the steps\n",
    "    await calculator.followup(instruct3)\n",
    "    \n",
    "    calculator.messages_to_csv()        # log all messages to csv\n",
    "    calculator.log_to_csv()             # log all api calls to csv\n",
    "\n",
    "    return li.l_call(calculator.conversation.responses, lambda i: i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_38_668920.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_38_669254.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_38_769745.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_38_769954.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_38_885544.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_38_886015.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_38_886430.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_38_886568.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_38_939701.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_38_939995.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_39_190912.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_39_191339.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_39_448392.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_39_449361.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_39_450583.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_39_450910.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_39_488348.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_39_488803.csv\n",
      "5 logs saved to data/logs/calculator/messages_2023-12-28T00_47_39_561843.csv\n",
      "2 logs saved to data/logs/calculator/llmlog_2023-12-28T00_47_39_563056.csv\n",
      "num_workload: 10\n",
      "run clock time: 61.81 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "outs = await li.al_call(contexts, calculator_workflow)  \n",
    "\n",
    "elapsed_time = timer() - start\n",
    "print(f\"num_workload: {num_iterations}\")\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 5, 5, case: 0\n",
      "\n",
      "Outputs: ['10', '10']\n",
      "------\n",
      "\n",
      "Inputs: -6, 8, case: 0\n",
      "\n",
      "Outputs: ['14', '28']\n",
      "------\n",
      "\n",
      "Inputs: -1, 7, case: 0\n",
      "\n",
      "Outputs: ['6', '14']\n",
      "------\n",
      "\n",
      "Inputs: -4, 5, case: 1\n",
      "\n",
      "Outputs: ['3', '-1']\n",
      "------\n",
      "\n",
      "Inputs: 8, 3, case: 1\n",
      "\n",
      "Outputs: ['5', '10']\n",
      "------\n",
      "\n",
      "Inputs: 8, 6, case: 1\n",
      "\n",
      "Outputs: ['2', '4']\n",
      "------\n",
      "\n",
      "Inputs: -8, 0, case: 1\n",
      "\n",
      "Outputs: ['8', '16']\n",
      "------\n",
      "\n",
      "Inputs: 6, 9, case: 1\n",
      "\n",
      "Outputs: ['3', '5']\n",
      "------\n",
      "\n",
      "Inputs: 5, 1, case: 1\n",
      "\n",
      "Outputs: ['4', '8']\n",
      "------\n",
      "\n",
      "Inputs: -6, 0, case: 0\n",
      "\n",
      "Outputs: ['6.0', '2.0']\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, out in enumerate(outs):\n",
    "    print(f\"Inputs: {ints1[idx]}, {ints2[idx]}, case: {cases[idx]}\\n\")\n",
    "    print(f\"Outputs: {out}\")\n",
    "    print(\"------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
