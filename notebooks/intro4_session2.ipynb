{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Session 2: Customization and Rate-Limited Concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Session` object can be fully customized, including models, model parameters and rate limits, to accustom various usecases. \n",
    "\n",
    "Most usefully you can customize: \n",
    "- `llmconfig`: the default model parameters for every API call in the session\n",
    "- `api_service`: rate limit api_service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Default llmconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4-1106-preview',\n",
       " 'frequency_penalty': 0,\n",
       " 'max_tokens': None,\n",
       " 'n': 1,\n",
       " 'presence_penalty': 0,\n",
       " 'response_format': {'type': 'text'},\n",
       " 'seed': None,\n",
       " 'stop': None,\n",
       " 'stream': False,\n",
       " 'temperature': 0.7,\n",
       " 'top_p': 1,\n",
       " 'tools': None,\n",
       " 'tool_choice': 'none',\n",
       " 'user': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us see the default llmconfig, which is currently using OpenAI\n",
    "li.oai_llmconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you wish to change the default behavior\n",
    "- you can either pass in a new llmconfig into the Session\n",
    "- or update the config in the session directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"you are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing in the llmconfig as a dict - recommended if want a completely different config or preset configuration\n",
    "# llmconfig = {...}\n",
    "# session1 = li.Session(system, \n",
    "#                       llmconfig=llmconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo',\n",
       " 'frequency_penalty': 0,\n",
       " 'max_tokens': None,\n",
       " 'n': 1,\n",
       " 'presence_penalty': 0,\n",
       " 'response_format': {'type': 'text'},\n",
       " 'seed': None,\n",
       " 'stop': None,\n",
       " 'stream': False,\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 1,\n",
       " 'tools': None,\n",
       " 'tool_choice': 'none',\n",
       " 'user': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the llmconfig directly in Session object - recommended if only a few changes\n",
    "session2 = li.Session(system)\n",
    "session2.llmconfig.update({\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.5})\n",
    "\n",
    "session2.llmconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Default api_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# let's say you use more than one API key\n",
    "# api_key2 = os.getenv(\"OPENAI_API_KEY2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the OpenAI api service\n",
    "api_service = li.OpenAIService(\n",
    "    # api_key = api_key2,           # you can change the api key here - default to OPENAI_API_KEY\n",
    "    # token_encoding_name,          # or token encoding name  - default to "cl100k_base"\n",
    "    max_requests_per_minute=10,     # or rate limits  - default to OpenAI tier-1 `gpt-4`\n",
    "    max_tokens_per_minute=10_000\n",
    "    )\n",
    "\n",
    "# and then you can pass in the api_service to the session\n",
    "session3 = li.Session(system, \n",
    "                      api_service=api_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you wish the rate limit to be applied across sessions, you need to pass in the same api_service when creating the sessions\n",
    "\n",
    "session4 = li.Session(system,api_service=api_service)\n",
    "session5 = li.Session(system,api_service=api_service)\n",
    "session6 = li.Session(system,api_service=api_service)\n",
    "\n",
    "# now the rate limit is applied across session3-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use numpy to generate random numbers for this part\n",
    "# %pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us use a simple conditional calculator session as an example\n",
    "# in this example, we will have two steps in the instruction, first step would be choosing between sum or diff based on a case number\n",
    "# and second step would be choosing between times or plus based on the sign of the first step\n",
    "\n",
    "system = \"You are asked to perform as a calculator. Return only a numeric value, i.e. int or float, no text.\"\n",
    "\n",
    "instruct1 = {\n",
    "    \"sum the absolute values\": \"provided with 2 numbers, return the sum of their absolute values. i.e. |x|+|y|\",}\n",
    "\n",
    "instruct2 = {\n",
    "    \"diff the absolute values\": \"provided with 2 numbers, return the difference of absolute values. i.e. |x|-|y|\",}\n",
    "\n",
    "instruct3 = {\n",
    "    \"if previous response is positive\": \"times 2. i.e. *2\", # case 0\n",
    "    \"else\": \"plus 2. i.e. +2\",                              # case 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a case and context\n",
    "case = 0\n",
    "context = {\"x\": 7, \"y\": 3}\n",
    "instruct = instruct1 if case == 0 else instruct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1 result: 10\n",
      "step2 result: 20\n",
      "run clock time: 3.04 seconds\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "calculator = li.Session(system, dir='data/logs/calculator')\n",
    "step1 = await calculator.initiate(instruct, context=context)\n",
    "step2 = await calculator.followup(instruct3, temperature=0.5)     # you can also modify parameters for each API call\n",
    "\n",
    "print(f\"step1 result: {step1}\")\n",
    "print(f\"step2 result: {step2}\")\n",
    "\n",
    "elapsed_time = timer() - start\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': '5', 'y': '2', 'case': '1'}\n",
      "{'x': '-1', 'y': '9', 'case': '0'}\n",
      "{'x': '3', 'y': '2', 'case': '0'}\n",
      "{'x': '2', 'y': '1', 'case': '1'}\n",
      "{'x': '-1', 'y': '3', 'case': '1'}\n",
      "{'x': '-2', 'y': '9', 'case': '0'}\n",
      "{'x': '8', 'y': '3', 'case': '1'}\n",
      "{'x': '0', 'y': '5', 'case': '0'}\n",
      "{'x': '-3', 'y': '5', 'case': '0'}\n",
      "{'x': '-8', 'y': '9', 'case': '0'}\n"
     ]
    }
   ],
   "source": [
    "# now let us run 10 senerios in parallel\n",
    "import numpy as np\n",
    "num_iterations = 10\n",
    "\n",
    "# generate random numbers\n",
    "ints1 = np.random.randint(-10, 10, size=num_iterations)\n",
    "ints2 = np.random.randint(0, 10, size=num_iterations)\n",
    "cases = np.random.randint(0,2, size=num_iterations)\n",
    "\n",
    "# let's define a simple parser function\n",
    "f = lambda i: {\"x\": str(ints1[i]), \"y\": str(ints2[i]), \"case\": str(cases[i])}\n",
    "\n",
    "# and create the various contexts, l_call (list call) is a helper function to simplify loop\n",
    "contexts = li.l_call(range(num_iterations), f)\n",
    "\n",
    "li.l_call(range(num_iterations), lambda i: print(contexts[i]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'data/logs/calculator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a workflow for concurrent execution\n",
    "\n",
    "async def calculator_workflow(context):\n",
    "    \n",
    "    calculator = li.Session(system, dir=dir)       # construct a session instance\n",
    "    context = context.copy()\n",
    "    case = int(context.pop(\"case\"))\n",
    "    instruct = instruct1 if case == 0 else instruct2\n",
    "\n",
    "    await calculator.initiate(instruct, context=context)    # run the steps\n",
    "    await calculator.followup(instruct3, temperature=0.5)\n",
    "    \n",
    "    calculator.messages_to_csv()        # log all messages to csv\n",
    "    calculator.log_to_csv()             # log all api calls to csv\n",
    "\n",
    "    return li.l_call(calculator.conversation.responses, lambda i: i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 logs saved to data/logs/calculator2023-12-09T10_04_42_591617_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_42_592264_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_42_790269_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_42_790813_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_42_791145_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_42_791366_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_42_810475_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_42_810915_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_42_813164_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_42_813407_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_43_138462_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_43_139495_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_43_186858_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_43_187374_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_43_188234_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_43_188658_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_43_314126_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_43_315220_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_04_43_315880_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_04_43_316240_llmlog.csv\n",
      "num_workload: 10\n",
      "run clock time: 3.37 seconds\n"
     ]
    }
   ],
   "source": [
    "# concurrently run the workflow over all cases\n",
    "# use al_call (async list call) to run the workflow concurrently over all senerios\n",
    "start = timer()\n",
    "\n",
    "outs = await li.al_call(contexts, calculator_workflow)\n",
    "\n",
    "elapsed_time = timer() - start\n",
    "print(f\"num_workload: {num_iterations}\")\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 5, 2, case: 1\n",
      "\n",
      "Outputs: ['3', '6']\n",
      "------\n",
      "\n",
      "Inputs: -1, 9, case: 0\n",
      "\n",
      "Outputs: ['10', '18']\n",
      "------\n",
      "\n",
      "Inputs: 3, 2, case: 0\n",
      "\n",
      "Outputs: ['5.0', '6.0']\n",
      "------\n",
      "\n",
      "Inputs: 2, 1, case: 1\n",
      "\n",
      "Outputs: ['1', '4']\n",
      "------\n",
      "\n",
      "Inputs: -1, 3, case: 1\n",
      "\n",
      "Outputs: ['2', '4']\n",
      "------\n",
      "\n",
      "Inputs: -2, 9, case: 0\n",
      "\n",
      "Outputs: ['11', '22']\n",
      "------\n",
      "\n",
      "Inputs: 8, 3, case: 1\n",
      "\n",
      "Outputs: ['5', '10']\n",
      "------\n",
      "\n",
      "Inputs: 0, 5, case: 0\n",
      "\n",
      "Outputs: ['5', '7']\n",
      "------\n",
      "\n",
      "Inputs: -3, 5, case: 0\n",
      "\n",
      "Outputs: ['8', '16']\n",
      "------\n",
      "\n",
      "Inputs: -8, 9, case: 0\n",
      "\n",
      "Outputs: ['17', '34']\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, out in enumerate(outs):\n",
    "    print(f\"Inputs: {ints1[idx]}, {ints2[idx]}, case: {cases[idx]}\\n\")\n",
    "    print(f\"Outputs: {out}\")\n",
    "    print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Customized api_service concurrent calls\n",
    "\n",
    "by default, all the session will be created using the same default api_service to ensure rate limit is applied **globally**\n",
    "\n",
    "But if you would like to have a different api_service and use across sessions, you need to pass in the **same** api_service object during construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us change the rate limit to check whether it is working\n",
    "api_service = li.OpenAIService(max_requests_per_minute=10, max_tokens_per_minute=10_000)\n",
    "\n",
    "async def calculator_workflow(context):\n",
    "    \n",
    "    calculator = li.Session(system, dir=dir, api_service=api_service)       # construct a session instance\n",
    "    context = context.copy()\n",
    "    case = int(context.pop(\"case\"))\n",
    "    instruct = instruct1 if case == 0 else instruct2\n",
    "\n",
    "    await calculator.initiate(instruct, context=context)    # run the steps\n",
    "    await calculator.followup(instruct3)\n",
    "    \n",
    "    calculator.messages_to_csv()        # log all messages to csv\n",
    "    calculator.log_to_csv()             # log all api calls to csv\n",
    "\n",
    "    return li.l_call(calculator.conversation.responses, lambda i: i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_076942_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_077905_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_177248_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_177921_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_433836_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_434522_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_435023_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_435214_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_451061_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_451259_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_451451_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_451590_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_880993_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_881587_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_45_881891_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_45_882058_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_46_132258_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_46_133354_llmlog.csv\n",
      "5 logs saved to data/logs/calculator2023-12-09T10_05_47_138970_messages.csv\n",
      "2 logs saved to data/logs/calculator2023-12-09T10_05_47_139615_llmlog.csv\n",
      "num_workload: 10\n",
      "run clock time: 63.80 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "outs = await li.al_call(contexts, calculator_workflow)  \n",
    "\n",
    "elapsed_time = timer() - start\n",
    "print(f\"num_workload: {num_iterations}\")\n",
    "print(f\"run clock time: {elapsed_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 5, 2, case: 1\n",
      "\n",
      "Outputs: ['3', '6']\n",
      "------\n",
      "\n",
      "Inputs: -1, 9, case: 0\n",
      "\n",
      "Outputs: ['10.0', '18.0']\n",
      "------\n",
      "\n",
      "Inputs: 3, 2, case: 0\n",
      "\n",
      "Outputs: ['5', '10']\n",
      "------\n",
      "\n",
      "Inputs: 2, 1, case: 1\n",
      "\n",
      "Outputs: ['1', '4']\n",
      "------\n",
      "\n",
      "Inputs: -1, 3, case: 1\n",
      "\n",
      "Outputs: ['2', '4']\n",
      "------\n",
      "\n",
      "Inputs: -2, 9, case: 0\n",
      "\n",
      "Outputs: ['11', '22']\n",
      "------\n",
      "\n",
      "Inputs: 8, 3, case: 1\n",
      "\n",
      "Outputs: ['5', '10']\n",
      "------\n",
      "\n",
      "Inputs: 0, 5, case: 0\n",
      "\n",
      "Outputs: ['5', '10']\n",
      "------\n",
      "\n",
      "Inputs: -3, 5, case: 0\n",
      "\n",
      "Outputs: ['8', '16']\n",
      "------\n",
      "\n",
      "Inputs: -8, 9, case: 0\n",
      "\n",
      "Outputs: ['17', '34']\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, out in enumerate(outs):\n",
    "    print(f\"Inputs: {ints1[idx]}, {ints2[idx]}, case: {cases[idx]}\\n\")\n",
    "    print(f\"Outputs: {out}\")\n",
    "    print(\"------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
